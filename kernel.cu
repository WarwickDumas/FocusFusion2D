#include "kernel.h"
#include "helpers.cu"
#include "vector_tensor.cu"
#include "cuda_struct.h"
#include "constant.h"
#include "FFxtubes.h"

#define BWDSIDET
#define LONGITUDINAL

// TO DO:
// Line 1420:
// Yes, very much a waste. The edge positions should be calculated from the vertex positions, we can
// load flags to determine if it is an insulator-crossing triangle and that is the proper way to handle that.

 
#define FOUR_PI 12.5663706143592
#define TEST  (0) //iVertex == VERTCHOSEN) 
#define TEST_ELEC_VISC_TRI (0) //iMinor == CHOSEN)
#define TESTNEUTVISC2 (0) // iMinor == CHOSEN)
#define TESTPRESSUREY (0) //iVertex == VERTCHOSEN)
#define TEST_T (0) 
#define TEST3  (0)
#define TEST1 (0)
#define TESTTRI (0) // thermal pressure output & infer minor density & momflux_minor
#define TESTADVECT (0)
#define TESTADVECTZ (0)//iVertex == VERTCHOSEN)
#define TESTADVECTNEUT (0) //iVertex == VERTCHOSEN)
#define TESTIONVERTVISC (0)//(iVertex == VERTCHOSEN)
#define TESTNEUTVISC (0) // iVertex == VERTCHOSEN) 
#define TESTVISC (0) //iMinor == CHOSEN)
#define TESTIONVISC (0) 
#define TESTHEAT (0)
#define TESTHEATFULL (0)
#define TESTHEAT1 (0)
#define TESTTRI2 (0)
#define TESTTRI3 (0)
#define TESTHEAT2 (0)
#define TESTIONISE (0)
#define TESTOHMS (0) //iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL)
#define TEST_IONIZE (iVertex == VERTCHOSEN)
#define TESTACCEL (0) //iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL)
#define TESTACCEL2 (0) //iMinor - BEGINNING_OF_CENTRAL == VERTCHOSEN)
#define TESTACCEL_X (0) // PopOhms output
#define TESTLAP (0)
#define TESTLAP2 (0) //(iMinor == CHOSEN1) || (iMinor == CHOSEN2))
#define TESTVEZ (0)//iMinor == CHOSEN)
#define TEST_VS_MATRIX (0) //iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL)
#define TEST_VS_MATRIX2 (0) // iVertex == VERTCHOSEN
#define TESTVNX (0)
#define TESTVNY (0) //iMinor == CHOSEN)//PopOhms
#define TESTVNY2 (0) // iMinor == CHOSEN) //neutral momflux
#define TESTVNY3 (0)// || (iVertex == VERTCHOSEN2))
#define TESTVNZ (0)//iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL)
#define TEST_ADV_HEAT_FLAG 0
#define TEST_ADV_MASS_FLAG 0
#define TESTVNXVERT (0)
#define TESTVNYVERT (0)
#define TEST_ACCEL_Y (0) // iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL)
#define VISCMAG 1 
#define MIDPT_A
#define TEST_ACCEL_EZ (0)//iMinor == CHOSEN)
#define TEST_EPSILON_Y (0)
#define TEST_EPSILON_X (0)
#define TEST_EPSILON_Y_IMINOR (0)//iMinor == lChosen)
#define TEST_EPSILON_X_MINOR (0) // iMinor == CHOSEN)
 
#define ARTIFICIAL_RELATIVE_THRESH  1.0e10 // if we let it be more strict than heat thresh then it drives a difference generating heat!
#define ARTIFICIAL_RELATIVE_THRESH_HEAT  1.0e10   // typical initial density is 1e8 vs 1e18
#define LOW_THRESH_FOR_VISC_CALCS 1.0e10 // density. This should not be too much greater than the density where we do not soak away velocity and heat. At the moment it's 100 times.
#define MINIMUM_NU_EI_DENSITY       1.0e12


// Try excluding only if both sides are at this density -- it just doesn't matter.
// Just exclude to/from anything this sparse. It doesn't matter.

// Heat is the one that can be a problem as soaking away heat means that we never can grow our ionization level - we're basically stuck at 0
// unless a giant wave of ions sweeps in. We start at low ionization.. 1e8/1e18 = 1e-10.


// Change log. 090419: Change upwind density routine to just use n from the lowest cell that is upwind for at least 1 side.
// 230419: Change nu_n used in kappa_neut to be a lc of collision frequencies.

// 250419: Change to use min(ita_ours, ita_theirs). Maybe need to do same for kappa_par. 
// Change to apportion visc heat from tri per N.

//const int Chosens[7] = { 25454, 86529, 25453, 86381, 25455, 86530, 25750 };

__device__ f64 ArtificialUpliftFactor(f64 n_i, f64 n_n)
{
	// At n_i = 1e9, nn 1e14 we want nn equiv 1e20 so 1e6 uplift
	// We do not care much about small amt of neutrals as much as small amt of ions.

	if (n_i + n_n > 1.0e15) return 1.0;

	f64 t = (n_n*1.0e15 + n_i*(n_i + n_n)) / ((n_i + n_n)*(n_i + n_n));
	return min(t*t,1.0e6);

	// Having to boost up when < 1e15 because our dodgy point has > 1e14.
}
__device__ f64 ArtificialUpliftFactor_MT(f64 n_i, f64 n_n)
{
	// At n_i = 1e9, nn 1e14 we want nn equiv 1e20 so 1e6 uplift
	// We do not care much about small amt of neutrals as much as small amt of ions.

	if (n_i > 1.0e13) return 1.0;
	
	f64 additional_nn = min(exp(-n_i*n_i/0.5e24)*(1.0e30 / (n_i)), 1.0e20); // high effective density to produce hydrodynamics

	return 1.0 + additional_nn /n_n;

	//if (n_i + n_n > 1.0e15) return 1.0;
	
	//f64 t = (n_n*1.0e15 + n_i*(n_i + n_n)) / ((n_i + n_n)*(n_i + n_n));
	//return min(t*t,1.0e6);

	// Having to boost up when < 1e15 because our dodgy point has > 1e14.
}

__device__ __forceinline__ void CalculateCircumcenter(f64_vec2 * p_cc, f64_vec2 poscorner0, f64_vec2 poscorner1, f64_vec2 poscorner2)
{
	f64_vec2 Bb = poscorner1 - poscorner0;
	f64_vec2 C = poscorner2 - poscorner0;
	f64 D = 2.0*(Bb.x*C.y - Bb.y*C.x);
	f64 modB = Bb.x*Bb.x + Bb.y*Bb.y;
	f64 modC = C.x*C.x + C.y*C.y;
	p_cc->x = (C.y*modB - Bb.y*modC) / D + poscorner0.x;
	p_cc->y = (Bb.x*modC - C.x*modB) / D + poscorner0.y;
	// formula agrees with wikipedia so why does it give a stupid result.

}

__device__ __forceinline__ bool TestDomainPos(f64_vec2 pos)
{
	return (
		(pos.x*pos.x + pos.y*pos.y > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
		&&
		(pos.x*pos.x + (pos.y - CATHODE_ROD_R_POSITION)*(pos.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)		
		);
}

__device__ f64 GetRecombinationRate_given_v(f64 const Te, int i_v)
{
	f64 const TeeV = Te / kB;
	f64 const Tesq = TeeV*TeeV;
	f64 const Te3 = Tesq*TeeV;
	f64 rate, rate1, rate2;
	if (Te > 4.75e-11) return 0.0;
	if (Te < 1.875e-12) { // Magic numbers!!
		rate = (recomb_coeffs[i_v][0][4] + recomb_coeffs[i_v][0][3] * TeeV
			+ recomb_coeffs[i_v][0][2] * Tesq + recomb_coeffs[i_v][0][1] * Te3);
		if (i_v < 7) {
			rate /= (1.0 - recomb_coeffs[i_v][0][0] * TeeV);
		} else {
			rate += exp(-TeeV*0.5)*recomb_coeffs[i_v][0][0];
		};
	} else {
		if (Te < 2.25e-12) {
			rate1 = (recomb_coeffs[i_v][0][4] + recomb_coeffs[i_v][0][3] * TeeV
				+ recomb_coeffs[i_v][0][2] * Tesq + recomb_coeffs[i_v][0][1] * Te3);
			rate2 = (recomb_coeffs[i_v][1][4] + recomb_coeffs[i_v][1][3] * TeeV
				+ recomb_coeffs[i_v][1][2] * Tesq + recomb_coeffs[i_v][1][1] * Te3);
			if (i_v < 7) {
				rate1 /= (1.0 - recomb_coeffs[i_v][0][0] * TeeV);
				rate2 /= (1.0 - recomb_coeffs[i_v][1][0] * TeeV);
			}
			else {
				rate1 += exp(-TeeV*0.5)*recomb_coeffs[i_v][0][0];
				rate2 += exp(-TeeV*0.5)*recomb_coeffs[i_v][1][0];
			};
			f64 ppn_high = (Te - 1.875e-12) / (2.25e-12 - 1.875e-12);
			f64 ppn_low = (2.25e-12 - Te) / (2.25e-12-1.875e-12);
			rate = rate1*ppn_low + rate2*ppn_high;

		} else {
			if (Te < 1.05e-11) {
				rate = (recomb_coeffs[i_v][1][4] + recomb_coeffs[i_v][1][3] * TeeV
					+ recomb_coeffs[i_v][1][2] * Tesq + recomb_coeffs[i_v][1][1] * Te3);
				if (i_v < 7) {
					rate /= (1.0 - recomb_coeffs[i_v][1][0] * TeeV);
				}
				else {
					rate += exp(-TeeV*0.5)*recomb_coeffs[i_v][1][0];
				};
			} else {
				if (Te < 1.0875e-11) {
					rate1 = (recomb_coeffs[i_v][1][4] + recomb_coeffs[i_v][1][3] * TeeV
						+ recomb_coeffs[i_v][1][2] * Tesq + recomb_coeffs[i_v][1][1] * Te3);
					rate2 = (recomb_coeffs[i_v][2][4] + recomb_coeffs[i_v][2][3] * TeeV
						+ recomb_coeffs[i_v][2][2] * Tesq + recomb_coeffs[i_v][2][1] * Te3);
					if (i_v < 7) {
						rate1 /= (1.0 - recomb_coeffs[i_v][1][0] * TeeV);
						rate2 /= (1.0 - recomb_coeffs[i_v][2][0] * TeeV);
					} else {
						rate1 += exp(-TeeV*0.5)*recomb_coeffs[i_v][1][0];
						rate2 += exp(-TeeV*0.5)*recomb_coeffs[i_v][2][0];
					};
				f64	ppn_high = (Te - 1.05e-11) / (1.0875e-11 - 1.05e-11);
				f64	ppn_low = (1.0875e-11 - Te) / (1.0875e-11 - 1.05e-11);
					rate = rate1*ppn_low + rate2*ppn_high;

				} else {
					rate = (recomb_coeffs[i_v][2][4] + recomb_coeffs[i_v][2][3] * TeeV
						+ recomb_coeffs[i_v][2][2] * Tesq + recomb_coeffs[i_v][2][1] * Te3);
					if (i_v < 7) {
						rate /= (1.0 - recomb_coeffs[i_v][2][0] * TeeV);
					} else {
						rate += exp(-TeeV*0.5)*recomb_coeffs[i_v][2][0];
					};
				};
			};
		};
	};
	return rate;
}

__device__ f64 GetIonizationRate_given_v(f64 const Te, int i_v)
{

	f64 TeeV = Te / kB;
	if (Te > ionize_temps[i_v][9]) {
		TeeV = ionize_temps[i_v][9] / kB;
	}
	f64 Tesq = TeeV*TeeV;
	f64 Te3 = Tesq*TeeV;
	f64 Te4 = Tesq*Tesq;

	f64 rate, rate1, rate2;

	bool b_exp[5];
	memset(b_exp, 0, sizeof(bool) * 5);
	if (i_v < 18) {
		b_exp[0] = true; b_exp[1] = true; b_exp[2] = true;
	};
	if (i_v == 18) {
		b_exp[0] = true; b_exp[1] = true;
	}
	//printf("i_v %d b_exp %d %d %d %d %d \n", i_v, (b_exp[0]) ? 1 : 0, (b_exp[1]) ? 1 : 0, (b_exp[2]) ? 1 : 0, (b_exp[3]) ? 1 : 0, (b_exp[4]) ? 1 : 0);
	if (Te < ionize_temps[i_v][0]) {
		if (i_v < 19) {
			rate = 0.0;
		} // let's say 18 is where we treat as over critical velocity.
		else {
			TeeV = ionize_temps[i_v][0] / kB; // return low end value
			rate = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
				+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
				+ ionize_coeffs[i_v][0][0] * Te4);
		};
	} else {
		if (Te < ionize_temps[i_v][1]) {
			rate = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
				+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
				+ ionize_coeffs[i_v][0][0] * Te4);
			if (b_exp[0]) rate = exp(rate);

		}
		else {
			if (Te < ionize_temps[i_v][2]) {
				rate1 = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
					+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
					+ ionize_coeffs[i_v][0][0] * Te4);
				rate2 = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
					+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
					+ ionize_coeffs[i_v][1][0] * Te4);

				if (b_exp[0]) rate1 = exp(rate1);
				if (b_exp[1]) rate2 = exp(rate2);

				f64 ppn_high = (Te - ionize_temps[i_v][1]) / (ionize_temps[i_v][2] - ionize_temps[i_v][1]);
				f64 ppn_low = (ionize_temps[i_v][2] - Te) / (ionize_temps[i_v][2] - ionize_temps[i_v][1]);
				rate = rate1*ppn_low + rate2*ppn_high;

			}
			else {
				if (Te < ionize_temps[i_v][3])
				{
					rate = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
						+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
						+ ionize_coeffs[i_v][1][0] * Te4);
					if (b_exp[1]) rate = exp(rate);

				} else {
					if (Te < ionize_temps[i_v][4]) {
						rate1 = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
							+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
							+ ionize_coeffs[i_v][1][0] * Te4);
						rate2 = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
							+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
							+ ionize_coeffs[i_v][2][0] * Te4);

						if (b_exp[1]) rate1 = exp(rate1);
						if (b_exp[2]) rate2 = exp(rate2);

						f64 ppn_high = (Te - ionize_temps[i_v][3]) / (ionize_temps[i_v][4] - ionize_temps[i_v][3]);
						f64 ppn_low = (ionize_temps[i_v][4] - Te) / (ionize_temps[i_v][4] - ionize_temps[i_v][3]);

						rate = rate1*ppn_low + rate2*ppn_high;

					}
					else {
						if (Te < ionize_temps[i_v][5]) {
							rate = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
								+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
								+ ionize_coeffs[i_v][2][0] * Te4);
							if (b_exp[2]) rate = exp(rate);

						}
						else {
							if (Te < ionize_temps[i_v][6]) {
								rate1 = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
									+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
									+ ionize_coeffs[i_v][2][0] * Te4);
								rate2 = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
									+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
									+ ionize_coeffs[i_v][3][0] * Te4);

								if (b_exp[2]) rate1 = exp(rate1);
								if (b_exp[3]) rate2 = exp(rate2);

								f64 ppn_high = (Te - ionize_temps[i_v][5]) / (ionize_temps[i_v][6] - ionize_temps[i_v][5]);
								f64 ppn_low = (ionize_temps[i_v][6] - Te) / (ionize_temps[i_v][6] - ionize_temps[i_v][5]);

								rate = rate1*ppn_low + rate2*ppn_high;

							}
							else {
								if (Te < ionize_temps[i_v][7]) {
									rate = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
										+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
										+ ionize_coeffs[i_v][3][0] * Te4);
									if (b_exp[3]) rate = exp(rate); // it is always false anyway								
								}
								else {
									if (Te < ionize_temps[i_v][8]) {

										rate1 = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
											+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
											+ ionize_coeffs[i_v][3][0] * Te4);
										rate2 = (ionize_coeffs[i_v][4][4] + ionize_coeffs[i_v][4][3] * TeeV
											+ ionize_coeffs[i_v][4][2] * Tesq + ionize_coeffs[i_v][4][1] * Te3
											+ ionize_coeffs[i_v][4][0] * Te4);
										if (b_exp[3]) rate1 = exp(rate1); // it is always false anyway								
										if (b_exp[4]) rate2 = exp(rate2); // it is always false anyway								
										f64 ppn_high = (Te - ionize_temps[i_v][7]) / (ionize_temps[i_v][8] - ionize_temps[i_v][7]);
										f64 ppn_low = (ionize_temps[i_v][8] - Te) / (ionize_temps[i_v][8] - ionize_temps[i_v][7]);

										rate = rate1*ppn_low + rate2*ppn_high;
									}
									else {

										rate = (ionize_coeffs[i_v][4][4] + ionize_coeffs[i_v][4][3] * TeeV
											+ ionize_coeffs[i_v][4][2] * Tesq + ionize_coeffs[i_v][4][1] * Te3
											+ ionize_coeffs[i_v][4][0] * Te4);
										if (b_exp[4]) rate = exp(rate); // it is always false anyway								
									};
								}
							};
						};
					};
				};
			};
		};
	};
	return rate;
}

__device__ f64 GetIonizationRate_given_v_Debug(f64 const Te, int i_v)
{
	
	f64 TeeV = Te / kB;
	if (Te > ionize_temps[i_v][9]) {
		TeeV = ionize_temps[i_v][9] / kB;
	}
	f64 Tesq = TeeV*TeeV;
	f64 Te3 = Tesq*TeeV;
	f64 Te4 = Tesq*Tesq;
	
	f64 rate, rate1, rate2;

	bool b_exp[5];
	memset(b_exp, 0, sizeof(bool) * 5);
	if (i_v < 18) {
		b_exp[0] = true; b_exp[1] = true; b_exp[2] = true;
	};
	if (i_v == 18) {
		b_exp[0] = true; b_exp[1] = true;
	}
	printf("i_v %d b_exp %d %d %d %d %d \n", i_v, (b_exp[0]) ? 1 : 0, (b_exp[1]) ? 1 : 0, (b_exp[2]) ? 1 : 0, (b_exp[3]) ? 1 : 0, (b_exp[4]) ? 1 : 0);
	if (Te < ionize_temps[i_v][0]) {
		if (i_v < 19) {
			rate = 0.0;
			printf("Te %1.12E was below %1.12E and we returned 0 ionization rate.\n", Te, ionize_temps[i_v][0]);
		} // let's say 18 is where we treat as over critical velocity.
		else {
			TeeV = ionize_temps[i_v][0] / kB; // return low end value
			rate = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
				+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
				+ ionize_coeffs[i_v][0][0] * Te4);
			printf("used low end rate: %1.9E \n", rate);
		};
		
	} else {
		if (Te < ionize_temps[i_v][1]) {
			rate = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
				+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
				+ ionize_coeffs[i_v][0][0] * Te4);
			if (b_exp[0]) rate = exp(rate);

			printf("i_v %d Te %1.8E b_exp[0] %d 0-1 rate %1.8E coeffs[0] \n", i_v, Te,
				(b_exp[0] ? 1:0), rate, ionize_coeffs[i_v][0][0]);
			
		} else {
			if (Te < ionize_temps[i_v][2]) {
				rate1 = (ionize_coeffs[i_v][0][4] + ionize_coeffs[i_v][0][3] * TeeV
					+ ionize_coeffs[i_v][0][2] * Tesq + ionize_coeffs[i_v][0][1] * Te3
					+ ionize_coeffs[i_v][0][0] * Te4);
				rate2 = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
					+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
					+ ionize_coeffs[i_v][1][0] * Te4);
	
				if (b_exp[0]) rate1 = exp(rate1);
				if (b_exp[1]) rate2 = exp(rate2);

				f64 ppn_high = (Te - ionize_temps[i_v][1]) / (ionize_temps[i_v][2] - ionize_temps[i_v][1]);
				f64 ppn_low = (ionize_temps[i_v][2] - Te) / (ionize_temps[i_v][2] - ionize_temps[i_v][1]);
				rate = rate1*ppn_low + rate2*ppn_high;


				printf("i_v %d Te %1.8E b_exp[0] %d b_exp[1] %d 1-2 rate %1.8E coeffs[1][0] \n", i_v, Te,
					(b_exp[0] ? 1 : 0), (b_exp[1] ? 1 : 0), rate, ionize_coeffs[i_v][1][0]);
			} else {
				if (Te < ionize_temps[i_v][3])
				{
					rate = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
						+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
						+ ionize_coeffs[i_v][1][0] * Te4);
					if (b_exp[1]) rate = exp(rate);

					printf("i_v %d Te %1.8E b_exp[0] %d b_exp[1] %d Temps2-3 = 1 rate %1.8E coeffs[1][0] \n", i_v, Te,
						(b_exp[0] ? 1 : 0), (b_exp[1] ? 1 : 0), rate, ionize_coeffs[i_v][1][0]);
				} else {
					if (Te < ionize_temps[i_v][4]) {
						rate1 = (ionize_coeffs[i_v][1][4] + ionize_coeffs[i_v][1][3] * TeeV
							+ ionize_coeffs[i_v][1][2] * Tesq + ionize_coeffs[i_v][1][1] * Te3
							+ ionize_coeffs[i_v][1][0] * Te4);
						rate2 = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
							+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
							+ ionize_coeffs[i_v][2][0] * Te4);

						if (b_exp[1]) rate1 = exp(rate1);
						if (b_exp[2]) rate2 = exp(rate2);

						f64 ppn_high = (Te - ionize_temps[i_v][3]) / (ionize_temps[i_v][4] - ionize_temps[i_v][3]);
						f64 ppn_low = (ionize_temps[i_v][4] - Te) / (ionize_temps[i_v][4] - ionize_temps[i_v][3]);

						rate = rate1*ppn_low + rate2*ppn_high;
	

						printf("i_v %d Te %1.8E b_exp[1] %d b_exp[2] %d 3-4 -> 1-2 rate %1.8E coeffs[2][0] \n", i_v, Te,
							(b_exp[1] ? 1 : 0), (b_exp[2] ? 1 : 0), rate, ionize_coeffs[i_v][2][0]);

					} else {
						if (Te < ionize_temps[i_v][5]) {
							rate = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
								+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
								+ ionize_coeffs[i_v][2][0] * Te4);
							if (b_exp[2]) rate = exp(rate);

							printf("i_v %d Te %1.8E b_exp[2] %d b_exp[3] %d 4-5 rate %1.8E coeffs[3][0] \n", i_v, Te,
								(b_exp[2] ? 1 : 0), (b_exp[3] ? 1 : 0), rate, ionize_coeffs[i_v][3][0]);
						} else {
							if (Te < ionize_temps[i_v][6]) {
								rate1 = (ionize_coeffs[i_v][2][4] + ionize_coeffs[i_v][2][3] * TeeV
									+ ionize_coeffs[i_v][2][2] * Tesq + ionize_coeffs[i_v][2][1] * Te3
									+ ionize_coeffs[i_v][2][0] * Te4);
							rate2 = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
								+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
								+ ionize_coeffs[i_v][3][0] * Te4);

							if (b_exp[2]) rate1 = exp(rate1);
							if (b_exp[3]) rate2 = exp(rate2);

							f64 ppn_high = (Te - ionize_temps[i_v][5]) / (ionize_temps[i_v][6] - ionize_temps[i_v][5]);
							f64 ppn_low = (ionize_temps[i_v][6] - Te) / (ionize_temps[i_v][6] - ionize_temps[i_v][5]);

							rate = rate1*ppn_low + rate2*ppn_high;

							printf("i_v %d Te %1.8E b_exp[2] %d b_exp[3] %d 5-6-> 2-3 rate %1.8E coeffs[3][0] \n", i_v, Te,
								(b_exp[2] ? 1 : 0), (b_exp[3] ? 1 : 0), rate, ionize_coeffs[i_v][3][0]);

							} else {
								if (Te < ionize_temps[i_v][7]) {
									rate = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
										+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
										+ ionize_coeffs[i_v][3][0] * Te4);
									if (b_exp[3]) rate = exp(rate); // it is always false anyway								

									printf("i_v %d Te %1.8E b_exp[3] %d b_exp[3] %d 6-7 rate %1.8E coeffs[3][0] \n", i_v, Te,
										(b_exp[3] ? 1 : 0), (b_exp[3] ? 1 : 0), rate, ionize_coeffs[i_v][3][0]);

								} else {
									if (Te < ionize_temps[i_v][8]) {

										rate1 = (ionize_coeffs[i_v][3][4] + ionize_coeffs[i_v][3][3] * TeeV
											+ ionize_coeffs[i_v][3][2] * Tesq + ionize_coeffs[i_v][3][1] * Te3
											+ ionize_coeffs[i_v][3][0] * Te4);
										if (b_exp[3]) rate1 = exp(rate1); // it is always false anyway								
										rate2 = (ionize_coeffs[i_v][4][4] + ionize_coeffs[i_v][4][3] * TeeV
											+ ionize_coeffs[i_v][4][2] * Tesq + ionize_coeffs[i_v][4][1] * Te3
											+ ionize_coeffs[i_v][4][0] * Te4);
										if (b_exp[4]) rate2 = exp(rate2); // it is always false anyway								
										f64 ppn_high = (Te - ionize_temps[i_v][7]) / (ionize_temps[i_v][8] - ionize_temps[i_v][7]);
										f64 ppn_low = (ionize_temps[i_v][8] - Te) / (ionize_temps[i_v][8] - ionize_temps[i_v][7]);

										rate = rate1*ppn_low + rate2*ppn_high;

										printf("i_v %d Te %1.8E b_exp[3] %d b_exp[4] %d 7-8 rate %1.8E coeffs[4][0] \n", i_v, Te,
											(b_exp[3] ? 1 : 0), (b_exp[4] ? 1 : 0), rate, ionize_coeffs[i_v][4][0]);

									} else {

										rate = (ionize_coeffs[i_v][4][4] + ionize_coeffs[i_v][4][3] * TeeV
											+ ionize_coeffs[i_v][4][2] * Tesq + ionize_coeffs[i_v][4][1] * Te3
											+ ionize_coeffs[i_v][4][0] * Te4);
										if (b_exp[4]) rate = exp(rate); // it is always false anyway				

										printf("i_v %d Te %1.8E  b_exp[4] %d 8 -> 4 rate %1.8E coeffs[4][0] \n", i_v, Te,
											(b_exp[4] ? 1 : 0), rate, ionize_coeffs[i_v][4][0]);

									};
								}
							};
						};
					};
				};
			};
		};
	};
	return rate;
}

__device__ f64 GetIonizationRates(f64 const Te, f64 const v, f64 * p_Recombo_rate)
{
	int i_vleft, i_vright;
	f64 vleft, vright;
	f64 ppn_right, ppn_left;
	if (v < 1.0e7) {
		i_vleft = (int)(v / 2.0e6);
		i_vright = i_vleft + 1;
		vleft = 2.0e6*(double)(i_vleft);
		vright = 2.0e6*(double)(i_vright); // at most 1e7	
		ppn_right = (v - vleft) / (vright - vleft);
		ppn_left = (vright - v) / (vright - vleft);
	} else {
		if (v > 2.7e8) {
			i_vleft = 31;
			i_vright = 31;
			ppn_left = 1.0;
			ppn_right = 0.0;
			vleft = 1.0e7*(double)(i_vleft - 4);
			vright = 1.0e7*(double)(i_vright - 4); // careful	

		} else {
			i_vleft = 4 + (int)(v / 1.0e7); // cHeck ?
			if (i_vleft >= 31) i_vleft = 30;
			i_vright = i_vleft + 1;
			if (i_vright >= 32) i_vright = 31;

			vleft = 1.0e7*(double)(i_vleft - 4);
			vright = 1.0e7*(double)(i_vright - 4); // careful	
			ppn_right = (v - vleft) / (vright - vleft);
			ppn_left = (vright - v) / (vright - vleft);
		};
	};

	f64 rate_left = GetIonizationRate_given_v(Te, i_vleft);
	f64 rate_right = GetIonizationRate_given_v(Te, i_vright);
	f64 recomb_rate_left = GetRecombinationRate_given_v(Te, i_vleft);
	f64 recomb_rate_right = GetRecombinationRate_given_v(Te, i_vright);
	
	// now we need to go again given which v column to another function!
	f64 rate = rate_left*ppn_left + rate_right*ppn_right;
	*p_Recombo_rate = recomb_rate_left*ppn_left + recomb_rate_right*ppn_right;
	return rate;
}


__device__ f64 GetIonizationRatesDebug(f64 const Te, f64 const v, f64 * p_Recombo_rate)
{
	int i_vleft, i_vright;
	f64 vleft, vright;
	f64 ppn_right, ppn_left;
	if (v < 1.0e7) {
		i_vleft = (int)(v / 2.0e6);
		i_vright = i_vleft + 1;
		vleft = 2.0e6*(double)(i_vleft);
		vright = 2.0e6*(double)(i_vright); // at most 1e7	
		ppn_right = (v - vleft) / (vright - vleft);
		ppn_left = (vright - v) / (vright - vleft);

		printf("GIRD small v: vleft %1.8E vright %1.8E ppn %1.8E %1.8E \n",
			vleft, vright, ppn_left, ppn_right);
	}
	else {
		if (v > 2.7e8) {
			i_vleft = 31;
			i_vright = 31;
			ppn_left = 1.0;
			ppn_right = 0.0;
			vleft = 1.0e7*(double)(i_vleft - 4);
			vright = 1.0e7*(double)(i_vright - 4); // careful	

			printf("GIRD high v: vleft %1.8E vright %1.8E ppn %1.8E %1.8E \n",
				vleft, vright, ppn_left, ppn_right);
		}
		else {
			i_vleft = 4 + (int)(v / 1.0e7); // cHeck ?
			if (i_vleft >= 31) i_vleft = 30;
			i_vright = i_vleft + 1;

			vleft = 1.0e7*(double)(i_vleft - 4);
			vright = 1.0e7*(double)(i_vright - 4); // careful	
			ppn_right = (v - vleft) / (vright - vleft);
			ppn_left = (vright - v) / (vright - vleft);

			printf("GIRD moderate v: vleft %1.8E vright %1.8E ppn %1.8E %1.8E \n",
				vleft, vright, ppn_left, ppn_right);
		};
	};

	f64 rate_left = GetIonizationRate_given_v_Debug(Te, i_vleft);
	f64 rate_right = GetIonizationRate_given_v_Debug(Te, i_vright);
	f64 recomb_rate_left = GetRecombinationRate_given_v(Te, i_vleft);
	f64 recomb_rate_right = GetRecombinationRate_given_v(Te, i_vright);

	printf("GIRD : rate_left %1.8E rate_right %1.8E \n", rate_left, rate_right);

	// now we need to go again given which v column to another function!
	f64 rate = rate_left*ppn_left + rate_right*ppn_right;
	*p_Recombo_rate = recomb_rate_left*ppn_left + recomb_rate_right*ppn_right;
	return rate;
}


__global__ void kernelCompare(
	f64_vec2 * __restrict__ p_epsxy,
	f64 * __restrict__ p_epsiz,
	f64 * __restrict__ p_epsez,
	f64_vec2 * __restrict__ p_epsxyp,
	f64 * __restrict__ p_epsizp,
	f64 * __restrict__ p_epsezp,
	f64 * __restrict__ p_distance
	)
{
	long const index = blockDim.x*blockIdx.x + threadIdx.x;
	f64_vec2 eps1 = p_epsxy[index];
	f64_vec2 eps2 = p_epsxyp[index];
	f64 diff = eps1.x - eps2.x;
	
	f64 epsez = p_epsez[index];
	f64 epsezp = p_epsezp[index];
	diff = epsez - epsezp;
	
	if (index == CHOSEN) printf("%d epsez %1.14E pred %1.14E diff %1.8E ppn %1.4E\n",
		index, epsez, epsezp, diff, diff/epsez);
	
	p_distance[index] = fabs(diff/(fabs(epsez)+1.0e+2)); 

	/*if ((fabs(diff) > 1.0e-10) && (fabs(diff) > fabs(1.0e-8*eps2.x))) printf("%d x dimension : %1.10E %1.10E\n", index,
		eps1.x, eps2.x);
	diff = eps1.y - eps2.y;
	if ((fabs(diff) > 1.0e-10) && (fabs(diff) > 1.0e-8*fabs(eps2.y))) printf("%d y dimension : %1.10E %1.10E \n", index,
		eps1.y, eps2.y);
	f64 epsiz = p_epsiz[index];
	f64 epsizp = p_epsizp[index];
	diff = epsiz - epsizp;
	if ((fabs(diff) > 1.0e-10) && (fabs(diff) > 1.0e-8*fabs(epsizp))) printf("%d iz : %1.10E %1.10E \n", index,
		epsiz, epsizp);
	f64 epsez = p_epsez[index];
	f64 epsezp = p_epsezp[index];
	diff = epsez - epsezp;
	if ((fabs(diff) > 1.0e-10) && (fabs(diff) > 1.0e-8*fabs(epsezp))) printf("%d ez : %1.10E %1.10E \n", index,
		epsez, epsezp);*/

	// we do not want it to null out the routine
	p_epsxy[index] = eps1;
	p_epsez[index] = epsez;
}




__global__ void kernelSplitIntoSeedRegressors
(
	v4 * __restrict__ p_move,
	f64_vec3 * __restrict__ p_regr_i,
	f64_vec3 * __restrict__ p_regr_e,
	f64_vec2 * __restrict__ p_epsxy
) {
	long const iMinor = threadIdx.x + blockIdx.x*blockDim.x;

	v4 v = p_move[iMinor];
	// f64_vec2 eps = p_epsxy[iMinor];
	// leave epsilon out for now - would serve as multiplying factor.

	f64_vec3 regr_i, regr_e;
	regr_e.x = 0.0; regr_e.y = 0.0;
	regr_i.x = v.vxy.x; regr_i.y = v.vxy.y;
	regr_i.z = v.viz;
	regr_e.z = v.vez;

	p_regr_i[iMinor] = regr_i;
	p_regr_e[iMinor] = regr_e;
}


__global__ void kernelCalcJacobi_Viscosity(

	structural * __restrict__ p_info_minor,
	
	f64_vec2 * __restrict__ p_epsilon_xy,
	f64 * __restrict__ p_epsilon_iz,
	f64 * __restrict__ p_epsilon_ez,

	f64_tens3 * __restrict__ p_matrix_i,
	f64_tens3 * __restrict__ p_matrix_e, // inverted matrix R^-1 so Jacobi = R^-1 epsilon
	
	f64_vec3 * __restrict__ p_Jacobi_i,
	f64_vec3 * __restrict__ p_Jacobi_e)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	structural info = p_info_minor[iMinor];
	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {
		f64_vec2 epsilon_xy = p_epsilon_xy[iMinor];
		f64 epsilon_iz = p_epsilon_iz[iMinor];
		f64 epsilon_ez = p_epsilon_ez[iMinor];

		f64_tens3 matrix = p_matrix_i[iMinor];
		p_Jacobi_i[iMinor] = matrix*Make3(epsilon_xy, epsilon_iz);
		matrix = p_matrix_e[iMinor];
		p_Jacobi_e[iMinor] = matrix*Make3(epsilon_xy, epsilon_ez);

	} else {
		memset(&(p_Jacobi_i[iMinor]), 0, sizeof(f64_vec3));
		memset(&(p_Jacobi_e[iMinor]), 0, sizeof(f64_vec3));
	}
	
	if (threadIdx.x < threadsPerTileMajor) {
		
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		if ((info.flag == DOMAIN_VERTEX)) {
			f64_vec2 epsilon_xy = p_epsilon_xy[iVertex + BEGINNING_OF_CENTRAL];
			f64 epsilon_iz = p_epsilon_iz[iVertex + BEGINNING_OF_CENTRAL];
			f64 epsilon_ez = p_epsilon_ez[iVertex + BEGINNING_OF_CENTRAL];

			f64_tens3 matrix = p_matrix_i[iVertex + BEGINNING_OF_CENTRAL];
			p_Jacobi_i[iVertex + BEGINNING_OF_CENTRAL] = matrix*Make3(epsilon_xy, epsilon_iz);
			matrix = p_matrix_e[iVertex + BEGINNING_OF_CENTRAL];
			p_Jacobi_e[iVertex + BEGINNING_OF_CENTRAL] = matrix*Make3(epsilon_xy, epsilon_ez);

		} else {
			memset(&(p_Jacobi_i[iVertex + BEGINNING_OF_CENTRAL]), 0, sizeof(f64_vec3));
			memset(&(p_Jacobi_e[iVertex + BEGINNING_OF_CENTRAL]), 0, sizeof(f64_vec3));
		};
	}
}

__global__ void kernelCalc_Matrices_for_Jacobi_Viscosity(
	f64 const hsub,
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_parallel_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_ita_parallel_elec_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_elec_minor,   // nT / nu ready to look up
	f64_vec3 * __restrict__ p_B_minor,
		
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_AreaMinor,

	f64_tens3 * __restrict__ p_matrix_i,
	f64_tens3 * __restrict__ p_matrix_e
	)
{
	//__shared__ v4 shared_vie[threadsPerTileMinor]; // sort of thing we want as input
	// Not used, right? Nothing nonlinear?
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_B[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];

	//__shared__ v4 shared_vie_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_B_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

														 // 4+2+2+1+1 *1.5 = 15 per thread. That is possibly as slow as having 24 per thread. 
														 // Thus putting some stuff in shared may speed up if there are spills.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64_vec2 opppos, prevpos, nextpos;
	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_B[threadIdx.x] = p_B_minor[iMinor].xypart();
	shared_ita_par[threadIdx.x] = p_ita_parallel_ion_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_ion_minor[iMinor];

	// Perhaps the real answer is this. Advection and therefore advective momflux
	// do not need to be recalculated very often at all. At 1e6 cm/s, we aim for 1 micron,
	// get 1e-10s to actually do the advection !!
	// So an outer cycle. Still limiting the number of total things in a minor tile. We might like 384 = 192*2.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_B_verts[threadIdx.x] = p_B_minor[iVertex + BEGINNING_OF_CENTRAL].xypart();
		if ((info.flag == DOMAIN_VERTEX))
		{
		//	memcpy(&(shared_vie_verts[threadIdx.x]), &(p_vie_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(v4));
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
		//	memset(&(shared_vie_verts[threadIdx.x]), 0, sizeof(v4));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	f64_vec2 cc0, cc1;

	if (threadIdx.x < threadsPerTileMajor) {

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!

		if (info.flag == DOMAIN_VERTEX) 
			//|| (info.flag == OUTERMOST)) // !!!!!!!!!!!!!!!!
		{
			// We are losing energy if there is viscosity into OUTERMOST.
			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

			f64_tens3 J; // Jacobean
			memset(&J, 0, sizeof(f64_tens3));
			//d_eps_x_by_d_vx = 1.0;
			J.xx = 1.0;
			J.yy = 1.0;
			J.zz = 1.0;
			// d_eps_z_by_d_viz = 1.0;  // Note that eps includes v_k+1

			if (shared_ita_par_verts[threadIdx.x] > 0.0) {
				short iprev = tri_len - 1;
				short i = 0;
				short inext;
				if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
				{
					prevpos = shared_pos[izTri[iprev] - StartMinor];
				}
				else {
					prevpos = p_info_minor[izTri[iprev]].pos;
				}
				if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
					prevpos = Clockwise_d*prevpos;
				}
				if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
					prevpos = Anticlockwise_d*prevpos;
				}

				if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
				{
					opppos = shared_pos[izTri[i] - StartMinor];
				}
				else {
					opppos = p_info_minor[izTri[i]].pos;
				}
				if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
					opppos = Clockwise_d*opppos;
				}
				if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
					opppos = Anticlockwise_d*opppos;
				}

				f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
				f64_vec3 omega_ci;

				// ** Be especially vigilant to the changes we need to make to go from ion to electron.
#pragma unroll 
				for (i = 0; i < tri_len; i++)
				{
					// Tri 0 is anticlockwise of neighbour 0, we think
					inext = i + 1; if (inext >= tri_len) inext = 0;

					// Now sort out anticlock vars:
					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
					}

					// Order of calculations may help things to go out/into scope at the right times so careful with that.

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					//gradvy.y = -0.5*(
					//	(our_v.vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
					//	+ (prev_v.vxy.y + our_v.vxy.y)*(prevpos.x - info.pos.x)
					//	+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
					//	+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
					//	) / area_quadrilateral;
					//
					// so we want to know, eps += U v_self for U 4x4

					f64 grad_vjdx_coeff_on_vj_self = 0.5*(prevpos.y - nextpos.y) / area_quadrilateral;
					f64 grad_vjdy_coeff_on_vj_self = 0.5*(nextpos.x - prevpos.x) / area_quadrilateral;

					f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
					{
						f64_vec2 opp_B;
						if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
						{
							opp_B = shared_B[izTri[i] - StartMinor];
							if (shared_ita_par_verts[threadIdx.x] < shared_ita_par[izTri[i] - StartMinor])
							{
								ita_par = shared_ita_par_verts[threadIdx.x];
								nu = shared_nu_verts[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par[izTri[i] - StartMinor];
								nu = shared_nu[izTri[i] - StartMinor];
							};
						}
						else {
							opp_B = p_B_minor[izTri[i]].xypart();
							f64 ita_theirs = p_ita_parallel_ion_minor[izTri[i]];
							f64 nu_theirs = p_nu_ion_minor[izTri[i]];
							if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
								ita_par = shared_ita_par_verts[threadIdx.x];
								nu = shared_nu_verts[threadIdx.x];
							}
							else {
								ita_par = ita_theirs;
								nu = nu_theirs;
							};
						}
						if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
							opp_B = Clockwise_d*opp_B;
						}
						if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
							opp_B = Anticlockwise_d*opp_B;
						}
						omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
					} // Guaranteed DOMAIN_VERTEX never needs to skip an edge; we include CROSSING_INS in viscosity.

					f64_vec2 edge_normal;
					edge_normal.x = endpt1.y - endpt0.y;
					edge_normal.y = endpt0.x - endpt1.x;

					if (ita_par > 0.0) {
						Augment_Jacobean(&J,
							hsub / (p_n_minor[iVertex + BEGINNING_OF_CENTRAL].n*p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] * m_ion),
							edge_normal, ita_par, nu, omega_ci,
							grad_vjdx_coeff_on_vj_self,
							grad_vjdy_coeff_on_vj_self
						);
					};

					endpt0 = endpt1;
					prevpos = opppos;
					opppos = nextpos;
				}; // next i
			}; // ita_par > 0

			f64_tens3 result;
			J.Inverse(result);

			memcpy(&(p_matrix_i[iVertex + BEGINNING_OF_CENTRAL]), &result, sizeof(f64_tens3));
			  // inverted it so that we are ready to put Jacobi = result.eps
			
		} else {
			// NOT domain vertex: Do nothing			

			// NOTE: We did not include OUTERMOST. Justification / effect ??
		};
	};
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	info = p_info_minor[iMinor];

	//if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	{
		long izNeighMinor[6];
		char szPBC[6];

		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
			
			f64_tens3 J; // Jacobean
			memset(&J, 0, sizeof(f64_tens3));
			//d_eps_x_by_d_vx = 1.0;
			J.xx = 1.0;
			J.yy = 1.0;
			J.zz = 1.0;
			
			if (shared_ita_par[threadIdx.x] > 0.0) {
				short inext, iprev = 5, i = 0;
				if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
				{
					prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				}
				else {
					if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					};
				};
				if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
					prevpos = Clockwise_d*prevpos;
				}
				if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
					prevpos = Anticlockwise_d*prevpos;
				}

				i = 0;
				if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
				{
					opppos = shared_pos[izNeighMinor[i] - StartMinor];
				}
				else {
					if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						opppos = p_info_minor[izNeighMinor[i]].pos;
					};
				};
				if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
					opppos = Clockwise_d*opppos;
				}
				if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
					opppos = Anticlockwise_d*opppos;
				}

				f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
				f64_vec3 omega_ci;
				// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
				for (i = 0; i < 6; i++)
				{
					inext = i + 1; if (inext > 5) inext = 0;

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
					}

					//	nu = 1.0e10; // DEBUG
					bool bUsableSide = true;
					{
						f64_vec2 opp_B(0.0, 0.0);

						if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
						{
							opp_B = shared_B[izNeighMinor[i] - StartMinor];
							if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
								nu = shared_nu[izNeighMinor[i] - StartMinor];
							};

							if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
						}
						else {
							if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
								(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
							{
								opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
								{
									ita_par = shared_ita_par[threadIdx.x];
									nu = shared_nu[threadIdx.x];
								}
								else {
									ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
									nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								};

								if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
							}
							else {
								opp_B = p_B_minor[izNeighMinor[i]].xypart();
								f64 ita_par_opp = p_ita_parallel_ion_minor[izNeighMinor[i]];
								f64 nu_theirs = p_nu_ion_minor[izNeighMinor[i]];
								if (shared_ita_par[threadIdx.x] < ita_par_opp) {
									ita_par = shared_ita_par[threadIdx.x];
									nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
								}
								else {
									ita_par = ita_par_opp;
									nu = nu_theirs; // Did I know we were doing this? We use the MINIMUM ita ?

									// . We should probably stop that.
								}

								if (ita_par_opp == 0.0) bUsableSide = false;
							}
						}
						if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
							opp_B = Clockwise_d*opp_B;
						}
						if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
							opp_B = Anticlockwise_d*opp_B;
						}
						omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
					}

					f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);

					f64_vec2 edge_normal;
					edge_normal.x = endpt1.y - endpt0.y;
					edge_normal.y = endpt0.x - endpt1.x;

					if (bUsableSide) {

						// New definition of endpoint of minor edge:

						f64 area_quadrilateral = 0.5*(
							(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
							+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
							+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
							+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
							);

						f64 grad_vjdx_coeff_on_vj_self = 0.5*(prevpos.y - nextpos.y) / area_quadrilateral;
						f64 grad_vjdy_coeff_on_vj_self = 0.5*(nextpos.x - prevpos.x) / area_quadrilateral;


						if (info.flag == CROSSING_INS) {
							char flag = p_info_minor[izNeighMinor[i]].flag;
							if (flag == CROSSING_INS) {

								// can't do prev_v == 0.0
								// have to see if prev pos inside ins.

								if (prevpos.dot(prevpos) < DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER) // prev is in the insulator.
								{
									f64 area_triangle = 0.5*(
										(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
										+ (opppos.x + info.pos.x)*(opppos.y - info.pos.y)
										+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y));
									grad_vjdx_coeff_on_vj_self = 0.5*((info.pos.y - nextpos.y)+ (opppos.y - info.pos.y)) / area_triangle;
									grad_vjdy_coeff_on_vj_self = -0.5*((info.pos.x - nextpos.x)+ (opppos.x - info.pos.x)) / area_triangle;
								}
								else {
									if (nextpos.dot(nextpos) < DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER) // prev is in the insulator.
									{
										f64 area_triangle = 0.5*(
											(prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
											+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
											+ (info.pos.x + opppos.x)*(info.pos.y - opppos.y)
											);
										grad_vjdx_coeff_on_vj_self = 0.5*(
											(prevpos.y - info.pos.y)
											+ (info.pos.y - opppos.y) // nextpos = pos_anti, assumed
											) / area_triangle;
										grad_vjdy_coeff_on_vj_self = -0.5*(
											(prevpos.x - info.pos.x)											
											+ (info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;
									};
								};
							};
						};

						Augment_Jacobean(&J,
							hsub / (p_n_minor[iMinor].n * p_AreaMinor[iMinor] * m_ion),
							edge_normal, ita_par, nu, omega_ci,
							grad_vjdx_coeff_on_vj_self,
							grad_vjdy_coeff_on_vj_self
						);
						
					}

					endpt0 = endpt1;
					prevpos = opppos;
					opppos = nextpos;
				};
			}; // ita_par > 0.0

			f64_tens3 result;
			J.Inverse(result);
			memcpy(&(p_matrix_i[iMinor]), &result, sizeof(f64_tens3));
			
		} else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		}
	} // scope

	__syncthreads();

	// Now do electron: overwrite ita and nu, copy-paste the above codes very carefully
	shared_ita_par[threadIdx.x] = p_ita_parallel_elec_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_elec_minor[iMinor];

	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

		if ((info.flag == DOMAIN_VERTEX))  // keeping consistent with ion above where we did put OUTERMOST here
		{// but we set ita to 0 in the pre routine for outermost.
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
		}
		else {
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	if (threadIdx.x < threadsPerTileMajor) {
		
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len; // ?!
		
		if ((info.flag == DOMAIN_VERTEX))
			//|| (info.flag == OUTERMOST)) 
		{
			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

			f64_tens3 J; // Jacobean
			memset(&J, 0, sizeof(f64_tens3));
			//d_eps_x_by_d_vx = 1.0;
			J.xx = 1.0;
			J.yy = 1.0;
			J.zz = 1.0;

			if (shared_ita_par_verts[threadIdx.x] > 0.0) {
				short iprev = tri_len - 1;
				short i = 0;
				short inext;
				if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
				{
					prevpos = shared_pos[izTri[iprev] - StartMinor];
				}
				else {
					prevpos = p_info_minor[izTri[iprev]].pos;
				}
				if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
					prevpos = Clockwise_d*prevpos;
				}
				if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
					prevpos = Anticlockwise_d*prevpos;
				}

				if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
				{
					opppos = shared_pos[izTri[i] - StartMinor];
				}
				else {
					opppos = p_info_minor[izTri[i]].pos;
				}
				if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
					opppos = Clockwise_d*opppos;
				}
				if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
					opppos = Anticlockwise_d*opppos;
				}

				f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
				f64_vec3 omega_ce;
#pragma unroll 
				for (i = 0; i < tri_len; i++)
				{
					// Tri 0 is anticlockwise of neighbour 0, we think
					inext = i + 1; if (inext >= tri_len) inext = 0;

					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
					}
					// All same as ion here:

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					f64 grad_vjdx_coeff_on_vj_self = 0.5*(prevpos.y - nextpos.y) / area_quadrilateral;
					f64 grad_vjdy_coeff_on_vj_self = 0.5*(nextpos.x - prevpos.x) / area_quadrilateral;

					{
						f64_vec2 opp_B;
						f64 opp_ita, opp_nu;
						if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
						{
							opp_B = shared_B[izTri[i] - StartMinor];
							opp_ita = shared_ita_par[izTri[i] - StartMinor];
							opp_nu = shared_nu[izTri[i] - StartMinor];
							//ita_par = 0.5*(shared_ita_par_verts[threadIdx.x] + shared_ita_par[izTri[i] - StartMinor]);
							//nu = 0.5*(shared_nu_verts[threadIdx.x] + shared_nu[izTri[i] - StartMinor]);
						}
						else {
							opp_B = p_B_minor[izTri[i]].xypart();
							opp_ita = p_ita_parallel_elec_minor[izTri[i]];
							opp_nu = p_nu_elec_minor[izTri[i]];
						}
						if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
							opp_B = Clockwise_d*opp_B;
						}
						if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
							opp_B = Anticlockwise_d*opp_B;
						}
						if (shared_ita_par_verts[threadIdx.x] < opp_ita) {
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = opp_ita;
							nu = opp_nu;
						}
						omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
					}

					f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
					f64_vec2 edge_normal;
					edge_normal.x = endpt1.y - endpt0.y;
					edge_normal.y = endpt0.x - endpt1.x;

					if (ita_par > 0.0)
						Augment_Jacobean(&J,
							hsub / (p_n_minor[iVertex + BEGINNING_OF_CENTRAL].n * p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] * m_e),
							edge_normal, ita_par, nu, omega_ce,
							grad_vjdx_coeff_on_vj_self,
							grad_vjdy_coeff_on_vj_self
						);

					endpt0 = endpt1;
					prevpos = opppos;
					opppos = nextpos;
				}; // next i
			}; // ita_par > 0

			f64_tens3 result;
			J.Inverse(result);
			memcpy(&(p_matrix_e[iVertex + BEGINNING_OF_CENTRAL]), &result, sizeof(f64_tens3));
		} else {
			// NOT domain vertex: Do nothing			
		};
	};

	// Electrons in tris:
	info = p_info_minor[iMinor];
	long izNeighMinor[6];
	char szPBC[6];
	
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	}
	else {
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
			
			f64_tens3 J; // Jacobean
			memset(&J, 0, sizeof(f64_tens3));
			//d_eps_x_by_d_vx = 1.0;
			J.xx = 1.0;
			J.yy = 1.0;
			J.zz = 1.0;
			
			if (shared_ita_par[threadIdx.x] > 0.0) {
				short inext, iprev = 5, i = 0;
				if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
				{
					prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				}
				else {
					if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					};
				};
				if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
					prevpos = Clockwise_d*prevpos;
				}
				if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
					prevpos = Anticlockwise_d*prevpos;
				}

				i = 0;
				if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
				{
					opppos = shared_pos[izNeighMinor[i] - StartMinor];
				}
				else {
					if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						opppos = p_info_minor[izNeighMinor[i]].pos;
					};
				};
				if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
					opppos = Clockwise_d*opppos;
				}
				if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
					opppos = Anticlockwise_d*opppos;
				}

				f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
				f64_vec3 omega_ce;
				// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
				for (i = 0; i < 6; i++)
				{
					inext = i + 1; if (inext > 5) inext = 0;

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
					}

					bool bUsableSide = true;
					{
						f64_vec2 opp_B;
						f64 opp_ita, opp_nu;
						if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
						{
							opp_B = shared_B[izNeighMinor[i] - StartMinor];
							opp_ita = shared_ita_par[izNeighMinor[i] - StartMinor];
							opp_nu = shared_nu[izNeighMinor[i] - StartMinor];
							if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
						}
						else {
							if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
								(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
							{
								opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								opp_ita = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								opp_nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
							}
							else {
								opp_B = p_B_minor[izNeighMinor[i]].xypart();
								opp_ita = p_ita_parallel_elec_minor[izNeighMinor[i]];
								opp_nu = p_nu_elec_minor[izNeighMinor[i]];
								if (opp_ita == 0.0) bUsableSide = false;
							}
						}
						if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
							opp_B = Clockwise_d*opp_B;
						}
						if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
							opp_B = Anticlockwise_d*opp_B;
						}
						if (shared_ita_par[threadIdx.x] < opp_ita) {
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						}
						else {
							ita_par = opp_ita;
							nu = opp_nu;
						}
						omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
					}

					f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
					f64_vec2 edge_normal;
					edge_normal.x = endpt1.y - endpt0.y;
					edge_normal.y = endpt0.x - endpt1.x;

					if (bUsableSide) {
						// New definition of endpoint of minor edge:

						f64 area_quadrilateral = 0.5*(
							(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
							+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
							+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
							+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
							);

						f64 grad_vjdx_coeff_on_vj_self = 0.5*(prevpos.y - nextpos.y) / area_quadrilateral;
						f64 grad_vjdy_coeff_on_vj_self = 0.5*(nextpos.x - prevpos.x) / area_quadrilateral;

						if (info.flag == CROSSING_INS) {
							char flag = p_info_minor[izNeighMinor[i]].flag;
							if (flag == CROSSING_INS) {

								// can't do prev_v == 0.0
								// have to see if prev pos inside ins.

								if (prevpos.dot(prevpos) < DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER) // prev is in the insulator.
								{
									f64 area_triangle = 0.5*(
										(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
										+ (opppos.x + info.pos.x)*(opppos.y - info.pos.y)
										+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y));
									grad_vjdx_coeff_on_vj_self = 0.5*((info.pos.y - nextpos.y) + (opppos.y - info.pos.y)) / area_triangle;
									grad_vjdy_coeff_on_vj_self = -0.5*((info.pos.x - nextpos.x) + (opppos.x - info.pos.x)) / area_triangle;
								}
								else {
									if (nextpos.dot(nextpos) < DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER) // prev is in the insulator.
									{
										f64 area_triangle = 0.5*(
											(prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
											+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
											+ (info.pos.x + opppos.x)*(info.pos.y - opppos.y)
											);
										grad_vjdx_coeff_on_vj_self = 0.5*(
											(prevpos.y - info.pos.y)
											+ (info.pos.y - opppos.y) // nextpos = pos_anti, assumed
											) / area_triangle;
										grad_vjdy_coeff_on_vj_self = -0.5*(
											(prevpos.x - info.pos.x)
											+ (info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;
									};
								};
							};
						};
						Augment_Jacobean(&J,
							hsub / (p_n_minor[iMinor].n * p_AreaMinor[iMinor] * m_e),
							edge_normal, ita_par, nu, omega_ce,
							grad_vjdx_coeff_on_vj_self,
							grad_vjdy_coeff_on_vj_self
						);
					};

					endpt0 = endpt1;
					prevpos = opppos;
					opppos = nextpos;
				};
			}; // ita_par > 0.0

			f64_tens3 result;
			J.Inverse(result);
			memcpy(&(p_matrix_e[iMinor]), &result, sizeof(f64_tens3));
		}
		else {
			// Not domain, not crossing_ins, not a frill			
		} // non-domain tri
	}; // was it FRILL
}



__global__ void kernelAverage_n_T_x_to_tris(
	nvals * __restrict__ p_n_minor,
	nvals * __restrict__ p_n_major,
	T3 * __restrict__ p_T_minor,
	structural * __restrict__ p_info,
	f64_vec2 * __restrict__ p_cc,

	LONG3 * __restrict__ p_tri_corner_index,
	CHAR4 * __restrict__ p_tri_periodic_corner_flags,

	bool bCalculateOnCircumcenters
)
{
	__shared__ nvals shared_n[threadsPerTileMajor];
	__shared__ T3 shared_T[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMajor];

	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; // iMinor OF VERTEX
	if (threadIdx.x < threadsPerTileMajor)
	{
		long getindex = blockIdx.x * threadsPerTileMajor + threadIdx.x;
		shared_n[threadIdx.x] = p_n_major[getindex];
		shared_T[threadIdx.x] = p_T_minor[BEGINNING_OF_CENTRAL + getindex];
		shared_pos[threadIdx.x] = p_info[BEGINNING_OF_CENTRAL + getindex].pos;
	};
	long const StartMajor = blockIdx.x*threadsPerTileMajor; // vertex iMinor
	long const EndMajor = StartMajor + threadsPerTileMajor;
	LONG3 const tri_corner_index = p_tri_corner_index[iMinor];
	CHAR4 const tri_corner_per_flag = p_tri_periodic_corner_flags[iMinor];
	structural info = p_info[iMinor];

	__syncthreads();

	T3 T(0.0, 0.0, 0.0);
	nvals n(0.0, 0.0);
	f64_vec2 pos(0.0, 0.0);
	f64_vec2 cc(0.0, 0.0);
	// New plan for this routine: go through position code for all cases except frills.
	// Then compute averaging coefficients for domain and crossing_ins, and use them.

	// 
	n.n = 0.0;
	n.n_n = 0.0;
	T.Te = 0.0; T.Ti = 0.0; T.Tn = 0.0;

	f64_vec2 poscorner0, poscorner1, poscorner2;
	if ((tri_corner_index.i1 >= StartMajor) && (tri_corner_index.i1 < EndMajor))
	{
		poscorner0 = shared_pos[tri_corner_index.i1 - StartMajor];
	} else {
		poscorner0 = p_info[tri_corner_index.i1 + BEGINNING_OF_CENTRAL].pos;
	};
	if (tri_corner_per_flag.per0 == ROTATE_ME_CLOCKWISE) poscorner0 = Clockwise_d*poscorner0;
	if (tri_corner_per_flag.per0 == ROTATE_ME_ANTICLOCKWISE) poscorner0 = Anticlockwise_d*poscorner0;
	
	if ((tri_corner_index.i2 >= StartMajor) && (tri_corner_index.i2 < EndMajor))
	{
		poscorner1 = shared_pos[tri_corner_index.i2 - StartMajor];
	} else {
		poscorner1 = p_info[tri_corner_index.i2 + BEGINNING_OF_CENTRAL].pos;
	};
	if (tri_corner_per_flag.per1 == ROTATE_ME_CLOCKWISE) poscorner1 = Clockwise_d*poscorner1;
	if (tri_corner_per_flag.per1 == ROTATE_ME_ANTICLOCKWISE) poscorner1 = Anticlockwise_d*poscorner1;
	
	if ((info.flag != INNER_FRILL) && (info.flag != OUTER_FRILL))
	{
		if ((tri_corner_index.i3 >= StartMajor) && (tri_corner_index.i3 < EndMajor))
		{
			poscorner2 = shared_pos[tri_corner_index.i3 - StartMajor];
		} else {
			poscorner2 = p_info[tri_corner_index.i3 + BEGINNING_OF_CENTRAL].pos;
		};
		if (tri_corner_per_flag.per2 == ROTATE_ME_CLOCKWISE) poscorner2 = Clockwise_d*poscorner2;
		if (tri_corner_per_flag.per2 == ROTATE_ME_ANTICLOCKWISE) poscorner2 = Anticlockwise_d*poscorner2;
		
		f64_vec2 Bb = poscorner1 - poscorner0;
		f64_vec2 C = poscorner2 - poscorner0;
		f64 D = 2.0*(Bb.x*C.y - Bb.y*C.x);
		f64 modB = Bb.x*Bb.x + Bb.y*Bb.y;
		f64 modC = C.x*C.x + C.y*C.y;
		cc.x = (C.y*modB - Bb.y*modC) / D + poscorner0.x;
		cc.y = (Bb.x*modC - C.x*modB) / D + poscorner0.y;

		pos = THIRD*(poscorner1 + poscorner0 + poscorner2);

		// Always project CC to insulator:
		if ((info.flag == CROSSING_INS))
		{
			f64_vec2 cc2 = cc;
			cc2.project_to_radius(cc, DEVICE_RADIUS_INSULATOR_OUTER);
		};

		// Hold up:
		// If cc is outside the triangle, move towards pos until it is inside.

		// Take cc-poscorner0 and look at the dimension that is perpendicular to poscorner1-poscorner2
		// Is it greater than we get for poscorner1-poscorner0

		// If so we've got to move towards pos; how do we know how far to move?
		// Presumably component length changes linearly with change in vector so check component length for pos.

		// Then test if we are outside the other edge normals.

		f64_vec2 minus = cc - poscorner0;
		f64_vec2 edgenormal;
		edgenormal.x = poscorner2.y - poscorner1.y;
		edgenormal.y = poscorner1.x - poscorner2.x;
		// Are 0,1,2 anticlockwise? yes
		// so if x = y2-y1 then it points out
		f64 edgemod = edgenormal.modulus();
		edgenormal /= edgemod;
		f64 dist = minus.dot(edgenormal);
		f64 dist2 = (poscorner2 - poscorner0).dot(edgenormal);
		if (dist > dist2) {
			f64 dist3 = (pos - poscorner0).dot(edgenormal);
			// dist2 = lambda*dist3 + (1-lambda) dist
			// lambda = (dist2-dist) / (dist3-dist)
			cc.x += ((dist2 - dist) / (dist3 - dist))*(pos.x - cc.x);
			cc.y += ((dist2 - dist) / (dist3 - dist))*(pos.y - cc.y);
		}

		minus = cc - poscorner2;
		edgenormal.x = poscorner1.y - poscorner0.y;
		edgenormal.y = poscorner0.x - poscorner1.x;
		edgemod = edgenormal.modulus();
		edgenormal /= edgemod;
		dist = minus.dot(edgenormal);
		dist2 = (poscorner0 - poscorner2).dot(edgenormal);
		if (dist > dist2) {
			f64 dist3 = (pos - poscorner2).dot(edgenormal);
			cc.x += ((dist2 - dist) / (dist3 - dist))*(pos.x - cc.x);
			cc.y += ((dist2 - dist) / (dist3 - dist))*(pos.y - cc.y);
		}

		minus = cc - poscorner1;
		edgenormal.x = poscorner0.y - poscorner2.y;
		edgenormal.y = poscorner2.x - poscorner0.x;
		edgemod = edgenormal.modulus();
		edgenormal /= edgemod;
		dist = minus.dot(edgenormal);
		dist2 = (poscorner0 - poscorner1).dot(edgenormal);
		if (dist > dist2) {
			f64 dist3 = (pos - poscorner1).dot(edgenormal);
			cc.x += ((dist2 - dist) / (dist3 - dist))*(pos.x - cc.x);
			cc.y += ((dist2 - dist) / (dist3 - dist))*(pos.y - cc.y);
		}
		
	} else {
		// FRILL
		pos = 0.5*(poscorner1 + poscorner0);
		f64_vec2 pos2 = pos;
		if (info.flag == INNER_FRILL) {
			pos2.project_to_radius(pos, FRILL_CENTROID_INNER_RADIUS_d);
		} else {
			pos2.project_to_radius(pos, FRILL_CENTROID_OUTER_RADIUS_d);
		};
		cc = pos;
	}
	
	// Now set up averaging coefficients and set n,T.
	// Outer frills it is thus set to n=0,T=0.
	// Well, circumcenter is equidistant so 1/3 is still reasonable average.
	
	// I think I prefer linear interpolation, making this a point estimate of n. The masses are saved
	// in the vertcells.
	
	if (info.flag == DOMAIN_TRIANGLE) {

		f64 lambda1, lambda2, lambda3;
		if (bCalculateOnCircumcenters) {
			
			f64_vec2 x0 = poscorner0, x1 = poscorner1, x2 = poscorner2;
			f64_vec2 a1, a2;
			f64 b1, b2;
//			a1.x = (x1.y - x2.y) / ((x0.x - x2.x)*(x1.y - x2.y) - (x1.x - x2.x)*(x0.y - x2.y));
//			a1.y = (x2.x - x1.x) / ((x0.x - x2.x)*(x1.y - x2.y) - (x1.x - x2.x)*(x0.y - x2.y));
//			b1 = -a1.x*x2.x - a1.y*x2.y;
//			a2.x = (x0.y - x2.y) / ((x1.x - x2.x)*(x0.y - x2.y) - (x1.y - x2.y)*(x0.x - x2.x));
//			a2.y = (x2.x - x0.x) / ((x1.x - x2.x)*(x0.y - x2.y) - (x1.y - x2.y)*(x0.x - x2.x));
//			b2 = -a2.x*x2.x - a2.y*x2.y;
//			lambda1 = a1.x*cc.x + a1.y*cc.y + b1;
//			lambda2 = a2.x*cc.x + a2.y*cc.y + b2;
//			lambda3 = 1.0 - lambda1 - lambda2;

			// We are getting lambda3 < 0 when the point is well inside the triangle.
			// What gives?

			// Try this instead:

			lambda1 = ((x1.y - x2.y)*(cc.x - x2.x) + (x2.x - x1.x)*(cc.y - x2.y)) /
				((x1.y - x2.y)*(x0.x - x2.x) + (x2.x - x1.x)*(x0.y - x2.y));
			lambda2 = ((x2.y-x0.y)*(cc.x-x2.x) + (x0.x-x2.x)*(cc.y-x2.y))/
				((x1.y - x2.y)*(x0.x - x2.x) + (x2.x - x1.x)*(x0.y - x2.y));
			lambda3 = 1.0 - lambda1 - lambda2;
			
			
		} else {
			lambda1 = THIRD;
			lambda2 = THIRD;
			lambda3 = THIRD;
		};

		if ((tri_corner_index.i1 >= StartMajor) && (tri_corner_index.i1 < EndMajor))
		{
			n += lambda1*shared_n[tri_corner_index.i1 - StartMajor];
			T += lambda1*shared_T[tri_corner_index.i1 - StartMajor];
			if (TESTTRI) printf("%d sharedvers n %1.10E contribnn %1.10E Tn %1.12E %d\n",
				iMinor, n.n, shared_n[tri_corner_index.i1 - StartMajor].n_n,
				shared_T[tri_corner_index.i1 - StartMajor].Tn, tri_corner_index.i1);
		} else {
			n += lambda1*p_n_major[tri_corner_index.i1];
			T += lambda1*p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL];
			if (TESTTRI) printf("%d loadvers n %1.10E contribnn %1.10E Tn %1.12E\n",
				iMinor, n.n, p_n_major[tri_corner_index.i1].n_n,
				p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL].Tn);
		};

		if ((tri_corner_index.i2 >= StartMajor) && (tri_corner_index.i2 < EndMajor))
		{
			n += lambda2*shared_n[tri_corner_index.i2 - StartMajor];
			T += lambda2*shared_T[tri_corner_index.i2 - StartMajor];
			if (TESTTRI) printf("%d sharedvers n %1.10E contribnn %1.10E Tn %1.12E %d\n",
				iMinor, n.n, shared_n[tri_corner_index.i2 - StartMajor].n_n,
				shared_T[tri_corner_index.i2 - StartMajor].Tn, tri_corner_index.i2);
		}
		else {
			n += lambda2*p_n_major[tri_corner_index.i2];
			T += lambda2*p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL];
			if (TESTTRI) printf("%d loadvers n %1.10E contribnn %1.10E Tn %1.12E\n",
				iMinor, n.n, p_n_major[tri_corner_index.i2].n_n,
				p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL].Tn);
		};

		if ((tri_corner_index.i3 >= StartMajor) && (tri_corner_index.i3 < EndMajor))
		{
			n += lambda3*shared_n[tri_corner_index.i3 - StartMajor];
			T += lambda3*shared_T[tri_corner_index.i3 - StartMajor];
			if (TESTTRI) printf("%d sharedvers n %1.10E contribnn %1.10E Tn %1.12E %d\n",
				iMinor, n.n, shared_n[tri_corner_index.i3 - StartMajor].n_n,
				shared_T[tri_corner_index.i3 - StartMajor], tri_corner_index.i3);
		}
		else {
			n += lambda3*p_n_major[tri_corner_index.i3];
			T += lambda3*p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL];
			if (TESTTRI) printf("%d loadvers n %1.10E contribnn %1.10E Tn %1.12E\n",
				iMinor, n.n, p_n_major[tri_corner_index.i3].n_n,
				p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL].Tn);
		}; 
		if (TESTTRI) 
			printf("%d: lambda %1.10E %1.10E %1.10E\ncorner n %1.10E %1.10E %1.10E\n"
				"cc %1.9E %1.9E | %1.9E %1.9E | %1.9E %1.9E | %1.9E %1.9E \n"
				"indexcorner %d %d %d result nn= %1.10E Tn %1.12E \n\n",
				iMinor, lambda1, lambda2, lambda3, 
				p_n_major[tri_corner_index.i1].n,
				p_n_major[tri_corner_index.i2].n,
				p_n_major[tri_corner_index.i3].n,
				cc.x,cc.y, poscorner0.x, poscorner0.y, poscorner1.x, poscorner1.y, poscorner2.x, poscorner2.y,
				tri_corner_index.i1, tri_corner_index.i2, tri_corner_index.i3,
				n.n_n, T.Tn
			);
		
	}
	else {
		// What else?
		if (info.flag == CROSSING_INS)
		{
			int iAbove = 0;
			if ((tri_corner_index.i1 >= StartMajor) && (tri_corner_index.i1 < EndMajor))
			{
				if (poscorner0.dot(poscorner0) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += shared_n[tri_corner_index.i1 - StartMajor];
					T += shared_T[tri_corner_index.i1 - StartMajor];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i1,
							shared_n[tri_corner_index.i1 - StartMajor].n_n,
							shared_T[tri_corner_index.i1 - StartMajor].Tn);
				};

			}
			else {
				if (poscorner0.dot(poscorner0) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += p_n_major[tri_corner_index.i1];
					T += p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i1,
							p_n_major[tri_corner_index.i1].n_n,
							p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL].Tn);
				}
			};

			if ((tri_corner_index.i2 >= StartMajor) && (tri_corner_index.i2 < EndMajor))
			{
				if (poscorner1.dot(poscorner1) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += shared_n[tri_corner_index.i2 - StartMajor];
					T += shared_T[tri_corner_index.i2 - StartMajor];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i2,
							shared_n[tri_corner_index.i2 - StartMajor].n_n,
							shared_T[tri_corner_index.i2 - StartMajor].Tn);
				};
			}
			else {
				if (poscorner1.dot(poscorner1) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += p_n_major[tri_corner_index.i2];
					T += p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i2,
							p_n_major[tri_corner_index.i2].n_n,
							p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL].Tn);
				};
			};

			if ((tri_corner_index.i3 >= StartMajor) && (tri_corner_index.i3 < EndMajor))
			{
				if (poscorner2.dot(poscorner2) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += shared_n[tri_corner_index.i3 - StartMajor];
					T += shared_T[tri_corner_index.i3 - StartMajor];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i3,
							shared_n[tri_corner_index.i3 - StartMajor].n_n,
							shared_T[tri_corner_index.i3 - StartMajor].Tn);
				};
			}
			else {
				if (poscorner2.dot(poscorner2) > DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					n += p_n_major[tri_corner_index.i3];
					T += p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL];
					iAbove++;
					if (TESTTRI)
						printf("%d INS tri: vertex %d nn %1.12E Tn %1.12E\n",
							iMinor, tri_corner_index.i3,
							p_n_major[tri_corner_index.i3].n_n,
							p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL].Tn);
				};
			};

#ifdef PROJECT_TO_INS_ALWAYS
			f64_vec2 pos2 = pos;
			pos2.project_to_radius(pos, DEVICE_RADIUS_INSULATOR_OUTER);
#else
			if (pos.dot(pos) < DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)				
			{
				f64_vec2 pos2 = pos;
				pos2.project_to_radius(pos, DEVICE_RADIUS_INSULATOR_OUTER);
			};
			// project only if below insulator.!
#endif

			f64 divide = 1.0 / (f64)iAbove;
			n.n *= divide;
			n.n_n *= divide;
			T.Tn *= divide;
			T.Ti *= divide;
			T.Te *= divide;
			if (TESTTRI)
				printf("%d INS tri: iAbove %d Tn divided: %1.14E\n", iMinor, iAbove, T.Tn);

		} else {
			if (info.flag == CROSSING_CATH)
			{

				int iAbove = 0;
				if ((tri_corner_index.i1 >= StartMajor) && (tri_corner_index.i1 < EndMajor))
				{
					if (poscorner0.x*poscorner0.x+(poscorner0.y-CATHODE_ROD_R_POSITION)*(poscorner0.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += shared_n[tri_corner_index.i1 - StartMajor];
						T += shared_T[tri_corner_index.i1 - StartMajor];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i1,
								shared_n[tri_corner_index.i1 - StartMajor].n_n,
								shared_T[tri_corner_index.i1 - StartMajor].Tn);
					};
				} else {
					if (poscorner0.x*poscorner0.x + (poscorner0.y - CATHODE_ROD_R_POSITION)*(poscorner0.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += p_n_major[tri_corner_index.i1];
						T += p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i1,
								p_n_major[tri_corner_index.i1].n_n,
								p_T_minor[tri_corner_index.i1 + BEGINNING_OF_CENTRAL].Tn);
					}
				};

				if ((tri_corner_index.i2 >= StartMajor) && (tri_corner_index.i2 < EndMajor))
				{
					if (poscorner1.x*poscorner1.x + (poscorner1.y - CATHODE_ROD_R_POSITION)*(poscorner1.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += shared_n[tri_corner_index.i2 - StartMajor];
						T += shared_T[tri_corner_index.i2 - StartMajor];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i2,
								shared_n[tri_corner_index.i2 - StartMajor].n_n,
								shared_T[tri_corner_index.i2 - StartMajor].Tn);
					};
				} else {
					if (poscorner1.x*poscorner1.x + (poscorner1.y - CATHODE_ROD_R_POSITION)*(poscorner1.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += p_n_major[tri_corner_index.i2];
						T += p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i2,
								p_n_major[tri_corner_index.i2].n_n,
								p_T_minor[tri_corner_index.i2 + BEGINNING_OF_CENTRAL].Tn);
					};
				};

				if ((tri_corner_index.i3 >= StartMajor) && (tri_corner_index.i3 < EndMajor))
				{
					if (poscorner2.x*poscorner2.x + (poscorner2.y - CATHODE_ROD_R_POSITION)*(poscorner2.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += shared_n[tri_corner_index.i3 - StartMajor];
						T += shared_T[tri_corner_index.i3 - StartMajor];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i3,
								shared_n[tri_corner_index.i3 - StartMajor].n_n,
								shared_T[tri_corner_index.i3 - StartMajor].Tn);
					};
				}
				else {
					if (poscorner2.x*poscorner2.x + (poscorner2.y - CATHODE_ROD_R_POSITION)*(poscorner2.y - CATHODE_ROD_R_POSITION) > CATHODE_ROD_RADIUS*CATHODE_ROD_RADIUS)
					{
						n += p_n_major[tri_corner_index.i3];
						T += p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL];
						iAbove++;
						if (TESTTRI)
							printf("%d CATH tri: vertex %d nn %1.12E Tn %1.12E\n",
								iMinor, tri_corner_index.i3,
								p_n_major[tri_corner_index.i3].n_n,
								p_T_minor[tri_corner_index.i3 + BEGINNING_OF_CENTRAL].Tn);
					};
				}; 

				f64_vec2 pos2 = pos;
		//		pos2.y -= CATHODE_ROD_R_POSITION;
				// Now we need to project it on to the circle about (0.0,5.0)
		//		pos2.project_to_radius(pos, CATHODE_ROD_RADIUS);
		//		pos.y += CATHODE_ROD_R_POSITION;

				// Do not project. !


				f64 divide = 1.0 / (f64)iAbove;
				n.n *= divide;
				n.n_n *= divide;
				T.Tn *= divide;
				T.Ti *= divide;
				T.Te *= divide;
				if (TESTTRI)
					printf("%d CATH tri: iAbove %d Tn divided: %1.14E\n", iMinor, iAbove, T.Tn);

			} else {
				// Cool neutrals in frill:
				if (info.flag == OUTER_FRILL) {
					n.n = INITIAL_BACKGROUND_ION_DENSITY;
					n.n_n = INITIAL_TOTAL_DENSITY - INITIAL_BACKGROUND_ION_DENSITY;
					T.Te = 4.0e-14; T.Ti = 4.0e-14; T.Tn = 4.0e-14;
				} else {
					// inner frill? out of domain?
					n.n = 0.0;
					n.n_n = 0.0;
					T.Te = 0.0; T.Ti = 0.0; T.Tn = 0.0;
				}
			}
			
		};
		// Outer frills it is thus set to n=0,T=0.
	};

	if (TESTTRI) printf("\n%d flag %d Tn %1.12E info.pos.x %1.9E cc.x %1.9E \n", iMinor, info.flag, T.Tn, pos.x, cc.x);

	p_n_minor[iMinor] = n;
	p_T_minor[iMinor] = T;
	info.pos = pos;
	p_info[iMinor] = info;
	p_cc[iMinor] = cc;
}


__global__ void kernelCreateShardModelOfDensities_And_SetMajorArea(
	structural * __restrict__ p_info_minor,
	nvals * __restrict__ p_n_major, 
	nvals * __restrict__ p_n_minor,
	long * __restrict__ p_izTri_vert,
	char * __restrict__ p_szPBCtri_vert,
	f64_vec2 * __restrict__ p_cc,
	ShardModel * __restrict__ p_n_shards,
	ShardModel * __restrict__ p_n_n_shards,
	//	long * __restrict__ Tri_n_lists,
	//	long * __restrict__ Tri_n_n_lists	,
	f64 * __restrict__ p_AreaMajor,
	bool bUseCircumcenter
	)// sets n_shards_n, n_shards, Tri_n_n_lists, Tri_n_lists
{
	// called for major tile
	// Interpolation to Tri_n_lists, Tri_n_n_lists is not yet implemented. But this would be output.

	// Inputs:
	// n, pTri->cent,  izTri,  pTri->periodic, pVertex->pos

	// Outputs:
	// pVertex->AreaCell
	// n_shards[iVertex]
	// Tri_n_n_lists[izTri[i]][o1 * 2] <--- 0 if not set by domain vertex

	// CALL AVERAGE OF n TO TRIANGLES - WANT QUADRATIC AVERAGE - BEFORE WE BEGIN
	// MUST ALSO POPULATE pVertex->AreaCell with major cell area

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ nvals shared_n[threadsPerTileMinor];

	// Here 4 doubles/minor. In 16*1024, 4 double*8 bytes*512 minor. 256 major. 
	// Choosing to store n_n while doing n which is not necessary.

	ShardModel n_; // to be populated
	
	int iNeigh, tri_len;
	f64 N_n, N, interpolated_n, interpolated_n_n;
	long i, inext, o1, o2;

	//memset(Tri_n_n_lists, 0, sizeof(f64)*NUMTRIANGLES * 6);
	//memset(Tri_n_lists, 0, sizeof(f64)*NUMTRIANGLES * 6);

	// We can afford to stick 6-8 doubles in shared. 8 vars*8 bytes*256 threads = 16*1024.
	if (bUseCircumcenter == false)
	{
		structural info2[2];
		memcpy(info2, p_info_minor + blockIdx.x*threadsPerTileMinor + 2 * threadIdx.x, sizeof(structural) * 2);
		shared_pos[2 * threadIdx.x] = info2[0].pos;
		shared_pos[2 * threadIdx.x + 1] = info2[1].pos;
	} else {
		memcpy(&(shared_pos[2 * threadIdx.x]), p_cc + blockIdx.x*threadsPerTileMinor + 2 * threadIdx.x, sizeof(f64_vec2) * 2);
	}
	memcpy(&(shared_n[2 * threadIdx.x]), p_n_minor + blockIdx.x*threadsPerTileMinor + 2 * threadIdx.x, sizeof(nvals) * 2);
		
	__syncthreads();

	long const StartMinor = blockIdx.x*threadsPerTileMinor; // vertex index
	long const EndMinor = StartMinor + threadsPerTileMinor;
	// To fit in Tri_n_n_lists stuff we should first let coeff[] go out of scope.
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX
	structural info = p_info_minor[BEGINNING_OF_CENTRAL + iVertex];

	if (info.flag == DOMAIN_VERTEX) {

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		f64 coeff[MAXNEIGH];   // total 21*12 = 252 bytes. 256 max for 192 threads.
		f64 ndesire0, ndesire1;
		f64_vec2 pos0, pos1;

		memcpy(izTri, p_izTri_vert + MAXNEIGH_d*iVertex, sizeof(long)*MAXNEIGH_d);
		memcpy(szPBC, p_szPBCtri_vert + MAXNEIGH_d*iVertex, sizeof(char)*MAXNEIGH_d);

		f64 n_avg = p_n_major[iVertex].n;
		// WHY WAS IT minor NOT major ?????????????????????????

		if ((izTri[0] >= StartMinor) && (izTri[0] < EndMinor)) {
			pos0 = shared_pos[izTri[0] - StartMinor];
			ndesire0 = shared_n[izTri[0] - StartMinor].n;
		}
		else {
			if (bUseCircumcenter) {
				pos0 = p_cc[izTri[0]];
			} else {
				pos0 = p_info_minor[izTri[0]].pos;
			} // there exists a more elegant way than this!!!

			ndesire0 = p_n_minor[izTri[0]].n;
		}
		if (szPBC[0] == ROTATE_ME_CLOCKWISE) pos0 = Clockwise_d*pos0;
		if (szPBC[0] == ROTATE_ME_ANTICLOCKWISE) pos0 = Anticlockwise_d*pos0;

		f64 tri_area;
		f64 N0 = 0.0; f64 coeffcent = 0.0;
		memset(coeff, 0, sizeof(f64)*MAXNEIGH_d);
		short i;
		f64 AreaMajor = 0.0;
		f64 high_n = ndesire0;
		f64 low_n = ndesire0;
#pragma unroll MAXNEIGH
		for (i = 0; i < info.neigh_len; i++)
		{
			// Temporary setting:
			n_.n[i] = ndesire0;

			inext = i + 1; if (inext == info.neigh_len) inext = 0;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor)) {
				pos1 = shared_pos[izTri[inext] - StartMinor];
				ndesire1 = shared_n[izTri[inext] - StartMinor].n;
			} else {
				if (bUseCircumcenter) {
					pos1 = p_cc[izTri[inext]];
				} else {
					pos1 = p_info_minor[izTri[inext]].pos;
				}
				ndesire1 = p_n_minor[izTri[inext]].n;
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) pos1 = Clockwise_d*pos1;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) pos1 = Anticlockwise_d*pos1;

			high_n = max(ndesire1, high_n);
			low_n = min(ndesire1, low_n);

			tri_area = fabs(0.5*
				((pos0.x + pos1.x) * (pos1.y - pos0.y)
					+ (info.pos.x + pos1.x) * (info.pos.y - pos1.y)
					+ (info.pos.x + pos0.x) * (pos0.y - info.pos.y)));

			if (TEST1) printf("%d : ndesire0 %1.10E ndesire1 %1.10E high_n low_n %1.8E %1.8E tri_area %1.9E\n", VERTCHOSEN,
				ndesire0, ndesire1, high_n, low_n, tri_area);

			N0 += tri_area*THIRD*(ndesire0 + ndesire1);
			coeff[i] += tri_area*THIRD;
			coeff[inext] += tri_area*THIRD;
			coeffcent += tri_area*THIRD;
			AreaMajor += tri_area;
			pos0 = pos1;
			ndesire0 = ndesire1;
		};
		// . If n_avg > n_max_corners then set all to n_avg.
		// . If n_min < n_needed < n_max then set n_cent = n_needed

		// Otherwise, we now have coeff array populated and will go round
		// repeatedly. We have to reload n lots of times.
		// This is not the typical case.

		p_AreaMajor[iVertex] = AreaMajor;
		
		if ((n_avg > high_n) || (n_avg < low_n)) {
#pragma unroll MAXNEIGH
			for (i = 0; i < info.neigh_len; i++)
				n_.n[i] = n_avg;
			n_.n_cent = n_avg;


			if (TEST1) printf("VERTCHOSEN (n_avg > high_n) || (n_avg < low_n) \n");
	//		if (iVertex == CHOSEN) printf("CHOSEN : Switch1 n_avg %1.12E \n",n_avg);

		} else {
			real n_C_need = (n_avg*AreaMajor - N0) / coeffcent;

			if ((n_C_need > low_n) && (n_C_need < high_n)) {
				n_.n_cent = n_C_need;

				if (TEST1) printf("VERTCHOSEN ((n_C_need > low_n) && (n_C_need < high_n)) \n");

	//			if (iVertex == CHOSEN) printf("CHOSEN : Switch2 n_C_need %1.12E low_n %1.12E high_n %1.12E\n", n_C_need,low_n,high_n);

			}
			else {
				// The laborious case.
	//			if (iVertex == CHOSEN) printf("Laborious case...\n");

				if (TEST1) printf("VERTCHOSEN  The laborious case. n_avg %1.10E n_C_need %1.10E low_n %1.10E high_n %1.10E\n",
					n_avg, n_C_need, low_n, high_n);
				bool fixed[MAXNEIGH];
				memset(fixed, 0, sizeof(bool) * MAXNEIGH);
				// cannot fit even this alongside the rest we have in L1.
				// Can we make szPBC go out of scope by here?

				f64 n_C, n_acceptable;
				if (n_C_need < low_n) {
					// the mass is low. So for those less than some n_acceptable,
					// let them attain n_desire, and fix n_C = low_n.
					// Then we'll see how high we can go with n_acceptable.

	//				if (iVertex == CHOSEN) printf("(n_C_need < low_n)\n");

					n_C = low_n;
					n_acceptable = (n_avg*AreaMajor - coeffcent*n_C) / (AreaMajor - THIRD*AreaMajor);
					// area-THIRD*area = sum of other coeffs, and of course
					// coeffcent = THIRD*area
					// n_acceptable > N/area since N=area*n_avg > area*low_n.

					// We accept things that are less than this 'max average', and
					// let that increase the threshold; go again until
					// the time we do not find any new lower items ;	
					bool found = 0;
					do {
						found = 0;
						f64 coeffremain = 0.0;
						f64 N_attained = coeffcent*low_n;
						for (i = 0; i < info.neigh_len; i++)
						{
							if (fixed[i] == 0) {
								// Go collect ndesire[i]:

								f64 ndesire;
								if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
								{
									ndesire = shared_n[izTri[i] - StartMinor].n;
								}
								else {
									ndesire = p_n_minor[izTri[i]].n;
								};
								
		//						if (iVertex == CHOSEN) printf("CHOSEN : ndesire %1.14E n_acceptable %1.14E\n", ndesire,n_acceptable);
								
								if (ndesire < n_acceptable) { // yes, use ndesire[i] ...
									fixed[i] = true;
									n_.n[i] = ndesire;
									N_attained += n_.n[i] * coeff[i];
									found = true;
								}
								else {
									coeffremain += coeff[i];
								};
							}
							else {
								N_attained += n_.n[i] * coeff[i];
							};
						};
						// It can happen that eventually ALL are found
						// to be < n_acceptable due to FP error.
						// On next pass found will be false.
						if ((found != 0) && (coeffremain > 0.0)) {
							n_acceptable = (n_avg*AreaMajor - N_attained) / coeffremain;
							// The value to which we have to set the remaining
							// n values.
						};
	//					if (iVertex == CHOSEN) printf("---\n");
					} while (found != 0);

				} else {
					n_C = high_n;
					n_acceptable = (n_avg*AreaMajor - coeffcent*n_C) / (AreaMajor - THIRD*AreaMajor);
					bool found = 0;
					do {
						found = 0;
						f64 coeffremain = 0.0;
						f64 N_attained = coeffcent*high_n;
						for (i = 0; i < info.neigh_len; i++)
						{
							if (fixed[i] == 0) {
								// Go collect ndesire[i]:

								f64 ndesire;
								if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
								{
									ndesire = shared_n[izTri[i] - StartMinor].n;
								} else {
									ndesire = p_n_minor[izTri[i]].n;
								};
								

	//							if (iVertex == CHOSEN) printf("CHOSEN : ndesire %1.14E n_acceptable %1.14E\n", ndesire, n_acceptable);


								if (ndesire > n_acceptable) {
									// yes, use ndesire[i] ...
									fixed[i] = true;
									n_.n[i] = ndesire;
									N_attained += n_.n[i] * coeff[i];
									found = true;
								} else {
									coeffremain += coeff[i];
								};
							} else {
								N_attained += n_.n[i] * coeff[i];
							};
						};
						if ((found != 0) && (coeffremain > 0.0)) {
							n_acceptable = (n_avg*AreaMajor - N_attained) / coeffremain;
						};

	//					if (iVertex == CHOSEN) printf("@@@ \n");
						
					} while (found != 0);
				};
				// Now we should set the remaining values to n_acceptable
				// which is less than ndesire[i] in all those cases.
				for (i = 0; i < info.neigh_len; i++)
				{
					if (fixed[i] == 0) n_.n[i] = n_acceptable;

					if (TEST1) printf("n[%d]: %1.10E\n", i, n_.n[i]);
				};
				n_.n_cent = n_C;
				
				if (TEST1) {
					for (i = 0; i < info.neigh_len; i++)
					{
						printf("%1.10E  \t\t", n_.n[i]);
					}
					printf("\nn_cent %1.14E \n\n", n_.n_cent);
				};
				
			};
		};

		memcpy(&(p_n_shards[iVertex]), &n_, sizeof(ShardModel));


		if (TEST1) printf("iVertex %d n_cent %1.10E nmajor(=n_avg) %1.10E \n*********\n",
			iVertex, n_.n_cent, n_avg);

		// Now start again: neutrals

		n_avg = p_n_major[iVertex].n_n;

		if ((izTri[0] >= StartMinor) && (izTri[0] < EndMinor)) {
			pos0 = shared_pos[izTri[0] - StartMinor];
			ndesire0 = shared_n[izTri[0] - StartMinor].n_n;
		} else {
			if (bUseCircumcenter) {
				pos0 = p_cc[izTri[0]];
			} else {
				pos0 = p_info_minor[izTri[0]].pos;
			};
			ndesire0 = p_n_minor[izTri[0]].n_n;
		}
		if (szPBC[0] == ROTATE_ME_CLOCKWISE) pos0 = Clockwise_d*pos0;
		if (szPBC[0] == ROTATE_ME_ANTICLOCKWISE) pos0 = Anticlockwise_d*pos0;

		N0 = 0.0;
		//coeffcent = 0.0;
		//memset(coeff, 0, sizeof(f64)*MAXNEIGH_d); // keep em
		high_n = ndesire0;
		low_n = ndesire0;

#pragma unroll MAXNEIGH
		for (i = 0; i < info.neigh_len; i++)
		{
			// Temporary setting:
			n_.n[i] = ndesire0;

			inext = i + 1; if (inext == info.neigh_len) inext = 0;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor)) {
				pos1 = shared_pos[izTri[inext] - StartMinor];
				ndesire1 = shared_n[izTri[inext] - StartMinor].n_n;
			} else {
				if (bUseCircumcenter) {
					pos1 = p_cc[izTri[inext]]; 
				} else {
					pos1 = p_info_minor[izTri[inext]].pos;
				}
				ndesire1 = p_n_minor[izTri[inext]].n_n;
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) pos1 = Clockwise_d*pos1;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) pos1 = Anticlockwise_d*pos1;

			high_n = max(ndesire1, high_n);
			low_n = min(ndesire1, low_n);

			tri_area = fabs(0.5*
				((pos0.x + pos1.x) * (pos1.y - pos0.y)
					+ (info.pos.x + pos1.x) * (info.pos.y - pos1.y)
					+ (info.pos.x + pos0.x) * (pos0.y - info.pos.y)));

			N0 += tri_area*THIRD*(ndesire0 + ndesire1); // Could consider moving it into loop above.

			pos0 = pos1;
			ndesire0 = ndesire1;
		};
		// . If n_avg > n_max_corners then set all to n_avg.
		// . If n_min < n_needed < n_max then set n_cent = n_needed

		// Otherwise, we now have coeff array populated and will go round
		// repeatedly. We have to reload n lots of times.
		// This is not the typical case.

		if ((n_avg > high_n) || (n_avg < low_n)) {
#pragma unroll MAXNEIGH
			for (i = 0; i < info.neigh_len; i++)
				n_.n[i] = n_avg;
			n_.n_cent = n_avg;


		} else {
			real n_C_need = (n_avg*AreaMajor - N0) / coeffcent;

			if ((n_C_need > low_n) && (n_C_need < high_n)) {
				n_.n_cent = n_C_need; // accept desired values

			} else {
				// The laborious case.


				bool fixed[MAXNEIGH];
				memset(fixed, 0, sizeof(bool) * MAXNEIGH);
				// cannot fit even this alongside the rest we have in L1.
				// Can we make szPBC go out of scope by here?

				f64 n_C, n_acceptable;
				if (n_C_need < low_n) {
					// the mass is low. So for those less than some n_acceptable,
					// let them attain n_desire, and fix n_C = low_n.
					// Then we'll see how high we can go with n_acceptable.

					n_C = low_n;
					n_acceptable = (n_avg*AreaMajor - coeffcent*n_C) / (AreaMajor - THIRD*AreaMajor);
					// area-THIRD*area = sum of other coeffs, and of course
					// coeffcent = THIRD*area
					// n_acceptable > N/area since N=area*n_avg > area*low_n.

					// We accept things that are less than this 'max average', and
					// let that increase the threshold; go again until
					// the time we do not find any new lower items ;		
					bool found = 0;
					do {
						found = 0;
						f64 coeffremain = 0.0;
						f64 N_attained = coeffcent*low_n;
						for (i = 0; i < info.neigh_len; i++)
						{
							if (fixed[i] == 0) {
								// Go collect ndesire[i]:

								f64 ndesire;
								if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
								{
									ndesire = shared_n[izTri[i] - StartMinor].n_n;
								}
								else {
									ndesire = p_n_minor[izTri[i]].n_n;
								};
								if (ndesire < n_acceptable) { // yes, use ndesire[i] ...
									fixed[i] = true;
									n_.n[i] = ndesire;
									N_attained += n_.n[i] * coeff[i];
									found = true;
								}
								else {
									coeffremain += coeff[i];
								};
							}
							else {
								N_attained += n_.n[i] * coeff[i];
							};
						};
						// It can happen that eventually ALL are found
						// to be < n_acceptable due to FP error.
						// On next pass found will be false.
						if ((found != 0) && (coeffremain > 0.0)) {
							n_acceptable = (n_avg*AreaMajor - N_attained) / coeffremain;
							// The value to which we have to set the remaining
							// n values.
						};
					} while (found != 0);

				}
				else {
					n_C = high_n;
					n_acceptable = (n_avg*AreaMajor - coeffcent*n_C) / (AreaMajor - THIRD*AreaMajor);
					bool found = 0;
					do {
						found = 0;
						f64 coeffremain = 0.0;
						f64 N_attained = coeffcent*high_n;
						for (i = 0; i < info.neigh_len; i++)
						{
							if (fixed[i] == 0) {
								// Go collect ndesire[i]:

								f64 ndesire;
								if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
								{
									ndesire = shared_n[izTri[i] - StartMinor].n_n;
								}
								else {
									ndesire = p_n_minor[izTri[i]].n_n;
								};
								if (ndesire > n_acceptable) {
									// yes, use ndesire[i] ...
									fixed[i] = true;
									n_.n[i] = ndesire;
									N_attained += n_.n[i] * coeff[i];
									found = true;
								}
								else {
									coeffremain += coeff[i];
								};
							}
							else {
								N_attained += n_.n[i] * coeff[i];
							};

						};
						if ((found != 0) && (coeffremain > 0.0)) {
							n_acceptable = (n_avg*AreaMajor - N_attained) / coeffremain;
						};
					} while (found != 0);

				};
				// Now we should set the remaining values to n_acceptable
				// which is less than ndesire[i] in all those cases.
				for (i = 0; i < info.neigh_len; i++)
				{
					if (fixed[i] == 0) n_.n[i] = n_acceptable;

				};
				n_.n_cent = n_C;
			};
		};

		memcpy(&(p_n_n_shards[iVertex]), &n_, sizeof(ShardModel));

		// Now done both species.

	} else { // NOT DOMAIN_VERTEX
		
		if (info.flag == OUTERMOST) {
			n_.n_cent = p_n_major[iVertex].n;
			for (i = 0; i < MAXNEIGH; i++)
				n_.n[i] = n_.n_cent;
			memcpy(&(p_n_shards[iVertex]), &n_, sizeof(ShardModel));

			if (iVertex == VERTCHOSEN) printf("%d n_major.n %1.10E n_.n[4] %1.8E n_.n_cent %1.8E\n\n\n",
				iVertex, p_n_major[iVertex].n, n_.n[4], n_.n_cent);

			n_.n_cent = p_n_major[iVertex].n_n;
			for (i = 0; i < MAXNEIGH; i++)
				n_.n[i] = n_.n_cent;
			memcpy(&(p_n_n_shards[iVertex]), &n_, sizeof(ShardModel));

			f64 AreaTotal = PPN_CIRCLE*M_PI*(DOMAIN_OUTER_RADIUS*DOMAIN_OUTER_RADIUS -
				INNER_A_BOUNDARY*INNER_A_BOUNDARY);
			p_AreaMajor[iVertex] = AreaTotal / (real)(numTilesMajor*threadsPerTileMajor); // ?
			// Setting area of outermost to average vertcell area
			// ...

			// Watch out for this when we make OUTERMOST FEWER


		} else {
			memset(&(p_n_shards[iVertex]), 0, sizeof(ShardModel));
			memset(&(p_n_n_shards[iVertex]), 0, sizeof(ShardModel));
			
			p_AreaMajor[iVertex] = 0.0; // NOTE BENE
		};
	};

	// NexT:  tri_n_lists.

	// Think I am not using this passing mechanism for n_shards information.

	/*
	for (i = 0; i < cp.numCoords; i++)
	{
	// for 2 triangles each corner:

	// first check which number corner this vertex is
	// make sure we enter them in order that goes anticlockwise for the
	// Then we need to make izMinorNeigh match this somehow

	// Let's say izMinorNeigh goes [across corner 0, across edge 2, corner 1, edge 0, corner 2, edge 1]
	// We want 0,1 to be the values corresp corner 0.

	// shard value 0 is in tri 0. We look at each pair of shard values in turn to interpolate.

	inext = i + 1; if (inext == cp.numCoords) inext = 0;

	interpolated_n = THIRD * (n_shards[iVertex].n[i] + n_shards[iVertex].n[inext] + n_shards[iVertex].n_cent);
	interpolated_n_n = THIRD * (n_shards_n[iVertex].n[i] + n_shards_n[iVertex].n[inext] + n_shards_n[iVertex].n_cent);
	// contribute to tris i and inext:
	o1 = (T + izTri[i])->GetCornerIndex(X + iVertex);
	o2 = (T + izTri[inext])->GetCornerIndex(X + iVertex);

	// Now careful which one's which:

	// inext sees this point as more anticlockwise.

	Tri_n_lists[izTri[inext]][o2 * 2 + 1] = interpolated_n;
	Tri_n_lists[izTri[i]][o1 * 2] = interpolated_n;

	Tri_n_n_lists[izTri[inext]][o2 * 2 + 1] = interpolated_n_n;
	Tri_n_n_lists[izTri[i]][o1 * 2] = interpolated_n_n;
	};*/

}


__global__ void kernelInferMinorDensitiesFromShardModel(
	structural * __restrict__ p_info,
	nvals * __restrict__ p_n_minor,
	ShardModel * __restrict__ p_n_shards,
	ShardModel * __restrict__ p_n_shards_n,
	LONG3 * __restrict__ p_tri_corner_index,
	LONG3 * __restrict__ p_who_am_I_to_corner,
	nvals * __restrict__ p_one_over_n
) {
	// Assume that we do the simplest thing possible.

	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; // iMinor OF VERTEX
	structural info = p_info[iMinor];
	nvals result;

	if (iMinor >= BEGINNING_OF_CENTRAL)
	{
	//	if (iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL) printf("\niMinor %d pos %1.10E %1.10E flag %d \n",
	//		iMinor, info.pos.x, info.pos.y, info.flag);

		if (info.flag == DOMAIN_VERTEX) {
			result.n = p_n_shards[iMinor - BEGINNING_OF_CENTRAL].n_cent;
			result.n_n = p_n_shards_n[iMinor - BEGINNING_OF_CENTRAL].n_cent;
			p_n_minor[iMinor] = result;
			result.n = 1.0 / result.n;
			result.n_n = 1.0 / result.n_n;
			p_one_over_n[iMinor] = result;


			// We are not being consistent.
			// We may wish to use major n here --> minor central n

			// We have not done the shard model for target n, we just average and then tween this back.
			// &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

		} else {
			// Outermost vertex?
			result.n = 0.0;
			result.n_n = 0.0;
			if (info.flag == OUTERMOST) {
				result.n_n = 1.0e18;
				result.n = UNIFORM_n_d;
			};
			p_n_minor[iMinor] = result;
			result.n_n = 1.0 / result.n_n;
			result.n = 1.0 / result.n;
			p_one_over_n[iMinor] = result;
		}
	} else {
		if (info.flag == DOMAIN_TRIANGLE) {
			LONG3 tri_corner_index = p_tri_corner_index[iMinor];
			LONG3 who_am_I_to_corner = p_who_am_I_to_corner[iMinor];
			result.n = THIRD*
				(p_n_shards[tri_corner_index.i1].n[who_am_I_to_corner.i1]
					+ p_n_shards[tri_corner_index.i2].n[who_am_I_to_corner.i2]
					+ p_n_shards[tri_corner_index.i3].n[who_am_I_to_corner.i3]);
			result.n_n = THIRD*
				(p_n_shards_n[tri_corner_index.i1].n[who_am_I_to_corner.i1]
					+ p_n_shards_n[tri_corner_index.i2].n[who_am_I_to_corner.i2]
					+ p_n_shards_n[tri_corner_index.i3].n[who_am_I_to_corner.i3]);
			p_n_minor[iMinor] = result;
			if (TESTTRI) printf("%d: %d %d %d shards n %1.10E %1.10E %1.10E result %1.10E\n",
				CHOSEN, tri_corner_index.i1, tri_corner_index.i2, tri_corner_index.i3,
				p_n_shards[tri_corner_index.i1].n[who_am_I_to_corner.i1],
				p_n_shards[tri_corner_index.i2].n[who_am_I_to_corner.i2],
				p_n_shards[tri_corner_index.i3].n[who_am_I_to_corner.i3], result.n);

			result.n = THIRD*(
				1.0/ p_n_shards[tri_corner_index.i1].n[who_am_I_to_corner.i1]
				+ 1.0/ p_n_shards[tri_corner_index.i2].n[who_am_I_to_corner.i2]
					+ 1.0/p_n_shards[tri_corner_index.i3].n[who_am_I_to_corner.i3]);
			result.n_n = THIRD*
				(1.0/p_n_shards_n[tri_corner_index.i1].n[who_am_I_to_corner.i1]
					+ 1.0/p_n_shards_n[tri_corner_index.i2].n[who_am_I_to_corner.i2]
					+ 1.0/p_n_shards_n[tri_corner_index.i3].n[who_am_I_to_corner.i3]);
			p_one_over_n[iMinor] = result;


		} else {
			if ((info.flag == CROSSING_INS) || (info.flag == CROSSING_CATH)) {
				LONG3 tri_corner_index = p_tri_corner_index[iMinor];
				LONG3 who_am_I_to_corner = p_who_am_I_to_corner[iMinor];
				result.n = 0.0;
				result.n_n = 0.0;
				
				structural info1, info2, info3;
				info1 = p_info[BEGINNING_OF_CENTRAL + tri_corner_index.i1];
				info2 = p_info[BEGINNING_OF_CENTRAL + tri_corner_index.i2];
				info3 = p_info[BEGINNING_OF_CENTRAL + tri_corner_index.i3];
				int numabove = 0;
				if (info1.flag == DOMAIN_VERTEX) {
					numabove++;
					result.n += p_n_shards[tri_corner_index.i1].n[who_am_I_to_corner.i1];
					result.n_n += p_n_shards_n[tri_corner_index.i1].n[who_am_I_to_corner.i1];
				};
				if (info2.flag == DOMAIN_VERTEX) {
					numabove++;
					result.n += p_n_shards[tri_corner_index.i2].n[who_am_I_to_corner.i2];
					result.n_n += p_n_shards_n[tri_corner_index.i2].n[who_am_I_to_corner.i2];
				};
				if (info3.flag == DOMAIN_VERTEX) {
					numabove++;
					result.n += p_n_shards[tri_corner_index.i3].n[who_am_I_to_corner.i3];
					result.n_n += p_n_shards_n[tri_corner_index.i3].n[who_am_I_to_corner.i3];
				};
				if (TESTTRI) printf("%d: %d %d %d C-INS shards n %1.10E %1.10E %1.10E result %1.10E numabove %d\n",
					CHOSEN, tri_corner_index.i1, tri_corner_index.i2, tri_corner_index.i3,
					p_n_shards[tri_corner_index.i1].n[who_am_I_to_corner.i1],
					p_n_shards[tri_corner_index.i2].n[who_am_I_to_corner.i2],
					p_n_shards[tri_corner_index.i3].n[who_am_I_to_corner.i3], result.n,
					numabove);

				result.n /= (f64)numabove;
				result.n_n /= (f64)numabove;
				p_n_minor[iMinor] = result;
				result.n = 1.0 / result.n;
				result.n_n = 1.0 / result.n_n;
				p_one_over_n[iMinor] = result;
			} else {
				memset(&(p_n_minor[iMinor]), 0, sizeof(nvals));
			}
		}
	}
}



__global__ void kernelCalculate_ita_visc(
	structural * __restrict__ p_info_minor,
	nvals * __restrict__ p_n_minor,
	T3 * __restrict__ p_T_minor,

	f64 * __restrict__ p_nu_ion_minor,
	f64 * __restrict__ p_nu_elec_minor,
	f64 * __restrict__ p_nu_nn_visc,
	f64 * __restrict__ p_ita_par_ion_minor,
	f64 * __restrict__ p_ita_par_elec_minor,
	f64 * __restrict__ p_ita_neutral_minor,

	int * __restrict__ p_iSelect,
	int * __restrict__ p_iSelectNeut
	)
{
	// Save nu_iHeart, nu_eHeart, nu_nn_visc.

	f64 TeV, sigma_MT, sigma_visc, sqrt_T, nu_en_visc;
	T3 T;
	f64 nu_in_visc, nu_ni_visc, nu_ii, nu_nn;
	nvals our_n;
	long const index = threadIdx.x + blockIdx.x * blockDim.x; 
	structural info = p_info_minor[index];

	// DEBUG
	//if (index == CHOSEN) printf("index %d info.flag %d info.pos %1.13E %1.14E \n",
	//	index, info.flag, info.pos.x, info.pos.y);
	int Selected_ie = 0, Selected_neut = 0;

	if (p_iSelect != 0) {
		Selected_ie = p_iSelect[index];
		Selected_neut = p_iSelectNeut[index];
	} else {
		Selected_ie = 1;
		Selected_neut = 1;
	};

	if (((info.flag == DOMAIN_VERTEX)  ||
		// (info.flag == OUTERMOST) ||
		(info.flag == DOMAIN_TRIANGLE) || 
		( (info.flag == CROSSING_INS) && (TestDomainPos(info.pos)))
		)	&& 
			((Selected_ie > 0) || (Selected_neut > 0))
		)
	{
		// We have not ruled out calculating traffic into outermost vertex cell - so this needs nu calculated in it.
		// (Does it actually try and receive traffic?)

		our_n = p_n_minor[index]; // never used again once we have kappa
		T = p_T_minor[index];

		TeV = T.Te * one_over_kB;
		Estimate_Ion_Neutral_Cross_sections_d(TeV, &sigma_MT, &sigma_visc);

		// commented.
//		sigma_visc *= ArtificialUpliftFactor(our_n.n, our_n.n_n);
//		sigma_MT *= ArtificialUpliftFactor(our_n.n, our_n.n_n);
		
		sqrt_T = sqrt(T.Te);
		nu_en_visc = our_n.n_n * sigma_visc * sqrt_T * over_sqrt_m_e;
		f64 nu_eiBar = nu_eiBarconst * kB_to_3halves *
			//max(MINIMUM_NU_EI_DENSITY,our_n.n) *
			// do we want to enhance nu here or not?
			our_n.n*
			Get_lnLambda_d(our_n.n, T.Te) / (T.Te*sqrt_T);

		//if ((our_n.n < LOW_THRESH_FOR_VISC_CALCS) || (T.Te < 1.1e-14)
		//	|| (info.pos.dot(info.pos) > VISCOSITY_MAX_RADIUS*VISCOSITY_MAX_RADIUS)) {
		if (Selected_ie == 0) {
			p_ita_par_elec_minor[index] = 0.0;
		} else {
			p_ita_par_elec_minor[index] =
				//0.73*our_n.n*T.Te / nu_eiBar; // ? Check it's not in eV in formulary
				0.5*our_n.n*T.Te / ((0.3*0.87 + 0.6)*nu_eiBar + 0.6*nu_en_visc);
			// This from Zhdanov chapter 7. Compare Braginskii.
			// 0.5/(0.3*0.87+0.6) = 0.58 not 0.73
		};
		p_nu_elec_minor[index] = (0.3*0.87 + 0.6)*nu_eiBar + 0.6*nu_en_visc;

		if (T.Te <= 0.0) printf("ita calc: Negative Te encountered iMinor = %d /n", index);

//		if ((index == 85822) || (index == 24335))
//			printf("\n###################################\nindex %d nu_e %1.14E ita %1.14E n %1.14E Te %1.14E \nnu_eiBar %1.14E nu_en_visc %1.14E\n\n",
//				index, (0.3*0.87 + 0.6)*nu_eiBar + 0.6*nu_en_visc, p_ita_par_elec_minor[index],
//				our_n.n, T.Te, nu_eiBar, nu_en_visc);
		
		//nu_eHeart:
	//	nu.e = nu_en_visc + 1.87*nu_eiBar;

	// FOR NOW IT DOES NOT MATTER BECAUSE WE IMPLEMENTED UNMAGNETISED
	// HOWEVER, WE SHOULD PROBABLY EXPECT THAT IN THE END OUR NU_eHEART IS RELEVANT TO OUTPUT HERE

		// TeV = T.Ti*one_over_kB;
		// Estimate_Ion_Neutral_Cross_sections_d(TeV, &sigma_MT, &sigma_visc); // could easily save one call
		sqrt_T = sqrt(T.Ti); // again not that hard to save one call
		nu_in_visc = our_n.n_n * sigma_visc * sqrt(T.Ti / m_ion + T.Tn / m_n);
		nu_ni_visc = nu_in_visc * (our_n.n / our_n.n_n);
		nu_ii = our_n.n*kB_to_3halves*Get_lnLambda_ion_d(our_n.n, T.Ti) *Nu_ii_Factor / (sqrt_T*T.Ti);
				
		//if (index == CHOSEN) printf("got to here. T.Ti %1.9E our_n.n %1.9E LOW_THRESH %1.9E \n",
		//	T.Ti, our_n.n, LOW_THRESH_FOR_VISC_CALCS);

		//if ((our_n.n < LOW_THRESH_FOR_VISC_CALCS) || (T.Ti < 1.1e-14)
		//	|| (info.pos.dot(info.pos) > VISCOSITY_MAX_RADIUS*VISCOSITY_MAX_RADIUS))
		
		if (Selected_ie == 0) {
			p_ita_par_ion_minor[index] = 0.0;
		} else {
			p_ita_par_ion_minor[index] = 0.5*our_n.n*T.Ti / (0.3*nu_ii + 0.4*nu_in_visc + 0.000273*nu_eiBar);
		};
		
		//0.96*our_n.n*T.Ti / nu_ii; // Formulary
		p_nu_ion_minor[index] = 0.3*nu_ii + 0.4*nu_in_visc + 0.000273*nu_eiBar;
		

		// I think we just need to knock out points where ita_par_ion_minor ends up low
		// rather than specifically low density.

		// Trouble is that will just give us nothing out of a shock front.

		// Maybe need to be more careful -- go 2 passes


		// What applies to viscosity will surely apply to heat conduction also.
		// We'll have to compare Zhdanov with what we have got.

		//nu_nn_visc:
		nu_nn = our_n.n_n * Estimate_Neutral_Neutral_Viscosity_Cross_section_d(T.Tn * one_over_kB)
			* sqrt(T.Tn / m_n);
						
		f64 nu_n = 0.3*nu_nn + 0.4*nu_ni_visc;

		// This is set at 1e10
		//if ((our_n.n_n < LOW_THRESH_FOR_VISC_CALCS)
		//	|| (info.pos.dot(info.pos) > VISCOSITY_MAX_RADIUS*VISCOSITY_MAX_RADIUS)) {
		if (Selected_neut == 0){
			p_ita_neutral_minor[index] = 0.0;
		} else {
			f64 ita = our_n.n_n*T.Tn / nu_n;
			
			// NEW THRESHOLD HERE TO STOP NEUTRAL VISC:
			if (T.Tn < 1.1e-14) ita = 0.0;

			p_ita_neutral_minor[index] = ita;
		};

		p_nu_nn_visc[index] = nu_n; 

	} else {
		p_ita_par_elec_minor[index] = 0.0;
		p_nu_elec_minor[index] = 0.0;
		p_ita_par_ion_minor[index] = 0.0;
		p_nu_ion_minor[index] = 0.0;
		p_ita_neutral_minor[index] = 0.0;
		p_nu_nn_visc[index] = 0.0;		
		// For CROSSING_CATH this is fine.
	}
}

// Historical record
// We have to reform heat cond so that we use min n but a good T throughout kappa
// We do try to use same n in numer and denom, including in ln Lambda.
// We have too many spills in the routine so we should make a separate ionisation routine
// and try to do species separately.


/*
__global__ void kernelAccumulateDiffusiveHeatRate_new_Longitudinalonly(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	long * __restrict__ pIndexNeigh,
	char * __restrict__ pPBCNeigh,
	long * __restrict__ izTri_verts,
	char * __restrict__ szPBCtri_verts,
	f64_vec2 * __restrict__ p_cc,

	nvals * __restrict__ p_n_major,
	T3 * __restrict__ p_T_major,
	T3 * __restrict__ p_T_k,
	f64_vec3 * __restrict__ p_B_major,

	f64 * __restrict__ p_kappa_n,
	f64 * __restrict__ p_kappa_i,
	f64 * __restrict__ p_kappa_e,

	f64 * __restrict__ p_nu_i,
	f64 * __restrict__ p_nu_e,

	NTrates * __restrict__ NTadditionrates,
	f64 * __restrict__ p_AreaMajor)
{
	// Think we might as well take kappa_par and nu from triangles really.
	// If n is modelled well then hopefully a nearby high-n does not have a big impact.

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajorClever]; // 2
	__shared__ f64_vec2 shared_pos[2 * threadsPerTileMajorClever];

	__shared__ f64 shared_T[threadsPerTileMajorClever];      // +3
															 //__shared__ f64 shared_T[threadsPerTileMajorClever];

	__shared__ f64_vec2 shared_B[threadsPerTileMajorClever]; // +2

															 // B is smooth. Unfortunately we have not fitted in Bz here.
															 // In order to do that perhaps rewrite so that variables are overwritten in shared.
															 // We do not need all T and nu in shared at the same time.
															 // This way is easier for NOW.
	__shared__ f64 shared_kappa[threadsPerTileMajorClever * 2];
	__shared__ f64 shared_nu[threadsPerTileMajorClever * 2];

	__shared__ long Indexneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // assume 48 bytes = 4*12 = 6 doubles

	__shared__ char PBCneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // 12 bytes each from L1. Have 42 per thread at 384 threads.
																	// We should at any rate try a major block of size 256. If 1 block will run at a time, so be it.
																	// Leave L1 in case of register overflow into it. <-- don't know how likely - do we only have 31 doubles in registry
																	// regardless # of threads and space? Or can be 63?
	__shared__ char PBCtri[MAXNEIGH_d*threadsPerTileMajorClever];
	// Balance of shared vs L1: 24*256*8 = 48K. That leaves 8 doublesworth in L1 for variables.

	long izTri[MAXNEIGH_d];  // so only 2 doubles left in L1. 31 in registers??

							 // Set threadsPerTileMajorClever to 256.

							 // It would help matters if we get rid of T3. We might as well therefore change to scalar flatpack T.

							 // We are hoping that it works well loading kappa(tri) and that this is not upset by nearby values. Obviously a bit of an experiment.

							 // Does make it somewhat laughable that we go to such efforts to reduce global accesses when we end up overflowing anyway. 
							 // If we can fit 24 doubles/thread in 48K that means we can fit 8 doubles/thread in 16K so that's most of L1 used up.

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	long const StartMinor = blockIdx.x*blockDim.x * 2;
	long const EndMinor = StartMinor + blockDim.x * 2;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	shared_pos_verts[threadIdx.x] = info.pos;
#ifdef CENTROID_HEATCONDUCTION
	{
		structural infotemp[2];
		memcpy(infotemp, p_info_minor + 2 * iVertex, 2 * sizeof(structural));
		shared_pos[threadIdx.x * 2] = infotemp[0].pos;
		shared_pos[threadIdx.x * 2 + 1] = infotemp[1].pos;
		f64 tempf64[2];
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
		// No nu to set for neutrals - not used
	}
#else
	{
		memcpy(&(shared_pos[threadIdx.x * 2]), p_cc + 2 * iVertex, 2 * sizeof(f64_vec2));
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
	}
#endif


	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_B[threadIdx.x] = p_B_major[iVertex].xypart();
		shared_T[threadIdx.x] = p_T_major[iVertex].Tn;
	}
	else {
		// SHOULD NOT BE LOOKING INTO INS.
		// How do we avoid? IMPORTANT
		memset(&(shared_B[threadIdx.x]), 0, sizeof(f64_vec2));
		shared_T[threadIdx.x] = 0.0;
	}


	__syncthreads();

	f64_vec2 grad_T;
	f64 T_anti, T_clock, T_out, T_outk;		// 5
	f64_vec2 pos_clock, pos_anti, pos_out;   // +6
	f64_vec2 B_out;       // +2
	NTrates ourrates;      // +5
	f64 kappa_parallel; // do we use them all at once or can we save 2 doubles here?
	f64 nu;                // 20 there  
	f64_vec2 edge_normal;  // 22
	f64_vec2 endpt_anti;    // 24 .. + 6 from above
	long indexneigh;     // into the 2-double buffer in L1
	f64_vec2 endpt_clock;    // As we only use endpt_anti afterwords we could union endpt_clock with edge_normal
							 // Come back and optimize by checking which things we need in scope at the same time?

	short iNeigh; // only fixed # of addresses so short makes no difference.
	char PBC; // char makes no difference.


	if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
	{
		// [ Ignore flux into edge of outermost vertex I guess ???]
	}
	else {
		if (info.flag == DOMAIN_VERTEX) {
			// The idea of not sending blocks full of non-domain vertices is another idea. Fiddly with indices.

			// Need this, we are adding on to existing d/dt N,NT :
			memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));

			memcpy(Indexneigh + MAXNEIGH_d * threadIdx.x,
				pIndexNeigh + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(long));
			memcpy(PBCneigh + MAXNEIGH_d * threadIdx.x,
				pPBCNeigh + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(char));
			memcpy(PBCtri + MAXNEIGH_d * threadIdx.x,
				szPBCtri_verts + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(char));

			// EXPERIMENT WHETHER IT IS FASTER WITH THESE OUTSIDE OR INSIDE THE BRANCH.


			memcpy(izTri, //+ MAXNEIGH_d * threadIdx.x,
				izTri_verts + MAXNEIGH_d * iVertex, MAXNEIGH_d * sizeof(long));

			// Now do Tn:

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_clock = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_clock = info2.pos;
#ifdef BWDSIDET
				T_clock = p_T_major[indexneigh].Tn;
#endif
			};
			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == NEEDS_ANTI) {
				pos_clock = Anticlock_rotate2(pos_clock);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_clock = Clockwise_rotate2(pos_clock);
			};
#ifndef BWDSIDET
			T_clock = p_T_k[indexneigh].Tn;
#endif

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_out = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_out = info2.pos;
#ifdef BWDSIDET
				T_out = p_T_major[indexneigh].Tn;
#endif
			};
#ifndef BWDSIDET
			T_outk = p_T_k[indexneigh].Tn; // ready for switch around
#endif

			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
			if (PBC == NEEDS_ANTI) {
				pos_out = Anticlock_rotate2(pos_out);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_out = Clockwise_rotate2(pos_out);
			};

			if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
			{
				endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
			}
			else {
#ifdef CENTROID_HEATCONDUCTION
				endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
				endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
			}
			PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
			if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;

			if (T_clock == 0.0) {
#ifdef BWDSIDET
				T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
#else
				T_clock = T_outk;
#endif
			};

#pragma unroll MAXNEIGH_d
			for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
			{
				{
					short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
				}
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_anti = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_anti = info2.pos;
#ifdef BWDSIDET
					T_anti = p_T_major[indexneigh].Tn;
#endif
				};
				if (PBC == NEEDS_ANTI) {
					pos_anti = Anticlock_rotate2(pos_anti);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_anti = Clockwise_rotate2(pos_anti);
				};
#ifndef BWDSIDET
				T_anti = p_T_k[indexneigh].Tn; // Stupid 3-struct

											   // Also need to update T_opp if it was not done already

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					T_out = shared_T[indexneigh - StartMajor];
				}
				else {
					T_out = p_T_major[indexneigh].Tn;
				};
#endif

				if (T_anti == 0.0) {
#ifdef BWDSIDET
					T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
#else
					T_anti = T_outk;
#endif					
				}; // So we are receiving 0 then doing this. But how come?

				   // Now let's see
				   // tri 0 has neighs 0 and 1 I'm pretty sure (check....) CHECK

				if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
				{
					endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_anti = p_info_minor[izTri[iNeigh]].pos;
					// we should switch back to centroids!!
#else
					endpt_anti = p_cc[izTri[iNeigh]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

				edge_normal.x = (endpt_anti.y - endpt_clock.y);
				edge_normal.y = (endpt_clock.x - endpt_anti.x);

				if (TEST) {
					printf("%d contrib %1.8E \n"
						"pos_anti %1.9E %1.9E pos_out %1.9E %1.9E pos_clock %1.9E %1.9E\n", iVertex,
						0.5*edge_normal.x*THIRD*(pos_anti.x + pos_clock.x
							+ info.pos.x + info.pos.x + pos_out.x + pos_out.x),
						pos_anti.x, pos_anti.y, pos_out.x, pos_out.y, pos_clock.x, pos_clock.y);
				}

				// SMARTY:
				if (pos_out.x*pos_out.x + pos_out.y*pos_out.y >
					DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{



					// How to detect? Loading a load of flags is a killer! We do need to load ... and this is why we should have not made info struct. Def not.

					//// 
					//if (insulator triangle)
					//{
					//	centroid1 = THIRD*(pos_anti + pos_out + info.pos);
					//	// project to radius of insulator
					//	centroid1.project_to_radius(3.44);
					//	// Now dot with unit vectors:
					//	f64_vec2 tempvec2;
					//	tempvec2.x = unit_vec1.x*centroid1.x + unit_vec1.y*centroid1.y;
					//	tempvec2.y = unit_vec2.x*centroid1.x + unit_vec2.y*centroid1.y;
					//	centroid1.x = tempvec2.x;
					//	centroid1.y = tempvec2.y;
					//} else {
					//	// centroid1 = THIRD*(pos_anti_twist + pos_out_twist);
					//	centroid1.x = THIRD*(
					//		  unit_vec1.x*(pos_anti.x - info.pos.x) + unit_vec1.y*(pos_anti.y - info.pos.y)
					//		+ unit_vec1.x*(pos_out.x - info.pos.x) + unit_vec1.y*(pos_out.y - info.pos.y)
					//		);
					//	centroid1.y = THIRD*(
					//		- unit_vec1.y*(pos_anti.x - info.pos.x) + unit_vec1.x*(pos_anti.y - info.pos.y)
					//		- unit_vec1.y*(pos_out.x - info.pos.x) + unit_vec1.x*(pos_out.y - info.pos.y)
					//		);
					//}

					//if (insulator triangle)
					//{
					//	centroid2 = THIRD*(pos_clock + pos_out + info.pos);

					//	// project to radius of insulator
					//} else {

					//}


					kappa_parallel = 0.0;
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p_kappa_n[izTri[iNeigh]];
					}
					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p_kappa_n[izTri[iPrev]];
						}
					}

					if ((pos_clock.x*pos_clock.x + pos_clock.y*pos_clock.y <
						DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
						||
						(pos_anti.x*pos_anti.x + pos_anti.y*pos_anti.y <
							DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER))
					{
						f64 edgelen = edge_normal.modulus();

						ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
							(T_out - shared_T[threadIdx.x]) / (pos_out - info.pos).modulus();
					}
					else {
						f64 Area_quadrilateral = 0.5*(
							(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
							+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
							+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
							+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
							);

						// When we come to do the other species, make a subroutine.

						grad_T.x = 0.5*( // notice minus
							(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
							+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
							+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
							+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
							) / Area_quadrilateral;

						grad_T.y = -0.5*( // notice minus
							(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
							+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
							+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
							+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
							) / Area_quadrilateral;

						ourrates.NnTn += TWOTHIRDS * kappa_parallel * grad_T.dot(edge_normal);
					}
				} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

				  // Now go round:	
				endpt_clock = endpt_anti;
				pos_clock = pos_out;
				pos_out = pos_anti;
#ifdef BWDSIDET
				T_clock = T_out;
				T_out = T_anti;
#else
				T_clock = T_outk;
				T_outk = T_anti;
#endif

			}; // next iNeigh

		}; // was it DOMAIN_VERTEX? Do what otherwise?
	}; // was it OUTERMOST/INNERMOST?

	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################

	{
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_i + 2 * iVertex, 2 * sizeof(f64));
		memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_i + 2 * iVertex, 2 * sizeof(f64));
	}
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_T[threadIdx.x] = p_T_major[iVertex].Ti;
		// Notice major inefficiency caused by not making them scalar T arrays
	}
	else {
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();

	if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
	{
		// [ Ignore flux into edge of outermost vertex I guess ???]
	}
	else {
		if (info.flag == DOMAIN_VERTEX) {

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_clock = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_clock = info2.pos;
#ifdef BWDSIDET
				T_clock = p_T_major[indexneigh].Ti;
#endif
			};
			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == NEEDS_ANTI) {
				pos_clock = Anticlock_rotate2(pos_clock);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_clock = Clockwise_rotate2(pos_clock);
			};
#ifndef BWDSIDET
			T_clock = p_T_k[indexneigh].Ti;
#endif

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_out = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_out = info2.pos;
#ifdef BWDSIDET
				T_out = p_T_major[indexneigh].Ti;
#endif
			};
			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
			if (PBC == NEEDS_ANTI) {
				pos_out = Anticlock_rotate2(pos_out);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_out = Clockwise_rotate2(pos_out);
			};

#ifndef BWDSIDET
			T_outk = p_T_k[indexneigh].Ti;
#endif

			if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
			{
				endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
			}
			else {
#ifdef CENTROID_HEATCONDUCTION
				endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
				endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
			}
			PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
			if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;

			if (T_clock == 0.0) {
#ifdef BWDSIDET
				T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
#else
				T_clock = T_outk;
#endif
			};

#pragma unroll MAXNEIGH_d
			for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
			{
				{
					short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
				}
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_anti = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_anti = info2.pos;
#ifdef BWDSIDET
					T_anti = p_T_major[indexneigh].Ti;
#endif
				};
#ifndef BWDSIDET
				T_anti = p_T_k[indexneigh].Ti;

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					T_out = shared_T[indexneigh - StartMajor];
				}
				else {
					T_out = p_T_major[indexneigh].Ti;
				};
#endif
				if (PBC == NEEDS_ANTI) {
					pos_anti = Anticlock_rotate2(pos_anti);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_anti = Clockwise_rotate2(pos_anti);
				};

				if (T_anti == 0.0) {
#ifdef BWDSIDET
					T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
#else
					T_anti = T_outk;
#endif					
				}; // So we are receiving 0 then doing this. But how come?

				if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
				{
					endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
					endpt_anti = p_cc[izTri[iNeigh]];
#endif					
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

				edge_normal.x = (endpt_anti.y - endpt_clock.y);
				edge_normal.y = (endpt_clock.x - endpt_anti.x);

				// SMARTY:
				if (pos_out.x*pos_out.x + pos_out.y*pos_out.y >
					DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{
					//f64 Area_quadrilateral = 0.5*(
					//	(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
					//	+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
					//	+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
					//	+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
					//	);
					//grad_T.x = 0.5*(
					//	(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
					//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
					//	+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
					//	+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
					//	) / Area_quadrilateral;
					//grad_T.y = -0.5*( // notice minus
					//	(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
					//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
					//	+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
					//	+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
					//	) / Area_quadrilateral;

					kappa_parallel = 0.0;
					f64 nu;
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
						nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p_kappa_i[izTri[iNeigh]];
						nu = 0.5*p_nu_i[izTri[iNeigh]];
					};
					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
							nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p_kappa_i[izTri[iPrev]];
							nu += 0.5*p_nu_i[izTri[iPrev]];
						}
					}

					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						B_out = shared_B[indexneigh - StartMajor];
					}
					else {
						f64_vec3 B_out3 = p_B_major[indexneigh];
						B_out = B_out3.xypart();
					}
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
					if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

					//kappa.xx = kappa_parallel * (nu_eHeart*nu_eHeart + omega.x*omega.x) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.xy = kappa_parallel * (omega.x*omega.y - nu_eHeart *omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.yx = kappa_parallel * (omega.x*omega.y + nu_eHeart * omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.yy = kappa_parallel * (omega.y*omega.y + nu_eHeart * nu_eHeart) / (nu_eHeart * nu_eHeart + omega_sq);

					{ // scoping brace
						f64_vec3 omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);
						// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?
						//
						//						ourrates.NiTi += TWOTHIRDS * kappa_parallel *(
						//							edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y + nu * omega.z)*grad_T.y)
						//							+ edge_normal.y*((omega.x*omega.y - nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
						//							) / (nu * nu + omega.dot(omega));
						f64 edgelen = edge_normal.modulus();
						f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

						ourrates.NiTi += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
							(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
							/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));


						if (TEST) printf("%d iNeigh %d kappa_ion %1.8E nu %1.8E |o| %1.8E contrib %1.8E \n",
							iVertex, iNeigh, kappa_parallel, nu,
							omega.modulus(),
							TWOTHIRDS * kappa_parallel *(
								edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y + nu * omega.z)*grad_T.y)
								+ edge_normal.y*((omega.x*omega.y - nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
								) / (nu * nu + omega.dot(omega))
						);

					}
				} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

				  // Now go round:	
				endpt_clock = endpt_anti;
				pos_clock = pos_out;
				pos_out = pos_anti;
#ifndef BWDSIDET
				T_clock = T_outk;
				T_outk = T_anti;
#else
				T_clock = T_out;
				T_out = T_anti;
#endif

			}; // next iNeigh

		}; // was it DOMAIN_VERTEX? Do what otherwise?
	}; // was it OUTERMOST/INNERMOST?


	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################

	{
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_e + 2 * iVertex, 2 * sizeof(f64));
		memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_e + 2 * iVertex, 2 * sizeof(f64));
	}
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_T[threadIdx.x] = p_T_major[iVertex].Te;
	}
	else {
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();


	if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
	{
		// [ Ignore flux into edge of outermost vertex I guess ???]
	}
	else {
		if (info.flag == DOMAIN_VERTEX) {

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_clock = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_clock = info2.pos;
#ifdef BWDSIDET
				T_clock = p_T_major[indexneigh].Te;
#endif
			};
#ifndef BWDSIDET
			T_clock = p_T_k[indexneigh].Te;
#endif

			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == NEEDS_ANTI) {
				pos_clock = Anticlock_rotate2(pos_clock);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_clock = Clockwise_rotate2(pos_clock);
			};

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
				T_out = shared_T[indexneigh - StartMajor];
#endif
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_out = info2.pos;
#ifdef BWDSIDET
				T_out = p_T_major[indexneigh].Te;
#endif
			};
#ifndef BWDSIDET
			T_outk = p_T_k[indexneigh].Te;
#endif
			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
			if (PBC == NEEDS_ANTI) {
				pos_out = Anticlock_rotate2(pos_out);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_out = Clockwise_rotate2(pos_out);
			};

			if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
			{
				endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
			}
			else {
#ifdef CENTROID_HEATCONDUCTION
				endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
				endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
			}
			PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
			if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;

			if (T_clock == 0.0) {
#ifdef BWDSIDET
				T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
#else
				T_clock = T_outk;
#endif
			};

#pragma unroll MAXNEIGH_d
			for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
			{
				{
					short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
				}
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_anti = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_anti = info2.pos;
#ifdef BWDSIDET
					T_anti = p_T_major[indexneigh].Te;
#endif
				};
#ifndef BWDSIDET
				T_anti = p_T_k[indexneigh].Te;
				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					T_out = shared_T[indexneigh - StartMajor];
				}
				else {
					T_out = p_T_major[indexneigh].Te;
				}
#endif
				if (PBC == NEEDS_ANTI) {
					pos_anti = Anticlock_rotate2(pos_anti);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_anti = Clockwise_rotate2(pos_anti);
				};

				if (T_anti == 0.0) {
#ifdef BWDSIDET
					T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
#else
					T_anti = T_outk;
#endif					
				}; // So we are receiving 0 then doing this. But how come?

				if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
				{
					endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];

					if (TEST) {
						printf("%d : %d endpt_anti %1.9E %1.9E SHARED endpt_clock %1.9E %1.9E izTri[iNeigh] %d\n",
							iVertex, iNeigh, endpt_anti.x, endpt_anti.y, endpt_clock.x, endpt_clock.y, izTri[iNeigh]);
					}
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
					endpt_anti = p_cc[izTri[iNeigh]];
#endif

					if (TEST) {
						printf("%d : %d endpt_anti %1.9E %1.9E GLOBAL endpt_clock %1.9E %1.9E izTri[iNeigh] %d\n",
							iVertex, iNeigh, endpt_anti.x, endpt_anti.y, endpt_clock.x, endpt_clock.y, izTri[iNeigh]);
					}
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

				// It decided to rotate something it shouldn't oughta. Rotated tri 23600 = tri 2 for 11582.

				edge_normal.x = (endpt_anti.y - endpt_clock.y);
				edge_normal.y = (endpt_clock.x - endpt_anti.x);

				// SMARTY:
				if (pos_out.x*pos_out.x + pos_out.y*pos_out.y >
					DEVICE_RADIUS_INSULATOR_OUTER*DEVICE_RADIUS_INSULATOR_OUTER)
				{

					// f64 grad_out = (T_out - shared_T[threadIdx.x]) / delta_0out;


					//f64 Area_quadrilateral = 0.5*(
					//	(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
					//	+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
					//	+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
					//	+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
					//	);
					//grad_T.x = 0.5*(
					//	(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
					//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
					//	+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
					//	+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
					//	) / Area_quadrilateral;
					//grad_T.y = -0.5*( // notice minus
					//	(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
					//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
					//	+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
					//	+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
					//	) / Area_quadrilateral;

					kappa_parallel = 0.0;
					f64 nu;
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
						nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p_kappa_e[izTri[iNeigh]];
						nu = 0.5*p_nu_e[izTri[iNeigh]];
					};

					if (TEST) printf("izTri %d kappa_par %1.9E \n",
						izTri[iNeigh], p_kappa_e[izTri[iNeigh]]);

					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
							nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p_kappa_e[izTri[iPrev]];
							nu += 0.5*p_nu_e[izTri[iPrev]];
						}
					}

					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						B_out = shared_B[indexneigh - StartMajor];
					}
					else {
						f64_vec3 B_out3 = p_B_major[indexneigh];
						B_out = B_out3.xypart();
					}
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
					if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

					//kappa.xx = kappa_parallel * (nu_eHeart*nu_eHeart + omega.x*omega.x) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.xy = kappa_parallel * (omega.x*omega.y - nu_eHeart *omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.yx = kappa_parallel * (omega.x*omega.y + nu_eHeart * omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
					//kappa.yy = kappa_parallel * (omega.y*omega.y + nu_eHeart * nu_eHeart) / (nu_eHeart * nu_eHeart + omega_sq);

					{ // scoping brace
						f64_vec3 omega = Make3(qovermc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qovermc);
						// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?

						//		ourrates.NeTe += TWOTHIRDS * kappa_parallel *(
						//			edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y - nu * omega.z)*grad_T.y)
						//			+ edge_normal.y*((omega.x*omega.y + nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
						//			) / (nu * nu + omega.dot(omega));
						f64 edgelen = edge_normal.modulus();
						f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

						ourrates.NeTe += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
							(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
							/ (delta_out*edgelen*(nu * nu + omega.dot(omega)));

						// Expensive debug: remove!

						if (TESTHEAT2) printf(
							"iVertex %d iNeigh %d %d contribNeTe %1.9E edge_normal %1.8E %1.8E \n"
							"T %1.9E Tout %1.9E T_anti %1.9E T_clock %1.9E\n"
							"   kappa_par %1.9E nu %1.9E |omega| %1.9E Area %1.9E\n"
							"our_n %1.9E our n_n %1.9E nearby n %1.9E %1.9E\n"
							"pos %1.8E %1.8E opp %1.8E %1.8E anti %1.8E %1.8E clock %1.8E %1.8E\n"
							"omega %1.8E %1.8E grad_T %1.9E %1.9E \n"
							"=================================================\n",
							iVertex, iNeigh, indexneigh,
							TWOTHIRDS * kappa_parallel *(
								edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y - nu * omega.z)*grad_T.y)
								+ edge_normal.y*((omega.x*omega.y + nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
								) / (nu * nu + omega.dot(omega)),
							edge_normal.x, edge_normal.y, shared_T[threadIdx.x], T_out, T_anti, T_clock,
							kappa_parallel, nu, sqrt(omega.dot(omega)),
							p_AreaMajor[iVertex],
							p_n_major[iVertex].n, p_n_major[iVertex].n_n, p_n_major[indexneigh].n, p_n_major[indexneigh].n_n,
							info.pos.x, info.pos.y, pos_out.x, pos_out.y, pos_anti.x, pos_anti.y, pos_clock.x, pos_clock.y,
							omega.x, omega.y, grad_T.x, grad_T.y);


					}
				} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

				  // Now go round:	
				endpt_clock = endpt_anti;
				pos_clock = pos_out;
				pos_out = pos_anti;
#ifdef BWDSIDET
				T_clock = T_out;
				T_out = T_anti;
#else
				T_clock = T_outk;
				T_outk = T_anti;
#endif

			}; // next iNeigh

			memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));

		}; // was it DOMAIN_VERTEX? Do what otherwise?
	}; // was it OUTERMOST/INNERMOST?

}*/

__global__ void kernelAccumulateDiffusiveHeatRate_new_Longitudinalonly_scalarT(
	structural * __restrict__ p_info_minor,
	long * __restrict__ pIndexNeigh,
	char * __restrict__ pPBCNeigh,
	long * __restrict__ izTri_verts,
	char * __restrict__ szPBCtri_verts,
	f64_vec2 * __restrict__ p_cc,

	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_T_n, f64 * __restrict__ p_T_i, f64 * __restrict__ p_T_e,
//	T3 * __restrict__ p_T_k,
	f64_vec3 * __restrict__ p_B_major,

	f64 * __restrict__ p_kappa_n,
	f64 * __restrict__ p_kappa_i,
	f64 * __restrict__ p_kappa_e,

	f64 * __restrict__ p_nu_i,
	f64 * __restrict__ p_nu_e,

	NTrates * __restrict__ NTadditionrates,
	f64 * __restrict__ p_AreaMajor,
	bool * __restrict__ p_maskbool3,
	bool * __restrict__ p_maskblock,
	bool bUseMask)
{
	// Think we might as well take kappa_par and nu from triangles really.
	// If n is modelled well then hopefully a nearby high-n does not have a big impact.

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajorClever]; // 2

	// DO NOT WANT:
	__shared__ f64_vec2 shared_pos[2 * threadsPerTileMajorClever];

	__shared__ f64 shared_T[threadsPerTileMajorClever];      // +3
															 //__shared__ f64 shared_T[threadsPerTileMajorClever];

	__shared__ f64_vec2 shared_B[threadsPerTileMajorClever]; // +2

															 // B is smooth. Unfortunately we have not fitted in Bz here.
															 // In order to do that perhaps rewrite so that variables are overwritten in shared.
															 // We do not need all T and nu in shared at the same time.
															 // This way is easier for NOW.
	__shared__ f64 shared_kappa[threadsPerTileMajorClever * 2];
	__shared__ f64 shared_nu[threadsPerTileMajorClever * 2];

	__shared__ long Indexneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // assume 48 bytes = 4*12 = 6 doubles

	__shared__ char PBCneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // 12 bytes each from L1. Have 42 per thread at 384 threads.
																	// We should at any rate try a major block of size 256. If 1 block will run at a time, so be it.
																	// Leave L1 in case of register overflow into it. <-- don't know how likely - do we only have 31 doubles in registry
																	// regardless # of threads and space? Or can be 63?
	__shared__ char PBCtri[MAXNEIGH_d*threadsPerTileMajorClever];
	// Balance of shared vs L1: 24*256*8 = 48K. That leaves 8 doublesworth in L1 for variables.

	long izTri[MAXNEIGH_d];  // so only 2 doubles left in L1. 31 in registers??

							 // Set threadsPerTileMajorClever to 256.

							 // It would help matters if we get rid of T3. We might as well therefore change to scalar flatpack T.

							 // We are hoping that it works well loading kappa(tri) and that this is not upset by nearby values. Obviously a bit of an experiment.

							 // Does make it somewhat laughable that we go to such efforts to reduce global accesses when we end up overflowing anyway. 
							 // If we can fit 24 doubles/thread in 48K that means we can fit 8 doubles/thread in 16K so that's most of L1 used up.

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	long const StartMinor = blockIdx.x*blockDim.x * 2;
	long const EndMinor = StartMinor + blockDim.x * 2;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	bool bMask[3];

	if (bUseMask)
		if (p_maskblock[blockIdx.x] == 0) return;

	if (bUseMask) {
		bMask[0] = p_maskbool3[iVertex];
		bMask[1] = p_maskbool3[iVertex + NUMVERTICES];
		bMask[2] = p_maskbool3[iVertex + NUMVERTICES*2];
		//memcpy(bMask, p_maskbool3 + iVertex * 3, 3 * sizeof(bool));
	}

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	shared_pos_verts[threadIdx.x] = info.pos;
#ifdef CENTROID_HEATCONDUCTION
	{
		structural infotemp[2];
		memcpy(infotemp, p_info_minor + 2 * iVertex, 2 * sizeof(structural));
		shared_pos[threadIdx.x * 2] = infotemp[0].pos;
		shared_pos[threadIdx.x * 2 + 1] = infotemp[1].pos;
		f64 tempf64[2];
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
		// No nu to set for neutrals - not used
	}
#else
	{
		memcpy(&(shared_pos[threadIdx.x * 2]), p_cc + 2 * iVertex, 2 * sizeof(f64_vec2));
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
	}
#endif
	
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_B[threadIdx.x] = p_B_major[iVertex].xypart();
		shared_T[threadIdx.x] = p_T_n[iVertex];
	} else {
		// SHOULD NOT BE LOOKING INTO INS.
		// How do we avoid?
		memset(&(shared_B[threadIdx.x]), 0, sizeof(f64_vec2));
		shared_T[threadIdx.x] = 0.0;
	}
	
	__syncthreads();

	f64_vec2 grad_T;
	f64 T_anti, T_clock, T_out, T_outk;		// 5
	f64_vec2 pos_clock, pos_anti, pos_out;   // +6
	f64_vec2 B_out;       // +2
	NTrates ourrates;      // +5
	f64 kappa_parallel; // do we use them all at once or can we save 2 doubles here?
	f64 nu;                // 20 there  
	f64_vec2 edge_normal;  // 22
	f64_vec2 endpt_anti;    // 24 .. + 6 from above
	long indexneigh;     // into the 2-double buffer in L1
	f64_vec2 endpt_clock;    // As we only use endpt_anti afterwords we could union endpt_clock with edge_normal
							 // Come back and optimize by checking which things we need in scope at the same time?

	short iNeigh; // only fixed # of addresses so short makes no difference.
	char PBC; // char makes no difference.

	if  ( (info.flag == DOMAIN_VERTEX) && (
		((bUseMask == 0) || (bMask[0] == true) || (bMask[1] == true) || (bMask[2] == true))))
	{
		// Need this, we are adding on to existing d/dt N,NT :
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));

		memcpy(Indexneigh + MAXNEIGH_d * threadIdx.x,
			pIndexNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(long));
		memcpy(PBCneigh + MAXNEIGH_d * threadIdx.x,
			pPBCNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));
		memcpy(PBCtri + MAXNEIGH_d * threadIdx.x,
			szPBCtri_verts + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));

		memcpy(izTri, //+ MAXNEIGH_d * threadIdx.x,
			izTri_verts + MAXNEIGH_d * iVertex, MAXNEIGH_d * sizeof(long));
	};

	if ((bUseMask == 0) || (bMask[0] == true) )
	{
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
		{
			// [ Ignore flux into edge of outermost vertex I guess ???]
		}
		else {
			if (info.flag == DOMAIN_VERTEX) {
				// The idea of not sending blocks full of non-domain vertices is another idea. Fiddly with indices.

				// Now do Tn:

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_clock = shared_pos_verts[indexneigh - StartMajor];
					T_clock = shared_T[indexneigh - StartMajor];
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_clock = info2.pos;
					T_clock = p_T_n[indexneigh];
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == NEEDS_ANTI) {
					pos_clock = Anticlock_rotate2(pos_clock);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_clock = Clockwise_rotate2(pos_clock);
				};

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_out = shared_pos_verts[indexneigh - StartMajor];
					T_out = shared_T[indexneigh - StartMajor];
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_out = info2.pos;
					T_out = p_T_n[indexneigh]; // saved nothing here, only in loading
				};

				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
				if (PBC == NEEDS_ANTI) {
					pos_out = Anticlock_rotate2(pos_out);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_out = Clockwise_rotate2(pos_out);
				};

				if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
				{
					endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
					endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;
//
//				if (T_clock == 0.0) {
//					T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
//				};
//				Mimic

#pragma unroll MAXNEIGH_d
				for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
				{
					{
						short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
					}
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						pos_anti = shared_pos_verts[indexneigh - StartMajor];
						T_anti = shared_T[indexneigh - StartMajor];
					}
					else {
						structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
						pos_anti = info2.pos;
						T_anti = p_T_n[indexneigh];
					};
					if (PBC == NEEDS_ANTI) {
						pos_anti = Anticlock_rotate2(pos_anti);
					};
					if (PBC == NEEDS_CLOCK) {
						pos_anti = Clockwise_rotate2(pos_anti);
					};

					//if (T_anti == 0.0) {
					//	T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
					//}; // So we are receiving 0 then doing this. But how come?
					//Mimic

					   // Now let's see
					   // tri 0 has neighs 0 and 1 I'm pretty sure (check....) CHECK

					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
					}
					else {
#ifdef CENTROID_HEATCONDUCTION
						endpt_anti = p_info_minor[izTri[iNeigh]].pos;
						// we should switch back to centroids!!
#else
						endpt_anti = p_cc[izTri[iNeigh]];
#endif
					}
					PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
					if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

					edge_normal.x = (endpt_anti.y - endpt_clock.y);
					edge_normal.y = (endpt_clock.x - endpt_anti.x);

					if (0) {
						printf("%d contrib %1.8E \n"
							"pos_anti %1.9E %1.9E pos_out %1.9E %1.9E pos_clock %1.9E %1.9E\n",
							iVertex,
							0.5*edge_normal.x*THIRD*(pos_anti.x + pos_clock.x
								+ info.pos.x + info.pos.x + pos_out.x + pos_out.x),
							pos_anti.x, pos_anti.y, pos_out.x, pos_out.y, pos_clock.x, pos_clock.y);
					}

					// SMARTY:
					if (TestDomainPos(pos_out))
					{


						kappa_parallel = 0.0;
						if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
						{
							kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
						}
						else {
							kappa_parallel = 0.5*p_kappa_n[izTri[iNeigh]];
						}
						{
							short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
							if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
							{
								kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
							}
							else {
								kappa_parallel += 0.5*p_kappa_n[izTri[iPrev]];
							}
						}

						if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))
						{
							f64 edgelen = edge_normal.modulus();

							ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
								(T_out - shared_T[threadIdx.x]) / (pos_out - info.pos).modulus();
						}
						else {
							f64 Area_quadrilateral = 0.5*(
								(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
								+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
								+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
								+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
								);

							// When we come to do the other species, make a subroutine.

							grad_T.x = 0.5*( // notice minus
								(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
								+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
								+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
								) / Area_quadrilateral;

							grad_T.y = -0.5*( // notice minus
								(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
								+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
								+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
								) / Area_quadrilateral;

							ourrates.NnTn += TWOTHIRDS * kappa_parallel * grad_T.dot(edge_normal);
						}
						// This is correct, grad T in same coordinates as edge_normal...

					} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

					  // Now go round:	
					endpt_clock = endpt_anti;
					pos_clock = pos_out;
					pos_out = pos_anti;
					T_clock = T_out;
					T_out = T_anti;
				}; // next iNeigh

			}; // was it DOMAIN_VERTEX? Do what otherwise?
		}; // was it OUTERMOST/INNERMOST?
	}; // mask

	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################
	
	
#pragma unroll
	for (int iSpecies = 1; iSpecies < 3; iSpecies++)
	{
		if (iSpecies == 1)
		{
			memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_i + 2 * iVertex, 2 * sizeof(f64));
			memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_i + 2 * iVertex, 2 * sizeof(f64));
			if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
				shared_T[threadIdx.x] = p_T_i[iVertex];
				// Notice major inefficiency caused by not making them scalar T arrays
			}
			else {
				shared_T[threadIdx.x] = 0.0;
			}
		}
		else {
			memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_e + 2 * iVertex, 2 * sizeof(f64));
			memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_e + 2 * iVertex, 2 * sizeof(f64));
			if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
				shared_T[threadIdx.x] = p_T_e[iVertex];
				// Notice major inefficiency caused by not making them scalar T arrays
			}
			else {
				shared_T[threadIdx.x] = 0.0;
			}
		};
		// Maybe this alone means combining the ion & electron code was stupid. Maybe it can't make contig access.
		
		__syncthreads();

		if ((bUseMask == 0) || (bMask[iSpecies] == true)) // either there is no masking, or this is switched on
		{
			
			{
				if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
				{
					// [ Ignore flux into edge of outermost vertex I guess ???]
				}
				else {
					if (info.flag == DOMAIN_VERTEX) {

						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
						if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
						{
							pos_clock = shared_pos_verts[indexneigh - StartMajor];
							T_clock = shared_T[indexneigh - StartMajor];
						}
						else {
							structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
							pos_clock = info2.pos;
							if (iSpecies == 1) {
								T_clock = p_T_i[indexneigh];
							}
							else {
								T_clock = p_T_e[indexneigh];
							};
						};
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
						if (PBC == NEEDS_ANTI) {
							pos_clock = Anticlock_rotate2(pos_clock);
						};
						if (PBC == NEEDS_CLOCK) {
							pos_clock = Clockwise_rotate2(pos_clock);
						};

						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
						if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
						{
							pos_out = shared_pos_verts[indexneigh - StartMajor];
							T_out = shared_T[indexneigh - StartMajor];
						}
						else {
							structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
							pos_out = info2.pos;
							if (iSpecies == 1) {
								T_out = p_T_i[indexneigh];
							}
							else {
								T_out = p_T_e[indexneigh];
							};
						};
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
						if (PBC == NEEDS_ANTI) {
							pos_out = Anticlock_rotate2(pos_out);
						};
						if (PBC == NEEDS_CLOCK) {
							pos_out = Clockwise_rotate2(pos_out);
						};

						if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
						{
							endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
						}
						else {
#ifdef CENTROID_HEATCONDUCTION
							endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
							endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
						}
						PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
						if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
						if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;
//
//						if (T_clock == 0.0) {
//							T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
//						};
//						Mimic

#pragma unroll MAXNEIGH_d
						for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
						{
							{
								short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
								indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
								PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
							}
							if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
							{
								pos_anti = shared_pos_verts[indexneigh - StartMajor];
								T_anti = shared_T[indexneigh - StartMajor];
							}
							else {
								structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
								pos_anti = info2.pos;
								if (iSpecies == 1)
								{
									T_anti = p_T_i[indexneigh];
								}
								else {
									T_anti = p_T_e[indexneigh];
								};
							};
							if (PBC == NEEDS_ANTI) {
								pos_anti = Anticlock_rotate2(pos_anti);
							};
							if (PBC == NEEDS_CLOCK) {
								pos_anti = Clockwise_rotate2(pos_anti);
							};

						//	if (T_anti == 0.0) {
						//		T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
						//	}; // So we are receiving 0 then doing this. But how come?


							// BUG -- masked stuff will go wrong.


							if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
							{
								endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
							}
							else {
#ifdef CENTROID_HEATCONDUCTION
								endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
								endpt_anti = p_cc[izTri[iNeigh]];
#endif					
							}
							PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
							if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
							if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

							edge_normal.x = (endpt_anti.y - endpt_clock.y);
							edge_normal.y = (endpt_clock.x - endpt_anti.x);

							// SMARTY:
							if (TestDomainPos(pos_out))
							{
								//f64 Area_quadrilateral = 0.5*(
								//	(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
								//	+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
								//	+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
								//	+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
								//	);
								//grad_T.x = 0.5*(
								//	(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
								//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
								//	+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
								//	+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
								//	) / Area_quadrilateral;
								//grad_T.y = -0.5*( // notice minus
								//	(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
								//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
								//	+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
								//	+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
								//	) / Area_quadrilateral;

								kappa_parallel = 0.0;
								f64 nu;
								if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
								{
									kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
									nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
								}
								else {
									if (iSpecies == 1) {
										kappa_parallel = 0.5*p_kappa_i[izTri[iNeigh]];
										nu = 0.5*p_nu_i[izTri[iNeigh]];
									}
									else {
										kappa_parallel = 0.5*p_kappa_e[izTri[iNeigh]];
										nu = 0.5*p_nu_e[izTri[iNeigh]];
									};
								};
								{
									short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
									if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
									{
										kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
										nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
									}
									else {
										if (iSpecies == 1) {
											kappa_parallel += 0.5*p_kappa_i[izTri[iPrev]];
											nu += 0.5*p_nu_i[izTri[iPrev]];
										}
										else {
											kappa_parallel += 0.5*p_kappa_e[izTri[iPrev]];
											nu += 0.5*p_nu_e[izTri[iPrev]];
										};
									}

								}

								indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
								if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
								{
									B_out = shared_B[indexneigh - StartMajor];
								}
								else {
									f64_vec3 B_out3 = p_B_major[indexneigh];
									B_out = B_out3.xypart();
								}
								PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
								if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
								if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

								//kappa.xx = kappa_parallel * (nu_eHeart*nu_eHeart + omega.x*omega.x) / (nu_eHeart * nu_eHeart + omega_sq);
								//kappa.xy = kappa_parallel * (omega.x*omega.y - nu_eHeart *omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
								//kappa.yx = kappa_parallel * (omega.x*omega.y + nu_eHeart * omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
								//kappa.yy = kappa_parallel * (omega.y*omega.y + nu_eHeart * nu_eHeart) / (nu_eHeart * nu_eHeart + omega_sq);

								{ // scoping brace


									f64_vec3 omega;
									if (iSpecies == 1) {
										omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);
									}
									else {
										omega = Make3(qovermc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qovermc);
									};
									// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?
			//
			//						ourrates.NiTi += TWOTHIRDS * kappa_parallel *(
			//							edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y + nu * omega.z)*grad_T.y)
			//							+ edge_normal.y*((omega.x*omega.y - nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
			//							) / (nu * nu + omega.dot(omega));
									f64 edgelen = edge_normal.modulus();
									f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));
								

									// We don't need test for T == 0 because we don't use anti or clock
									// and we ruled out looking into insulator.


									if (iSpecies == 1) {
																				
										ourrates.NiTi += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
												(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
												/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));										


										//if (iVertex == VERTCHOSEN) {
										//	printf("iVertex %d T_out T_our %1.10E %1.10E contrib %1.10E\n",
										//		iVertex, T_out, shared_T[threadIdx.x],
										//		TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
										//		(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
										//		/ (delta_out*edgelen *(nu * nu + omega.dot(omega)))
										//	);
										//}

									} else {									
										ourrates.NeTe += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
												(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
												/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));											
									};
									
									if ((TESTHEAT1) && (iSpecies == 2))
										printf("%d iNeigh %d %d e factor %1.12E contrib %1.12E T_out %1.12E T_self %1.12E\n",
											iVertex, iNeigh, indexneigh, 
											TWOTHIRDS * kappa_parallel *
											(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
											/ (delta_out*edgelen *(nu * nu + omega.dot(omega))),
											(T_out - shared_T[threadIdx.x])*
											TWOTHIRDS * kappa_parallel *
											(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
											/ (delta_out*edgelen *(nu * nu + omega.dot(omega))),
											T_out, shared_T[threadIdx.x]
											);
								}
							} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

							  // Now go round:	
							endpt_clock = endpt_anti;
							pos_clock = pos_out;
							pos_out = pos_anti;
							T_clock = T_out;
							T_out = T_anti;
						}; // next iNeigh

					}; // was it DOMAIN_VERTEX? Do what otherwise?
				}; // was it OUTERMOST/INNERMOST?
			
			}; // debug

		}; // mask

		__syncthreads();
	};
	
	if ((TESTHEAT1)) printf("%d ourrates.NeTe %1.10E \n", iVertex, ourrates.NeTe);

	memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));

	// It was not necessarily sensible to combine ion and electron
	// However, it is quite daft having a separate routine for vector2 grad T (??)

}

__global__ void kernelAccumulateDiffusiveHeatRate_new_Longitudinalonly_1species(
	structural * __restrict__ p_info_minor,
	long * __restrict__ pIndexNeigh,
	char * __restrict__ pPBCNeigh,
	long * __restrict__ izTri_verts,
	char * __restrict__ szPBCtri_verts,
	f64_vec2 * __restrict__ p_cc,

	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p__T,
	f64_vec3 * __restrict__ p_B_major,

	f64 * __restrict__ p__kappa,
	f64 * __restrict__ p__nu,

	NTrates * __restrict__ NTadditionrates,
	f64 * __restrict__ p_AreaMajor,
	bool * __restrict__ p_maskbool,
	bool * __restrict__ p_maskblock,
	bool bUseMask,
	int iSpecies)
{
	// Think we might as well take kappa_par and nu from triangles really.
	// If n is modelled well then hopefully a nearby high-n does not have a big impact.

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajorClever]; // 2

																	 // DO NOT WANT:
	__shared__ f64_vec2 shared_pos[2 * threadsPerTileMajorClever]; // but as far as we know, we are having to use circumcenters.
																   // Maybe it works without them now that we have the longitudinal assumptions --- don't know for sure.
																   // But it means we are not being consistent with our definition of a cell?
																   // Like having major cells Voronoi => velocity living on centroids (which it must, for visc + A) is in slightly the wrong place.

	__shared__ f64 shared_T[threadsPerTileMajorClever];

	__shared__ f64_vec2 shared_B[threadsPerTileMajorClever]; // +2

															 // B is smooth. Unfortunately we have not fitted in Bz here.
															 // In order to do that perhaps rewrite so that variables are overwritten in shared.
															 // We do not need all T and nu in shared at the same time.
															 // This way is easier for NOW.
	__shared__ f64 shared_kappa[threadsPerTileMajorClever * 2];
	__shared__ f64 shared_nu[threadsPerTileMajorClever * 2];

	__shared__ long Indexneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // assume 48 bytes = 4*12 = 6 doubles

	__shared__ char PBCneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // 12 bytes each from L1. Have 42 per thread at 384 threads.
																	// We should at any rate try a major block of size 256. If 1 block will run at a time, so be it.
																	// Leave L1 in case of register overflow into it. <-- don't know how likely - do we only have 31 doubles in registry
																	// regardless # of threads and space? Or can be 63?
	__shared__ char PBCtri[MAXNEIGH_d*threadsPerTileMajorClever];
	// Balance of shared vs L1: 24*256*8 = 48K. That leaves 8 doublesworth in L1 for variables.

	long izTri[MAXNEIGH_d];  // so only 2 doubles left in L1. 31 in registers??

							 // Set threadsPerTileMajorClever to 256.

							 // It would help matters if we get rid of T3. We might as well therefore change to scalar flatpack T.

							 // We are hoping that it works well loading kappa(tri) and that this is not upset by nearby values. Obviously a bit of an experiment.

							 // Does make it somewhat laughable that we go to such efforts to reduce global accesses when we end up overflowing anyway. 
							 // If we can fit 24 doubles/thread in 48K that means we can fit 8 doubles/thread in 16K so that's most of L1 used up.

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	long const StartMinor = blockIdx.x*blockDim.x * 2;
	long const EndMinor = StartMinor + blockDim.x * 2;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	bool bMask;

	if (bUseMask)
		if (p_maskblock[blockIdx.x] == 0) return;

	if (bUseMask) bMask = p_maskbool[iVertex];

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	shared_pos_verts[threadIdx.x] = info.pos;
#ifdef CENTROID_HEATCONDUCTION
	{
		structural infotemp[2];
		memcpy(infotemp, p_info_minor + 2 * iVertex, 2 * sizeof(structural));
		shared_pos[threadIdx.x * 2] = infotemp[0].pos;
		shared_pos[threadIdx.x * 2 + 1] = infotemp[1].pos;
		// No nu to set for neutrals - not used
	}
#else
	{
		memcpy(&(shared_pos[threadIdx.x * 2]), p_cc + 2 * iVertex, 2 * sizeof(f64_vec2));
	}
#endif

	memcpy(&(shared_nu[threadIdx.x * 2]), p__nu + 2 * iVertex, 2 * sizeof(f64));
	memcpy(&(shared_kappa[threadIdx.x * 2]), p__kappa + 2 * iVertex, 2 * sizeof(f64));

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_B[threadIdx.x] = p_B_major[iVertex].xypart();
		shared_T[threadIdx.x] = p__T[iVertex];
	}
	else {
		// SHOULD NOT BE LOOKING INTO INS.
		// How do we avoid?
		memset(&(shared_B[threadIdx.x]), 0, sizeof(f64_vec2));
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();

	f64_vec2 grad_T;
	f64 T_anti, T_clock, T_out, T_outk;		// 5
	f64_vec2 pos_clock, pos_anti, pos_out;   // +6
	f64_vec2 B_out;       // +2
	NTrates ourrates;      // +5
	f64 kappa_parallel; // do we use them all at once or can we save 2 doubles here?
	f64 nu;                // 20 there  
	f64_vec2 edge_normal;  // 22
	f64_vec2 endpt_anti;    // 24 .. + 6 from above
	long indexneigh;     // into the 2-double buffer in L1
	f64_vec2 endpt_clock;    // As we only use endpt_anti afterwords we could union endpt_clock with edge_normal
							 // Come back and optimize by checking which things we need in scope at the same time?

	short iNeigh; // only fixed # of addresses so short makes no difference.
	char PBC; // char makes no difference.

	if ((info.flag == DOMAIN_VERTEX) && ((bUseMask == 0) || (bMask == true)))
	{
		// Need this, we are adding on to existing d/dt N,NT :
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));

		memcpy(Indexneigh + MAXNEIGH_d * threadIdx.x,
			pIndexNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(long));
		memcpy(PBCneigh + MAXNEIGH_d * threadIdx.x,
			pPBCNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));
		memcpy(PBCtri + MAXNEIGH_d * threadIdx.x,
			szPBCtri_verts + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));
		memcpy(izTri, //+ MAXNEIGH_d * threadIdx.x,
			izTri_verts + MAXNEIGH_d * iVertex, MAXNEIGH_d * sizeof(long));

		// The idea of not sending blocks full of non-domain vertices is another idea. Fiddly with indices.

		// Now do Tn:

		indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
		if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
		{
			pos_clock = shared_pos_verts[indexneigh - StartMajor];
			T_clock = shared_T[indexneigh - StartMajor];
		}
		else {
			structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
			pos_clock = info2.pos;
			T_clock = p__T[indexneigh];
		};
		PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
		if (PBC == NEEDS_ANTI) {
			pos_clock = Anticlock_rotate2(pos_clock);
		};
		if (PBC == NEEDS_CLOCK) {
			pos_clock = Clockwise_rotate2(pos_clock);
		};

		indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
		if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
		{
			pos_out = shared_pos_verts[indexneigh - StartMajor];
			T_out = shared_T[indexneigh - StartMajor];
		}
		else {
			structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
			pos_out = info2.pos;
			T_out = p__T[indexneigh]; // saved nothing here, only in loading
		};

		PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
		if (PBC == NEEDS_ANTI) {
			pos_out = Anticlock_rotate2(pos_out);
		};
		if (PBC == NEEDS_CLOCK) {
			pos_out = Clockwise_rotate2(pos_out);
		};

		if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
		{
			endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
		}
		else {
#ifdef CENTROID_HEATCONDUCTION
			endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
			endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
		}
		PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
		if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
		if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;

		//		if (T_clock == 0.0) {
		//			T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
		//		};
		//		Mimic


#pragma unroll MAXNEIGH_d
		for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
		{
			{
				short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
			}
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_anti = shared_pos_verts[indexneigh - StartMajor];
				T_anti = shared_T[indexneigh - StartMajor];
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_anti = info2.pos;
				T_anti = p__T[indexneigh];
			};
			if (PBC == NEEDS_ANTI) {
				pos_anti = Anticlock_rotate2(pos_anti);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_anti = Clockwise_rotate2(pos_anti);
			};

			//if (T_anti == 0.0) {
			//	T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
			//}; // So we are receiving 0 then doing this. But how come?
			//Mimic
			// Now let's see
			// tri 0 has neighs 0 and 1 I'm pretty sure (check....) CHECK

			if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
			{
				endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
			}
			else {
#ifdef CENTROID_HEATCONDUCTION
				endpt_anti = p_info_minor[izTri[iNeigh]].pos;
				// we should switch back to centroids!!
#else
				endpt_anti = p_cc[izTri[iNeigh]];
#endif
			}
			PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
			if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
			if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

			edge_normal.x = (endpt_anti.y - endpt_clock.y);
			edge_normal.y = (endpt_clock.x - endpt_anti.x);

			// SMARTY:
			if (TestDomainPos(pos_out))
			{
				kappa_parallel = 0.0;
				f64 nu;
				if (iSpecies == 0) {
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p__kappa[izTri[iNeigh]];
					}
					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p__kappa[izTri[iPrev]];
						}
					}

					if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))
					{
						f64 edgelen = edge_normal.modulus();

						ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
							(T_out - shared_T[threadIdx.x]) / (pos_out - info.pos).modulus();
					}
					else {
						f64 Area_quadrilateral = 0.5*(
							(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
							+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
							+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
							+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
							);

						// When we come to do the other iSpecies, make a subroutine.

						grad_T.x = 0.5*( // notice minus
							(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
							+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
							+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
							+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
							) / Area_quadrilateral;

						grad_T.y = -0.5*( // notice minus
							(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
							+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
							+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
							+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
							) / Area_quadrilateral;

						ourrates.NnTn += TWOTHIRDS * kappa_parallel * grad_T.dot(edge_normal);
					};
					// This is correct, grad T in same coordinates as edge_normal...

				}
				else {
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
						nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p__kappa[izTri[iNeigh]];
						nu = 0.5*p__nu[izTri[iNeigh]];
					};
					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
							nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p__kappa[izTri[iPrev]];
							nu += 0.5*p__nu[izTri[iPrev]];
						};
					};

					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						B_out = shared_B[indexneigh - StartMajor];
					}
					else {
						f64_vec3 B_out3 = p_B_major[indexneigh];
						B_out = B_out3.xypart();
					}
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
					if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

					{ // scoping brace
						f64_vec3 omega;
						if (iSpecies == 1) {
							omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);
						}
						else {
							omega = Make3(qovermc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qovermc);
						};

						f64 edgelen = edge_normal.modulus();
						f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

						if (iSpecies == 1) {
							ourrates.NiTi += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
								(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
								/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));
						}
						else {
							ourrates.NeTe += TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
								(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
								/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));

							if (TESTHEAT)
								printf("%d %d iSpecies %d contrib %1.10E kappa_par %1.9E\nT_out %1.9E T %1.9E nu %1.9E omega %1.9E %1.9E\n", iVertex, iNeigh, iSpecies,
									TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
									(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
									/ (delta_out*edgelen *(nu * nu + omega.dot(omega))),
									kappa_parallel, T_out, shared_T[threadIdx.x], nu, omega.x, omega.y
								);
						};
					}
				}; // if iSpecies == 0

			} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

			  // Now go round:	
			endpt_clock = endpt_anti;
			pos_clock = pos_out;
			pos_out = pos_anti;
			T_clock = T_out;
			T_out = T_anti;
		}; // next iNeigh

	}; // DOMAIN vertex active in mask

	   // Turned out to be stupid having a struct called NTrates. We just want to modify one scalar at a time.

	memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));
}

#include "heatflux.cu"

;

__global__ void kernelCalc_SelfCoefficient_for_HeatConduction 
(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	long * __restrict__ pIndexNeigh,
	char * __restrict__ pPBCNeigh,
	long * __restrict__ izTri_verts,
	char * __restrict__ szPBCtri_verts,
	f64_vec2 * __restrict__ p_cc,
	nvals * __restrict__ p_n_major,
	f64_vec3 * __restrict__ p_B_major,

	f64 * __restrict__ p_kappa_n,
	f64 * __restrict__ p_kappa_i,
	f64 * __restrict__ p_kappa_e,

	f64 * __restrict__ p_nu_i,
	f64 * __restrict__ p_nu_e,
	f64 * __restrict__ p_AreaMajor,

	f64 * __restrict__ p_coeffself_n,
	f64 * __restrict__ p_coeffself_i,
	f64 * __restrict__ p_coeffself_e // outputs
	)
{
	// Think we might as well take kappa_par and nu from triangles really.
	// If n is modelled well then hopefully a nearby high-n does not have a big impact.

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajorClever]; // 2

																	 // DO NOT WANT:
	__shared__ f64_vec2 shared_pos[2 * threadsPerTileMajorClever];

	__shared__ f64_vec2 shared_B[threadsPerTileMajorClever]; // +2

															 // B is smooth. Unfortunately we have not fitted in Bz here.
															 // In order to do that perhaps rewrite so that variables are overwritten in shared.
															 // We do not need all T and nu in shared at the same time.
															 // This way is easier for NOW.
	__shared__ f64 shared_kappa[threadsPerTileMajorClever * 2];
	__shared__ f64 shared_nu[threadsPerTileMajorClever * 2];

	__shared__ long Indexneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // assume 48 bytes = 4*12 = 6 doubles

	__shared__ char PBCneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // 12 bytes each from L1. Have 42 per thread at 384 threads.
																	// We should at any rate try a major block of size 256. If 1 block will run at a time, so be it.
																	// Leave L1 in case of register overflow into it. <-- don't know how likely - do we only have 31 doubles in registry
																	// regardless # of threads and space? Or can be 63?
	__shared__ char PBCtri[MAXNEIGH_d*threadsPerTileMajorClever];
	// Balance of shared vs L1: 24*256*8 = 48K. That leaves 8 doublesworth in L1 for variables.

	long izTri[MAXNEIGH_d];  // so only 2 doubles left in L1. 31 in registers??

							 // Set threadsPerTileMajorClever to 256.

							 // It would help matters if we get rid of T3. We might as well therefore change to scalar flatpack T.

							 // We are hoping that it works well loading kappa(tri) and that this is not upset by nearby values. Obviously a bit of an experiment.

							 // Does make it somewhat laughable that we go to such efforts to reduce global accesses when we end up overflowing anyway. 
							 // If we can fit 24 doubles/thread in 48K that means we can fit 8 doubles/thread in 16K so that's most of L1 used up.

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	long const StartMinor = blockIdx.x*blockDim.x * 2;
	long const EndMinor = StartMinor + blockDim.x * 2;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	shared_pos_verts[threadIdx.x] = info.pos;
#ifdef CENTROID_HEATCONDUCTION
	{
		structural infotemp[2];
		memcpy(infotemp, p_info_minor + 2 * iVertex, 2 * sizeof(structural));
		shared_pos[threadIdx.x * 2] = infotemp[0].pos;
		shared_pos[threadIdx.x * 2 + 1] = infotemp[1].pos;
		f64 tempf64[2];
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
		// No nu to set for neutrals - not used
	}
#else
	{
		memcpy(&(shared_pos[threadIdx.x * 2]), p_cc + 2 * iVertex, 2 * sizeof(f64_vec2));
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
	}
#endif


	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_B[threadIdx.x] = p_B_major[iVertex].xypart();

	}
	else {
		// SHOULD NOT BE LOOKING INTO INS.
		// How do we avoid?
		memset(&(shared_B[threadIdx.x]), 0, sizeof(f64_vec2));
	}

	__syncthreads();

	f64_vec2 grad_T;
	f64_vec2 pos_clock, pos_anti, pos_out;   // +6
	f64_vec2 B_out;       // +2
	NTrates ourrates;      // +5
	f64 kappa_parallel; // do we use them all at once or can we save 2 doubles here?
	f64 nu;                // 20 there  
	f64_vec2 edge_normal;  // 22
	f64_vec2 endpt_anti;    // 24 .. + 6 from above
	long indexneigh;     // into the 2-double buffer in L1
	f64_vec2 endpt_clock;    // As we only use endpt_anti afterwords we could union endpt_clock with edge_normal
							 // Come back and optimize by checking which things we need in scope at the same time?

	short iNeigh; // only fixed # of addresses so short makes no difference.
	char PBC; // char makes no difference.
	
	memset(&ourrates, 0, sizeof(NTrates));

	if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
	{
		// [ Ignore flux into edge of outermost vertex I guess ???]
	}
	else {
		if (info.flag == DOMAIN_VERTEX) {
			// The idea of not sending blocks full of non-domain vertices is another idea. Fiddly with indices.

			// Need this, we are adding on to existing d/dt N,NT :
		
			memcpy(Indexneigh + MAXNEIGH_d * threadIdx.x,
				pIndexNeigh + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(long));
			memcpy(PBCneigh + MAXNEIGH_d * threadIdx.x,
				pPBCNeigh + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(char));
			memcpy(PBCtri + MAXNEIGH_d * threadIdx.x,
				szPBCtri_verts + MAXNEIGH_d * iVertex,
				MAXNEIGH_d * sizeof(char));

			// EXPERIMENT WHETHER IT IS FASTER WITH THESE OUTSIDE OR INSIDE THE BRANCH.
			
			memcpy(izTri, //+ MAXNEIGH_d * threadIdx.x,
				izTri_verts + MAXNEIGH_d * iVertex, MAXNEIGH_d * sizeof(long));
			
			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_clock = shared_pos_verts[indexneigh - StartMajor];
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_clock = info2.pos;
			};
			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == NEEDS_ANTI) {
				pos_clock = Anticlock_rotate2(pos_clock);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_clock = Clockwise_rotate2(pos_clock);
			};

			indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				pos_out = shared_pos_verts[indexneigh - StartMajor];
			}
			else {
				structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
				pos_out = info2.pos;
			};

			PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
			if (PBC == NEEDS_ANTI) {
				pos_out = Anticlock_rotate2(pos_out);
			};
			if (PBC == NEEDS_CLOCK) {
				pos_out = Clockwise_rotate2(pos_out);
			};

			if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
			{
				endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
			}
			else {
#ifdef CENTROID_HEATCONDUCTION
				endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
				endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
			}
			PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
			if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
			if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;


#pragma unroll MAXNEIGH_d
			for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
			{
				{
					short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
					PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
				}
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_anti = shared_pos_verts[indexneigh - StartMajor];
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_anti = info2.pos;
				};
				if (PBC == NEEDS_ANTI) {
					pos_anti = Anticlock_rotate2(pos_anti);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_anti = Clockwise_rotate2(pos_anti);
				};

				if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
				{
					endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_anti = p_info_minor[izTri[iNeigh]].pos;
					// we should switch back to centroids!!
#else
					endpt_anti = p_cc[izTri[iNeigh]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

				edge_normal.x = (endpt_anti.y - endpt_clock.y);
				edge_normal.y = (endpt_clock.x - endpt_anti.x);
				
				// SMARTY:
				if (TestDomainPos(pos_out))
				{


					kappa_parallel = 0.0;
					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
					}
					else {
						kappa_parallel = 0.5*p_kappa_n[izTri[iNeigh]];
					}
					{
						short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
						if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
						{
							kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
						}
						else {
							kappa_parallel += 0.5*p_kappa_n[izTri[iPrev]];
						}
					}

					if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))
					{
						f64 edgelen = edge_normal.modulus();

						ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
							(-1.0) / (pos_out - info.pos).modulus();
					}
					else {
						f64 Area_quadrilateral = 0.5*(
							(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
							+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
							+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
							+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
							);

						// When we come to do the other species, make a subroutine.

						//grad_T.x = 0.5*( // notice minus
						//	(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
						//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
						//	+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
						//	+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
						//	) / Area_quadrilateral;

						//grad_T.y = -0.5*( // notice minus
						//	(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
						//	+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
						//	+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
						//	+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
						//	) / Area_quadrilateral;

						grad_T.x = 0.5*(pos_clock.y - pos_anti.y) / Area_quadrilateral;
						grad_T.y = -0.5*(pos_clock.x - pos_anti.x) / Area_quadrilateral;
						
						ourrates.NnTn += TWOTHIRDS * kappa_parallel * grad_T.dot(edge_normal);
					}
					// This is correct, grad T in same coordinates as edge_normal...

				} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

				  // Now go round:	
				endpt_clock = endpt_anti;
				pos_clock = pos_out;
				pos_out = pos_anti;
			}; // next iNeigh

		}; // was it DOMAIN_VERTEX? Do what otherwise?
	}; // was it OUTERMOST/INNERMOST?

	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################


#pragma unroll
	for (int iSpecies = 1; iSpecies < 3; iSpecies++)
	{
		if (iSpecies == 1)
		{
			memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_i + 2 * iVertex, 2 * sizeof(f64));
			memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_i + 2 * iVertex, 2 * sizeof(f64));
		}
		else {
			memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_e + 2 * iVertex, 2 * sizeof(f64));
			memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_e + 2 * iVertex, 2 * sizeof(f64));
		};
		// Maybe this alone means combining the ion & electron code was stupid. Maybe it can't make contig access.

		__syncthreads();

		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
		{
			// [ Ignore flux into edge of outermost vertex I guess ???]
		}
		else {
			if (info.flag == DOMAIN_VERTEX) {

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_clock = shared_pos_verts[indexneigh - StartMajor];
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_clock = info2.pos;
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == NEEDS_ANTI) {
					pos_clock = Anticlock_rotate2(pos_clock);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_clock = Clockwise_rotate2(pos_clock);
				};

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_out = shared_pos_verts[indexneigh - StartMajor];
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_out = info2.pos;
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
				if (PBC == NEEDS_ANTI) {
					pos_out = Anticlock_rotate2(pos_out);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_out = Clockwise_rotate2(pos_out);
				};

				if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
				{
					endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
					endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;

#pragma unroll MAXNEIGH_d
				for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
				{
					{
						short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
					}
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						pos_anti = shared_pos_verts[indexneigh - StartMajor];
					}
					else {
						structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
						pos_anti = info2.pos;
					};
					if (PBC == NEEDS_ANTI) {
						pos_anti = Anticlock_rotate2(pos_anti);
					};
					if (PBC == NEEDS_CLOCK) {
						pos_anti = Clockwise_rotate2(pos_anti);
					};

					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
					}
					else {
#ifdef CENTROID_HEATCONDUCTION
						endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
						endpt_anti = p_cc[izTri[iNeigh]];
#endif					
					}
					PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
					if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

					edge_normal.x = (endpt_anti.y - endpt_clock.y);
					edge_normal.y = (endpt_clock.x - endpt_anti.x);

					// SMARTY:
					if (TestDomainPos(pos_out))
					{
						
						kappa_parallel = 0.0;
						f64 nu;
						if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
						{
							kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
							nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
						}
						else {
							if (iSpecies == 1) {
								kappa_parallel = 0.5*p_kappa_i[izTri[iNeigh]];
								nu = 0.5*p_nu_i[izTri[iNeigh]];
							}
							else {
								kappa_parallel = 0.5*p_kappa_e[izTri[iNeigh]];
								nu = 0.5*p_nu_e[izTri[iNeigh]];
							};
						};
						{
							short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
							if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
							{
								kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
								nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
							}
							else {
								if (iSpecies == 1) {
									kappa_parallel += 0.5*p_kappa_i[izTri[iPrev]];
									nu += 0.5*p_nu_i[izTri[iPrev]];
								}
								else {
									kappa_parallel += 0.5*p_kappa_e[izTri[iPrev]];
									nu += 0.5*p_nu_e[izTri[iPrev]];
								};
							}

						}

						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
						{
							B_out = shared_B[indexneigh - StartMajor];
						}
						else {
							f64_vec3 B_out3 = p_B_major[indexneigh];
							B_out = B_out3.xypart();
						}
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
						if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

						{ // scoping brace
							f64_vec3 omega;
							if (iSpecies == 1) {
								omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);
							}
							else {
								omega = Make3(qovermc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qovermc);
							};
							// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?
							//
							//						ourrates.NiTi += TWOTHIRDS * kappa_parallel *(
							//							edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y + nu * omega.z)*grad_T.y)
							//							+ edge_normal.y*((omega.x*omega.y - nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
							//							) / (nu * nu + omega.dot(omega));
							f64 edgelen = edge_normal.modulus();
							f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

							if (iSpecies == 1) {
								ourrates.NiTi += TWOTHIRDS * kappa_parallel *  (-1.0) *
									(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
									/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));
							} else {
								ourrates.NeTe += TWOTHIRDS * kappa_parallel *  (-1.0) *
									(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
									/ (delta_out*edgelen *(nu * nu + omega.dot(omega)));
							};

						}
					} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

					  // Now go round:	
					endpt_clock = endpt_anti;
					pos_clock = pos_out;
					pos_out = pos_anti;
				}; // next iNeigh

			}; // was it DOMAIN_VERTEX? Do what otherwise?
		}; // was it OUTERMOST/INNERMOST?

		__syncthreads();
	};


	// Now compute self coeff from d/dself dNT/dt
	nvals n_use = p_n_major[iVertex];
	f64 AreaMajor = p_AreaMajor[iVertex];
	f64 Nn = n_use.n_n*AreaMajor;
	f64 N = n_use.n *AreaMajor;
	p_coeffself_n[iVertex] = sqrt(Nn) - (h_use / sqrt(Nn))*ourrates.NnTn; // ourrates is ROC epsilon wrt self
	p_coeffself_i[iVertex] = sqrt(N) - (h_use / sqrt(N))*ourrates.NiTi; // ourrates is ROC epsilon wrt self
	p_coeffself_e[iVertex] = sqrt(N) - (h_use / sqrt(N))*ourrates.NeTe; // ourrates is ROC epsilon wrt self



	// NOTE BENE THAT WE UPDATED THIS IN VIEW OF:
	// epsilon *= sqrt(N);



	if ((iVertex == VERTCHOSEN) || (iVertex == VERTCHOSEN2))
		printf("%d  coeffself (1-h/N rates) %1.10E Rates %1.10E h/N %1.10E\n\n",
			iVertex, p_coeffself_n[iVertex],  ourrates.NnTn, h_use / N);

	//if (iVertex == VERTCHOSEN) printf("iVertex %d coeffself_i %1.10E \n", iVertex, p_coeffself_i[iVertex]);
	// ourrates is negative so this is > 1.
}

__global__ void kernelPowerminushalf
(f64 * __restrict__ p_input, f64 * __restrict__ p_output)
{
	long const index = threadIdx.x + blockIdx.x*blockDim.x;
	p_output[index] = 1.0 / sqrt(p_input[index]);
}

__global__ void kernelVolleyRegressors(
	f64 * __restrict__ p_regress,
	long const Length,
	char * __restrict__ p_iVolley
) {
	long const iVertex = threadIdx.x + blockDim.x*blockIdx.x;

	// p_regress is regr+NUMVERTICES. That is the position of epsilon.

	char cVolley = p_iVolley[iVertex];
	f64 regr1 = p_regress[iVertex];
	f64 regr2 = p_regress[iVertex + Length];
	
	f64 regr3 = regr1*((cVolley == 1)? 1 : 0);
	f64 regr4 = regr2*((cVolley == 1) ? 1 : 0);
	f64 regr5 = regr1*((cVolley == 2) ? 1 : 0);
	f64 regr6 = regr2*((cVolley == 2)? 1 : 0);
	f64 regr7 = regr1*((cVolley > 2)? 1 : 0);
	f64 regr8 = regr2*((cVolley > 2) ? 1 : 0);

	regr1 = regr1*((cVolley == 0) ? 1 : 0);
	regr2 = regr2*((cVolley == 0) ? 1 : 0);
	p_regress[iVertex] = regr1;
	p_regress[iVertex + Length] = regr2;
	p_regress[iVertex + 2 * Length] = regr3;
	p_regress[iVertex + 3 * Length] = regr4;
	p_regress[iVertex + 4 * Length] = regr5;
	p_regress[iVertex + 5 * Length] = regr6;

	// No thanks: leave 7 free
	//p_regress[iVertex + 6 * Length] = regr7;
	//p_regress[iVertex + 7 * Length] = regr8;

}

__global__ void kernelCreateEpsilonHeat
(
	f64 const hsub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_eps_n,
	f64 * __restrict__ p_eps_i,
	f64 * __restrict__ p_eps_e,
	f64 * __restrict__ p_NT_n,
	f64 * __restrict__ p_NT_i,
	f64 * __restrict__ p_NT_e,
	T3 * __restrict__ p_T_k,
	f64 * __restrict__ p_AreaMajor,
	nvals * __restrict__ p_n_major,
	NTrates * __restrict__ NTadditionrates, // it's especially silly having a whole struct of 5 instead of 3 here.
	bool * __restrict__ p_b_Failed,
	bool * __restrict__ p_bMask3,
	bool * __restrict__ p_bMaskblock,
	bool bUseMask
	)
{
	if ((bUseMask) && (p_bMaskblock[blockIdx.x] == 0)) return;
	
	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	bool bMask[3];
	if (bUseMask) {
		//memcpy(bMask, p_bMask3 + 3 * iVertex, sizeof(bool) * 3); // until we break out species!
		bMask[0] = p_bMask3[iVertex];
		bMask[1] = p_bMask3[iVertex + NUMVERTICES];
		bMask[2] = p_bMask3[iVertex + 2*NUMVERTICES];

		if ((bMask[0] == 0) && (bMask[1] == 0) && (bMask[2] == 0)) return;
	};

	structural info = p_info_major[iVertex];
	if (info.flag == DOMAIN_VERTEX) {

		T3 T_k = p_T_k[iVertex];
		NTrates ourrates;
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));
		f64 AreaMajor = p_AreaMajor[iVertex];
		nvals n = p_n_major[iVertex];
		f64 epsilon_n, epsilon_i, epsilon_e;
		bool bFail = false;

		if (bMask[0] || (bUseMask == 0)) {
			f64 sqrtNn = sqrt(AreaMajor*n.n_n);
			f64 NnTn = p_NT_n[iVertex]; // means sqrtN T
 			epsilon_n = NnTn - T_k.Tn*sqrtNn - (hsub / sqrtNn)*ourrates.NnTn;

			if (epsilon_n*epsilon_n > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(NnTn*NnTn + 1.0e-10*1.0e-10)) bFail = true;

		//	Note that ourrates already included the factor 1/sqrtN on our own sqrt(N)T
			

		} else {
			epsilon_n = 0.0;
		};

		if (bMask[1] || (bUseMask == 0)) {
			f64 sqrtN = sqrt(AreaMajor*n.n);
			f64 NTi = p_NT_i[iVertex];
			epsilon_i = NTi - T_k.Ti*sqrtN - (hsub / sqrtN)*ourrates.NiTi;
			if (epsilon_i*epsilon_i > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(NTi*NTi + 1.0e-10*1.0e-10)) bFail = true;

			if (iVertex == VERTCHOSEN) printf("%d NTi %1.10E sqrtN Tk %1.10E hsub / sqrtN %1.10E NiTi %1.10E eps_i %1.10E sqrtN %1.10E\n",
				iVertex, NTi, T_k.Ti*sqrtN, hsub / sqrtN, ourrates.NiTi, epsilon_i, sqrtN);

		} else {
			epsilon_i = 0.0;
		};

		if (bMask[2] || (bUseMask == 0)) {
			f64 sqrtN = sqrt(AreaMajor*n.n);
			f64 NTe = p_NT_e[iVertex]; // is this sqrtN T ?
			epsilon_e = NTe - T_k.Te*sqrtN - (hsub / sqrtN)*ourrates.NeTe;
			if (epsilon_e*epsilon_e > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(NTe*NTe + 1.0e-10*1.0e-10)) bFail = true;
		} else {
			epsilon_e = 0.0;
		};

		if (epsilon_n != epsilon_n)printf("epsilon_n NaN iVertex %d n_n %1.10E Area %1.10E \n",
			iVertex, n.n_n, AreaMajor);

		p_eps_n[iVertex] = epsilon_n;
		p_eps_i[iVertex] = epsilon_i;
		p_eps_e[iVertex] = epsilon_e;
		
		if (p_b_Failed != 0) {
			if (bFail)
				p_b_Failed[blockIdx.x] = true;
			// Why 1.0e-10 in absolute error, for minimum value we care about:
			// N = 2.0e12*7e-5 = 1e8 
			// root N = 1e4
			// root N * 1e-14 erg = 1e-10 for (root N) T
		};
	} else {
		p_eps_n[iVertex] = 0.0;
		p_eps_i[iVertex] = 0.0;
		p_eps_e[iVertex] = 0.0;
	};
}

__global__ void kernelSelectivelyZeroNTrates(
	NTrates * __restrict__ NTadditionrates,
	bool * __restrict__ p_bMask3
) {
	long const iVertex = threadIdx.x + blockIdx.x*blockDim.x;

	bool bMask[3];

	bMask[0] = p_bMask3[iVertex];
	bMask[1] = p_bMask3[iVertex + NUMVERTICES];
	bMask[2] = p_bMask3[iVertex + 2 * NUMVERTICES];
	if ((bMask[0] == 0) && (bMask[1] == 0) && (bMask[2] == 0)) return;

	NTrates dNTbydt;
	memcpy(&dNTbydt, &(NTadditionrates[iVertex]), sizeof(NTrates));

	if (bMask[0]) dNTbydt.NnTn = 0.0;
	if (bMask[1]) dNTbydt.NiTi = 0.0;
	if (bMask[2]) dNTbydt.NeTe = 0.0;

	memcpy(&(NTadditionrates[iVertex]), &dNTbydt, sizeof(NTrates));
}

__global__ void kernelCreateEpsilonHeat_Equilibrated
(
	f64 const hsub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_eps_n,
	f64 * __restrict__ p_eps_i,
	f64 * __restrict__ p_eps_e,
	f64 * __restrict__ p_sqrtDNT_n,
	f64 * __restrict__ p_sqrtDNT_i,
	f64 * __restrict__ p_sqrtDNT_e,
	T3 * __restrict__ p_T_k,
	f64 * __restrict__ p_AreaMajor,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_invsqrtD_n,
	f64 * __restrict__ p_invsqrtD_i,
	f64 * __restrict__ p_invsqrtD_e,
	NTrates * __restrict__ NTadditionrates, // it's especially silly having a whole struct of 5 instead of 3 here.
	bool * __restrict__ p_b_Failed,
	bool * __restrict__ p_bMask3,
	bool * __restrict__ p_bMaskblock,
	bool bUseMask
)
{
	if ((bUseMask) && (p_bMaskblock[blockIdx.x] == 0)) return;

	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	bool bMask[3];
	if (bUseMask) {
		bMask[0] = p_bMask3[iVertex];
		bMask[1] = p_bMask3[iVertex + NUMVERTICES];
		bMask[2] = p_bMask3[iVertex + 2*NUMVERTICES];
		if ((bMask[0] == 0) && (bMask[1] == 0) && (bMask[2] == 0)) return;
	};

	structural info = p_info_major[iVertex];
	if (info.flag == DOMAIN_VERTEX) {

		T3 T_k = p_T_k[iVertex];
		NTrates ourrates;
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));
		f64 AreaMajor = p_AreaMajor[iVertex];
		nvals n = p_n_major[iVertex];
		f64 epsilon_n, epsilon_i, epsilon_e;
		bool bFail = false;

		if ((bMask[0]) || (bUseMask == 0)) {
			f64 sqrtNn = sqrt(AreaMajor*n.n_n);
			f64 sqrtDN_T = p_sqrtDNT_n[iVertex]; // means sqrtDN T
			f64 sqrtDinv = p_invsqrtD_n[iVertex];

			// epsilon_i = NTi - T_k.Ti*sqrtN - (hsub / sqrtN)*ourrates.NiTi;
			// Multiply epsilon by D^-1/2 and
			// wherever a sqrt(DN)T appears multiply it by D_j^-1/2 to give sqrt(N)T
			// The multiplication D_j^-1/2 was already included in T -> ourrates

			epsilon_n = sqrtDinv*sqrtDinv*sqrtDN_T
				- sqrtDinv*(hsub / sqrtNn)*ourrates.NnTn
				- sqrtDinv*T_k.Tn*sqrtNn;

			f64 test_epsilon = epsilon_n / sqrtDinv; // divides take long.
			f64 sqrtNn_Tn = sqrtDinv*sqrtDN_T;
			if (test_epsilon*test_epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(sqrtNn_Tn*sqrtNn_Tn + 1.0e-10*1.0e-10)) bFail = true;
			
			// Let's be careful about that threshold. It's for N T^2. 

			// sqrt(N) that we care about ~ 1e4. T that we care about ~ 1e-14. We then go REL_THRESH*that.

			//	Note that ourrates already included the factor 1/sqrtN on our own sqrt(N)T
			
		} else {
			epsilon_n = 0.0;
		};

		if ((bMask[1]) || (bUseMask == 0)) {
			f64 sqrtN = sqrt(AreaMajor*n.n);
			f64 sqrtDN_T = p_sqrtDNT_i[iVertex]; // means sqrtDN T
			f64 sqrtDinv = p_invsqrtD_i[iVertex];

			epsilon_i = sqrtDinv*sqrtDinv*sqrtDN_T
				- sqrtDinv*(hsub / sqrtN)*ourrates.NiTi
				- sqrtDinv*T_k.Ti*sqrtN;

			//epsilon_i = NTi - T_k.Ti*sqrtN - (hsub / sqrtN)*ourrates.NiTi;
			f64 test_epsilon = epsilon_i / sqrtDinv;
			f64 sqrtN_Ti = sqrtDinv*sqrtDN_T;
			if (test_epsilon*test_epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(sqrtN_Ti*sqrtN_Ti + 1.0e-10*1.0e-10)) bFail = true;
		}
		else {
			epsilon_i = 0.0;
		};

		if ((bMask[2]) || (bUseMask == 0)) {
			f64 sqrtN = sqrt(AreaMajor*n.n);
			f64 sqrtDN_T = p_sqrtDNT_e[iVertex]; // means sqrtDN T
			f64 sqrtDinv = p_invsqrtD_e[iVertex];

			epsilon_e = sqrtDinv*sqrtDinv*sqrtDN_T
				- sqrtDinv*(hsub / sqrtN)*ourrates.NeTe
				- sqrtDinv*T_k.Te*sqrtN;

		//	epsilon_e = NTe - T_k.Te*sqrtN - (hsub / sqrtN)*ourrates.NeTe;
			f64 test_epsilon = epsilon_e / sqrtDinv;
			f64 sqrtN_Te = sqrtDinv*sqrtDN_T;
			if (test_epsilon*test_epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(sqrtN_Te*sqrtN_Te + 1.0e-10*1.0e-10)) bFail = true;
		}
		else {
			epsilon_e = 0.0;
		};

		//	if (TEST) printf("%d epsilon_e %1.8E NTe %1.8E nete %1.8E\n",
		//		iVertex, epsilon_e, NTe, ourrates.NeTe);
		if (epsilon_n != epsilon_n)printf("epsilon_n NaN iVertex %d n_n %1.10E Area %1.10E \n",
			iVertex, n.n_n, AreaMajor);

		p_eps_n[iVertex] = epsilon_n;
		p_eps_i[iVertex] = epsilon_i;
		p_eps_e[iVertex] = epsilon_e;

		if (p_b_Failed != 0) {
			if (bFail)
				p_b_Failed[blockIdx.x] = true;
			// Why 1.0e-10 in absolute error, for minimum value we care about:
			// N = 2.0e12*7e-5 = 1e8 
			// root N = 1e4
			// root N * 1e-14 erg = 1e-10 for (root N) T
		};
	}
	else {
		p_eps_n[iVertex] = 0.0;
		p_eps_i[iVertex] = 0.0;
		p_eps_e[iVertex] = 0.0;
	};
}

__global__ void kernelCreateEpsilonHeatOriginalScaling
(
	f64 const hsub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_eps_n,
	f64 * __restrict__ p_eps_i,
	f64 * __restrict__ p_eps_e,
	f64 * __restrict__ p_T_n,
	f64 * __restrict__ p_T_i,
	f64 * __restrict__ p_T_e,
	T3 * __restrict__ p_T_k,
	f64 * __restrict__ p_AreaMajor,
	nvals * __restrict__ p_n_major,
	NTrates * __restrict__ NTadditionrates ,// it's especially silly having a whole struct of 5 instead of 3 here.
	bool * __restrict__ bTest
)
{
	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	structural info = p_info_major[iVertex];
	if (info.flag == DOMAIN_VERTEX) {
		f64 Tn = p_T_n[iVertex];
		f64 Ti = p_T_i[iVertex];
		f64 Te = p_T_e[iVertex];
		T3 T_k = p_T_k[iVertex];
		f64 AreaMajor = p_AreaMajor[iVertex];
		nvals n = p_n_major[iVertex];
		NTrates ourrates;
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));
		f64 Nn = (AreaMajor*n.n_n);
		f64 epsilon_n = Tn - T_k.Tn - (hsub / Nn)*ourrates.NnTn;
		f64 N = (AreaMajor*n.n);
		f64 epsilon_i = Ti - T_k.Ti - (hsub / N)*ourrates.NiTi;
		f64 epsilon_e = Te - T_k.Te - (hsub / N)*ourrates.NeTe;
		p_eps_n[iVertex] = epsilon_n;
		p_eps_i[iVertex] = epsilon_i;
		p_eps_e[iVertex] = epsilon_e;

		if ((epsilon_n*epsilon_n > 1.0e-24*(Tn*Tn + 1.0e-14*1.0e-14))
			|| (epsilon_i*epsilon_i > 1.0e-24*(Ti*Ti + 1.0e-14*1.0e-14))
			|| (epsilon_e*epsilon_e > 1.0e-24*(Te*Te + 1.0e-14*1.0e-14))
			)
			bTest[blockIdx.x] = true;
	}
	else {
		p_eps_n[iVertex] = 0.0;
		p_eps_i[iVertex] = 0.0;
		p_eps_e[iVertex] = 0.0;
	}
}

__global__ void kernelAccumulateDiffusiveHeatRate_new_Full(
	structural * __restrict__ p_info_minor,
	long * __restrict__ pIndexNeigh,
	char * __restrict__ pPBCNeigh,
	long * __restrict__ izTri_verts,
	char * __restrict__ szPBCtri_verts,
	f64_vec2 * __restrict__ p_cc,

	nvals * __restrict__ p_n_major,
	T3 * __restrict__ p_T_major,
	//T3 * __restrict__ p_T_putative,
	bool * __restrict__ p_bool_longi,
	f64_vec3 * __restrict__ p_B_major,

	f64 * __restrict__ p_kappa_n,
	f64 * __restrict__ p_kappa_i,
	f64 * __restrict__ p_kappa_e,

	f64 * __restrict__ p_nu_i,
	f64 * __restrict__ p_nu_e,

	NTrates * __restrict__ NTadditionrates,
	f64 * __restrict__ p_AreaMajor,

	bool bCheckWhetherToDoctorUp,
	bool * __restrict__ p_maskbool3,
	bool * __restrict__ p_maskblock,
	bool bUseMask
	//T3 * __restrict__ p_T_putative
	) // test whether we are pushing heat uphill...
{
	// Think we might as well take kappa_par and nu from triangles really.
	// If n is modelled well then hopefully a nearby high-n does not have a big impact.

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajorClever]; // 2
	__shared__ f64_vec2 shared_pos[2 * threadsPerTileMajorClever];

	__shared__ f64 shared_T[threadsPerTileMajorClever];      // +3
														 //__shared__ f64 shared_T[threadsPerTileMajorClever];
	__shared__ f64_vec2 shared_B[threadsPerTileMajorClever]; // +2
												 // B is smooth. Unfortunately we have not fitted in Bz here.
															 // In order to do that perhaps rewrite so that variables are overwritten in shared.
															 // We do not need all T and nu in shared at the same time.
															 // This way is easier for NOW.
	__shared__ f64 shared_kappa[threadsPerTileMajorClever * 2];
	__shared__ f64 shared_nu[threadsPerTileMajorClever * 2];

	__shared__ long Indexneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // assume 48 bytes = 4*12 = 6 doubles

	__shared__ char PBCneigh[MAXNEIGH_d*threadsPerTileMajorClever]; // 12 bytes each from L1. Have 42 per thread at 384 threads.
																	// We should at any rate try a major block of size 256. If 1 block will run at a time, so be it.
																	// Leave L1 in case of register overflow into it. <-- don't know how likely - do we only have 31 doubles in registry
																	// regardless # of threads and space? Or can be 63?
	__shared__ char PBCtri[MAXNEIGH_d*threadsPerTileMajorClever];
	// Balance of shared vs L1: 24*256*8 = 48K. That leaves 8 doublesworth in L1 for variables.

	long izTri[MAXNEIGH_d];  // so only 2 doubles left in L1. 31 in registers??

							 // Set threadsPerTileMajorClever to 256.

							 // It would help matters if we get rid of T3. We might as well therefore change to scalar flatpack T.

							 // We are hoping that it works well loading kappa(tri) and that this is not upset by nearby values. Obviously a bit of an experiment.

							 // Does make it somewhat laughable that we go to such efforts to reduce global accesses when we end up overflowing anyway. 
							 // If we can fit 24 doubles/thread in 48K that means we can fit 8 doubles/thread in 16K so that's most of L1 used up.

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	long const StartMinor = blockIdx.x*blockDim.x * 2;
	long const EndMinor = StartMinor + blockDim.x * 2;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	bool bMask[3];

	if (bUseMask) 
		if (p_maskblock[blockIdx.x] == 0) return;

	if (bUseMask) {
		//memcpy(bMask, p_maskbool3 + iVertex * 3, 3 * sizeof(bool));
		bMask[0] = p_maskbool3[iVertex];
		bMask[1] = p_maskbool3[iVertex + NUMVERTICES];
		bMask[2] = p_maskbool3[iVertex + NUMVERTICES*2];

	}
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	shared_pos_verts[threadIdx.x] = info.pos;
#ifdef CENTROID_HEATCONDUCTION
	{
		structural infotemp[2];
		memcpy(infotemp, p_info_minor + 2 * iVertex, 2 * sizeof(structural));
		shared_pos[threadIdx.x * 2] = infotemp[0].pos;
		shared_pos[threadIdx.x * 2 + 1] = infotemp[1].pos;
		f64 tempf64[2];
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
		// No nu to set for neutrals - not used
	}
#else
	{
		memcpy(&(shared_pos[threadIdx.x * 2]), p_cc + 2 * iVertex, 2 * sizeof(f64_vec2));
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_n + 2 * iVertex, 2 * sizeof(f64));
	}
#endif


	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_B[threadIdx.x] = p_B_major[iVertex].xypart();

		if (TESTHEATFULL) printf("iVertex %d : B_major[iVertex] %1.10E %1.10E \n^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&^&\n",
			iVertex, p_B_major[iVertex].x, p_B_major[iVertex].y);

		shared_T[threadIdx.x] = p_T_major[iVertex].Tn;
	}
	else {
		// SHOULD NOT BE LOOKING INTO INS.
		// How do we avoid?
		memset(&(shared_B[threadIdx.x]), 0, sizeof(f64_vec2));
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();

	f64_vec2 grad_T;
	f64 T_anti, T_clock, T_out, T_outk;		// 5
	f64_vec2 pos_clock, pos_anti, pos_out;   // +6
	f64_vec2 B_out;       // +2
	NTrates ourrates;      // +5
	f64 kappa_parallel; // do we use them all at once or can we save 2 doubles here?
	f64 nu;                // 20 there  
	f64_vec2 edge_normal;  // 22
	f64_vec2 endpt_anti;    // 24 .. + 6 from above
	long indexneigh;     // into the 2-double buffer in L1
	f64_vec2 endpt_clock;    // As we only use endpt_anti afterwords we could union endpt_clock with edge_normal
							 // Come back and optimize by checking which things we need in scope at the same time?
	short iNeigh; // only fixed # of addresses so short makes no difference.
	char PBC; // char makes no difference.

	if ((bUseMask == 0) || (bMask[0] == true) || (bMask[1] == true) || (bMask[2] == true))
	{
		// Need this, we are adding on to existing d/dt N,NT :
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));

		memcpy(Indexneigh + MAXNEIGH_d * threadIdx.x,
			pIndexNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(long));
		memcpy(PBCneigh + MAXNEIGH_d * threadIdx.x,
			pPBCNeigh + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));
		memcpy(PBCtri + MAXNEIGH_d * threadIdx.x,
			szPBCtri_verts + MAXNEIGH_d * iVertex,
			MAXNEIGH_d * sizeof(char));

		memcpy(izTri, //+ MAXNEIGH_d * threadIdx.x,
			izTri_verts + MAXNEIGH_d * iVertex, MAXNEIGH_d * sizeof(long));
	}


	if ((bUseMask == 0) || (bMask[0] == true)) // either there is no masking, or this is switched on
	{
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
		{
		//	// [ Ignore flux into edge of outermost vertex I guess ???]
		//	long index0 = Indexneigh[MAXNEIGH_d * threadIdx.x + 0];
		//	long index1 = Indexneigh[MAXNEIGH_d * threadIdx.x + 1];
		//	long index2 = Indexneigh[MAXNEIGH_d * threadIdx.x + 2];
		//	long index3 = Indexneigh[MAXNEIGH_d * threadIdx.x + 3];
		//	long index4 = Indexneigh[MAXNEIGH_d * threadIdx.x + 4];
		//	printf("DEBUG: iVertex %d info.neigh_len %d izNeigh %d %d %d %d \n"
		//		"flags 0 %d  %d  %d  %d  \n"
		//		"positions (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) \n"
		//		, iVertex, info.neigh_len,
		//		index0, index1, index2, index3, 
		//		p_info_minor[index0+BEGINNING_OF_CENTRAL].flag,
		//		p_info_minor[index1 + BEGINNING_OF_CENTRAL].flag,
		//		p_info_minor[index2 + BEGINNING_OF_CENTRAL].flag,
		//		p_info_minor[index3 + BEGINNING_OF_CENTRAL].flag,
		//		p_info_minor[index0 + BEGINNING_OF_CENTRAL].pos.x, p_info_minor[index0 + BEGINNING_OF_CENTRAL].pos.y,
		//		p_info_minor[index1 + BEGINNING_OF_CENTRAL].pos.x, p_info_minor[index1 + BEGINNING_OF_CENTRAL].pos.y,
		//		p_info_minor[index2 + BEGINNING_OF_CENTRAL].pos.x, p_info_minor[index2 + BEGINNING_OF_CENTRAL].pos.y,
		//		p_info_minor[index3 + BEGINNING_OF_CENTRAL].pos.x, p_info_minor[index3 + BEGINNING_OF_CENTRAL].pos.y
		//	);
		} else {
			if (info.flag == DOMAIN_VERTEX) {
				// The idea of not sending blocks full of non-domain vertices is another idea. Fiddly with indices.
								
				// Now do Tn:
				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_clock = shared_T[indexneigh - StartMajor];
#endif
				} else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_clock = info2.pos;
#ifdef BWDSIDET
					T_clock = p_T_major[indexneigh].Tn;
#endif
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == NEEDS_ANTI) {
					pos_clock = Anticlock_rotate2(pos_clock);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_clock = Clockwise_rotate2(pos_clock);
				};
#ifndef BWDSIDET
				T_clock = p_T_k[indexneigh].Tn;
#endif

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_out = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_out = info2.pos;
#ifdef BWDSIDET
					T_out = p_T_major[indexneigh].Tn;
#endif
				};
#ifndef BWDSIDET
				T_outk = p_T_k[indexneigh].Tn; // ready for switch around
#endif

				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
				if (PBC == NEEDS_ANTI) {
					pos_out = Anticlock_rotate2(pos_out);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_out = Clockwise_rotate2(pos_out);
				};

				if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
				{
					endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
					endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;
//
//				if (T_clock == 0.0) {
//#ifdef BWDSIDET
//					T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//					T_clock = T_outk;
//#endif
//				};
//				Mimic

#pragma unroll MAXNEIGH_d
				for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
				{
					{
						short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
					}
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
						T_anti = shared_T[indexneigh - StartMajor];
#endif
					}
					else {
						structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
						pos_anti = info2.pos;
#ifdef BWDSIDET
						T_anti = p_T_major[indexneigh].Tn;
#endif
					};
					if (PBC == NEEDS_ANTI) {
						pos_anti = Anticlock_rotate2(pos_anti);
					};
					if (PBC == NEEDS_CLOCK) {
						pos_anti = Clockwise_rotate2(pos_anti);
					};
#ifndef BWDSIDET
					T_anti = p_T_k[indexneigh].Tn; // Stupid 3-struct

					// Also need to update T_opp if it was not done already

					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						T_out = shared_T[indexneigh - StartMajor];
					}
					else {
						T_out = p_T_major[indexneigh].Tn;
					};
#endif
//
//					if (T_anti == 0.0) {
//#ifdef BWDSIDET
//						T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//						T_anti = T_outk;
//#endif					
//					}; // So we are receiving 0 then doing this. But how come?
//					Mimic

					   // Now let's see
					   // tri 0 has neighs 0 and 1 I'm pretty sure (check....) CHECK

					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
					}
					else {
#ifdef CENTROID_HEATCONDUCTION
						endpt_anti = p_info_minor[izTri[iNeigh]].pos;
						// we should switch back to centroids!!
#else
						endpt_anti = p_cc[izTri[iNeigh]];
#endif
					}
					PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
					if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

					edge_normal.x = (endpt_anti.y - endpt_clock.y);
					edge_normal.y = (endpt_clock.x - endpt_anti.x);

					// SMARTY:
					if (TestDomainPos(pos_out))
					{


						// How to detect? Loading a load of flags is a killer! We do need to load ... and this is why we should have not made info struct. Def not.

						//// 
						//if (insulator triangle)
						//{
						//	centroid1 = THIRD*(pos_anti + pos_out + info.pos);
						//	// project to radius of insulator
						//	centroid1.project_to_radius(3.44);
						//	// Now dot with unit vectors:
						//	f64_vec2 tempvec2;
						//	tempvec2.x = unit_vec1.x*centroid1.x + unit_vec1.y*centroid1.y;
						//	tempvec2.y = unit_vec2.x*centroid1.x + unit_vec2.y*centroid1.y;
						//	centroid1.x = tempvec2.x;
						//	centroid1.y = tempvec2.y;
						//} else {
						//	// centroid1 = THIRD*(pos_anti_twist + pos_out_twist);
						//	centroid1.x = THIRD*(
						//		  unit_vec1.x*(pos_anti.x - info.pos.x) + unit_vec1.y*(pos_anti.y - info.pos.y)
						//		+ unit_vec1.x*(pos_out.x - info.pos.x) + unit_vec1.y*(pos_out.y - info.pos.y)
						//		);
						//	centroid1.y = THIRD*(
						//		- unit_vec1.y*(pos_anti.x - info.pos.x) + unit_vec1.x*(pos_anti.y - info.pos.y)
						//		- unit_vec1.y*(pos_out.x - info.pos.x) + unit_vec1.x*(pos_out.y - info.pos.y)
						//		);
						//}

						//if (insulator triangle)
						//{
						//	centroid2 = THIRD*(pos_clock + pos_out + info.pos);

						//	// project to radius of insulator
						//} else {

						//}


						kappa_parallel = 0.0;
						if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
						{
							kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
						}
						else {
							kappa_parallel = 0.5*p_kappa_n[izTri[iNeigh]];
						}
						{
							short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
							if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
							{
								kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
							}
							else {
								kappa_parallel += 0.5*p_kappa_n[izTri[iPrev]];
							}
						}

						if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))
						{
							f64 edgelen = edge_normal.modulus();

							ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
								(T_out - shared_T[threadIdx.x]) / (pos_out - info.pos).modulus();
						}
						else {
							f64 Area_quadrilateral = 0.5*(
								(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
								+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
								+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
								+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
								);

							// When we come to do the other species, make a subroutine.

							if ((T_clock == 0.0) || (T_anti == 0.0)) {

								f64 edgelen = edge_normal.modulus();
								
								ourrates.NnTn += TWOTHIRDS * kappa_parallel * edgelen *
									(T_out - shared_T[threadIdx.x]) / (pos_out - info.pos).modulus();

							} else {

								grad_T.x = 0.5*( // notice minus
									(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
									+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
									+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
									+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
									) / Area_quadrilateral;

								grad_T.y = -0.5*( // notice minus
									(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
									+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
									+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
									+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
									) / Area_quadrilateral;

								ourrates.NnTn += TWOTHIRDS * kappa_parallel * grad_T.dot(edge_normal);
							};
							// This is correct, grad T in same coordinates as edge_normal...
						};
					} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

					  // Now go round:	
					endpt_clock = endpt_anti;
					pos_clock = pos_out;
					pos_out = pos_anti;
#ifdef BWDSIDET
					T_clock = T_out;
					T_out = T_anti;
#else
					T_clock = T_outk;
					T_outk = T_anti;
#endif

				}; // next iNeigh

			}; // was it DOMAIN_VERTEX? Do what otherwise?
		}; // was it OUTERMOST/INNERMOST?
	};

	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################

	{
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_i + 2 * iVertex, 2 * sizeof(f64));
		memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_i + 2 * iVertex, 2 * sizeof(f64));
	}
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_T[threadIdx.x] = p_T_major[iVertex].Ti;
		// Notice major inefficiency caused by not making them scalar T arrays
	}
	else {
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();


	if ((bUseMask == 0) || (bMask[1] == true)) // either there is no masking, or this is switched on
	{
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
		{
			// [ Ignore flux into edge of outermost vertex I guess ???]
		}
		else {
			if (info.flag == DOMAIN_VERTEX) {

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_clock = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_clock = info2.pos;
#ifdef BWDSIDET
					T_clock = p_T_major[indexneigh].Ti;
#endif
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == NEEDS_ANTI) {
					pos_clock = Anticlock_rotate2(pos_clock);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_clock = Clockwise_rotate2(pos_clock);
				};
#ifndef BWDSIDET
				T_clock = p_T_k[indexneigh].Ti;
#endif

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_out = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_out = info2.pos;
#ifdef BWDSIDET
					T_out = p_T_major[indexneigh].Ti;
#endif
				};
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
				if (PBC == NEEDS_ANTI) {
					pos_out = Anticlock_rotate2(pos_out);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_out = Clockwise_rotate2(pos_out);
				};

#ifndef BWDSIDET
				T_outk = p_T_k[indexneigh].Ti;
#endif

				if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
				{
					endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
					endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;
//
//				if (T_clock == 0.0) {
//#ifdef BWDSIDET
//					T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//					T_clock = T_outk;
//#endif
//				};
//				Mimic

#pragma unroll MAXNEIGH_d
				for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
				{
					{
						short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
					}
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
						T_anti = shared_T[indexneigh - StartMajor];
#endif
					}
					else {
						structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
						pos_anti = info2.pos;
#ifdef BWDSIDET
						T_anti = p_T_major[indexneigh].Ti;
#endif
					};
#ifndef BWDSIDET
					T_anti = p_T_k[indexneigh].Ti;

					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						T_out = shared_T[indexneigh - StartMajor];
					}
					else {
						T_out = p_T_major[indexneigh].Ti;
					};
#endif
					if (PBC == NEEDS_ANTI) {
						pos_anti = Anticlock_rotate2(pos_anti);
					};
					if (PBC == NEEDS_CLOCK) {
						pos_anti = Clockwise_rotate2(pos_anti);
					};
//
//					if (T_anti == 0.0) {
//#ifdef BWDSIDET
//						T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//						T_anti = T_outk;
//#endif					
//					}; // So we are receiving 0 then doing this. But how come?
//					Mimic

					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];
					}
					else {
#ifdef CENTROID_HEATCONDUCTION
						endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
						endpt_anti = p_cc[izTri[iNeigh]];
#endif					
					}
					PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
					if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

					edge_normal.x = (endpt_anti.y - endpt_clock.y);
					edge_normal.y = (endpt_clock.x - endpt_anti.x);

					// SMARTY:
					if (TestDomainPos(pos_out))
					{
						kappa_parallel = 0.0;
						f64 nu;
						if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
						{
							kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
							nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
						} else {
							kappa_parallel = 0.5*p_kappa_i[izTri[iNeigh]];
							nu = 0.5*p_nu_i[izTri[iNeigh]];
						};

						{
							short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
							if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
							{
								kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
								nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
							} else {
								kappa_parallel += 0.5*p_kappa_i[izTri[iPrev]];
								nu += 0.5*p_nu_i[izTri[iPrev]];
							}
						}

						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
						{
							B_out = shared_B[indexneigh - StartMajor];
						} else {
							f64_vec3 B_out3 = p_B_major[indexneigh];
							B_out = B_out3.xypart();
						}
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
						if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

						if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))							
						{
							// Use longitudinal:

							f64 edgelen = edge_normal.modulus();
							f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));
							f64_vec3 omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);

							f64 long_contrib = TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
								(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
								/ (delta_out*edgelen*(nu * nu + omega.dot(omega)));
							ourrates.NiTi += long_contrib;

						} else {
							f64 Area_quadrilateral = 0.5*(
								(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
								+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
								+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
								+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
								);
							grad_T.x = 0.5*(
								(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
								+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
								+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
								) / Area_quadrilateral;
							grad_T.y = -0.5*( // notice minus
								(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
								+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
								+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
								) / Area_quadrilateral;

							//kappa.xx = kappa_parallel * (nu_eHeart*nu_eHeart + omega.x*omega.x) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.xy = kappa_parallel * (omega.x*omega.y - nu_eHeart *omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.yx = kappa_parallel * (omega.x*omega.y + nu_eHeart * omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.yy = kappa_parallel * (omega.y*omega.y + nu_eHeart * nu_eHeart) / (nu_eHeart * nu_eHeart + omega_sq);

							{ // scoping brace
								f64_vec3 omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);
								// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?

								f64 contrib = TWOTHIRDS * kappa_parallel *(
									edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y - nu * omega.z)*grad_T.y)
									+ edge_normal.y*((omega.x*omega.y + nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
									) / (nu * nu + omega.dot(omega));

								// Rule 1. Not a greater flow than isotropic
								// Rule 2. Not the opposite direction to isotropic - minimum zero
								f64 iso_contrib = TWOTHIRDS * kappa_parallel *(edge_normal.x*grad_T.x + edge_normal.y*grad_T.y);
								if (contrib > 0.0) {
									if ((iso_contrib > 0.0) && (contrib > iso_contrib)) contrib = iso_contrib;
									if (iso_contrib < 0.0) contrib = 0.0;
								} else {
									if ((iso_contrib < 0.0) && (contrib < iso_contrib)) contrib = iso_contrib;
									if (iso_contrib > 0.0) contrib = 0.0;
								}
								//
								//						if (TESTHEATFULL) printf("%d iNeigh %d kappa_ion %1.8E nu %1.8E |o| %1.8E contrib %1.8E \n",
								//							iVertex, iNeigh, kappa_parallel, nu,
								//							omega.modulus(),
								//							TWOTHIRDS * kappa_parallel *(
								//								edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y + nu * omega.z)*grad_T.y)
								//								+ edge_normal.y*((omega.x*omega.y - nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
								//								) / (nu * nu + omega.dot(omega))
								//						);
								//
								if (bCheckWhetherToDoctorUp) {

									// Now ask if this flow is going uphill:

									bool b_out = p_bool_longi[indexneigh * 2];
									bool b_here = p_bool_longi[iVertex * 2]; // 2 random reads --- we could put bools into shared easily

									if (b_out || b_here) {
										// use longitudinal flows

										f64 edgelen = edge_normal.modulus();
										f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

										f64 long_contrib = TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
											(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
											/ (delta_out*edgelen*(nu * nu + omega.dot(omega)));
										//		printf("ION %d : %d T T_out %1.8E %1.8E T_put T_putout %1.8E %1.8E cont %1.9E long %1.9E\n",
										//			iVertex, indexneigh, shared_T[threadIdx.x], T_out, T_here2, Tout2, contrib, long_contrib);

												// if (((T_here2 < Tout2) && (contrib < 0.0)) || ((T_here2 > Tout2) && (contrib > 0.0))) {
												// Either we are less but shrinking or more but growing

										contrib = long_contrib;
									};
								};

								ourrates.NiTi += contrib;
							}; // scoping brace
						}
					} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

					  // Now go round:	
					endpt_clock = endpt_anti;
					pos_clock = pos_out;
					pos_out = pos_anti;
#ifndef BWDSIDET
					T_clock = T_outk;
					T_outk = T_anti;
#else
					T_clock = T_out;
					T_out = T_anti;
#endif

				}; // next iNeigh

			}; // was it DOMAIN_VERTEX? Do what otherwise?
		}; // was it OUTERMOST/INNERMOST?
	}; // mask

	__syncthreads();

	// Did we make sure to include a call to syncthreads every time we carried on to update shared memory data in every other routine?
	// ##################################################################################################################################

	{
		memcpy(&(shared_kappa[threadIdx.x * 2]), p_kappa_e + 2 * iVertex, 2 * sizeof(f64));
		memcpy(&(shared_nu[threadIdx.x * 2]), p_nu_e + 2 * iVertex, 2 * sizeof(f64));
	}
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		shared_T[threadIdx.x] = p_T_major[iVertex].Te;
	}
	else {
		shared_T[threadIdx.x] = 0.0;
	}

	__syncthreads();

	if ((bUseMask) && (bMask[2] == 0)) return;
	
	{
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST))
		{
			// [ Ignore flux into edge of outermost vertex I guess ???]
		}
		else {
			if (info.flag == DOMAIN_VERTEX) {

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_clock = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_clock = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_clock = info2.pos;
#ifdef BWDSIDET
					T_clock = p_T_major[indexneigh].Te;
#endif
				};
#ifndef BWDSIDET
				T_clock = p_T_k[indexneigh].Te;
#endif

				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == NEEDS_ANTI) {
					pos_clock = Anticlock_rotate2(pos_clock);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_clock = Clockwise_rotate2(pos_clock);
				};

				indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + 0];
				if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
				{
					pos_out = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
					T_out = shared_T[indexneigh - StartMajor];
#endif
				}
				else {
					structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
					pos_out = info2.pos;
#ifdef BWDSIDET
					T_out = p_T_major[indexneigh].Te;
#endif
				};
#ifndef BWDSIDET
				T_outk = p_T_k[indexneigh].Te;
#endif
				PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + 0];
				if (PBC == NEEDS_ANTI) {
					pos_out = Anticlock_rotate2(pos_out);
				};
				if (PBC == NEEDS_CLOCK) {
					pos_out = Clockwise_rotate2(pos_out);
				};

				if ((izTri[info.neigh_len - 1] >= StartMinor) && (izTri[info.neigh_len - 1] < EndMinor))
				{
					endpt_clock = shared_pos[izTri[info.neigh_len - 1] - StartMinor];
				}
				else {
#ifdef CENTROID_HEATCONDUCTION
					endpt_clock = p_info_minor[izTri[info.neigh_len - 1]].pos;
#else
					endpt_clock = p_cc[izTri[info.neigh_len - 1]];
#endif
				}
				PBC = PBCtri[MAXNEIGH_d*threadIdx.x + info.neigh_len - 1];
				if (PBC == ROTATE_ME_CLOCKWISE) endpt_clock = Clockwise_d * endpt_clock;
				if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_clock = Anticlockwise_d * endpt_clock;
//
//				if (T_clock == 0.0) {
//#ifdef BWDSIDET
//					T_clock = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//					T_clock = T_outk;
//#endif
//				};
//				Mimic

#pragma unroll MAXNEIGH_d
				for (iNeigh = 0; iNeigh < info.neigh_len; iNeigh++)
				{
					{
						short iNext = iNeigh + 1; if (iNext == info.neigh_len) iNext = 0;
						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNext];
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNext];
					}
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						pos_anti = shared_pos_verts[indexneigh - StartMajor];
#ifdef BWDSIDET
						T_anti = shared_T[indexneigh - StartMajor];
#endif
					}
					else {
						structural info2 = p_info_minor[indexneigh + BEGINNING_OF_CENTRAL];
						pos_anti = info2.pos;
#ifdef BWDSIDET
						T_anti = p_T_major[indexneigh].Te;
#endif
					};
#ifndef BWDSIDET
					T_anti = p_T_k[indexneigh].Te;
					indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
					if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
					{
						T_out = shared_T[indexneigh - StartMajor];
					}
					else {
						T_out = p_T_major[indexneigh].Te;
					}
#endif
					if (PBC == NEEDS_ANTI) {
						pos_anti = Anticlock_rotate2(pos_anti);
					};
					if (PBC == NEEDS_CLOCK) {
						pos_anti = Clockwise_rotate2(pos_anti);
					};
//
//					if (T_anti == 0.0) {
//#ifdef BWDSIDET
//						T_anti = 0.5*(shared_T[threadIdx.x] + T_out);
//#else
//						T_anti = T_outk;
//#endif					
//					}; // So we are receiving 0 then doing this. But how come?
//					mimic

					if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
					{
						endpt_anti = shared_pos[izTri[iNeigh] - StartMinor];

						if (0) {
							printf("%d : %d endpt_anti %1.9E %1.9E SHARED endpt_clock %1.9E %1.9E izTri[iNeigh] %d\n",
								iVertex, iNeigh, endpt_anti.x, endpt_anti.y, endpt_clock.x, endpt_clock.y, izTri[iNeigh]);
						}
					}
					else {
#ifdef CENTROID_HEATCONDUCTION
						endpt_anti = p_info_minor[izTri[iNeigh]].pos;
#else
						endpt_anti = p_cc[izTri[iNeigh]];
#endif

						if (0) {
							printf("%d : %d endpt_anti %1.9E %1.9E GLOBAL endpt_clock %1.9E %1.9E izTri[iNeigh] %d\n",
								iVertex, iNeigh, endpt_anti.x, endpt_anti.y, endpt_clock.x, endpt_clock.y, izTri[iNeigh]);
						}
					}
					PBC = PBCtri[MAXNEIGH_d*threadIdx.x + iNeigh];
					if (PBC == ROTATE_ME_CLOCKWISE) endpt_anti = Clockwise_d * endpt_anti;
					if (PBC == ROTATE_ME_ANTICLOCKWISE) endpt_anti = Anticlockwise_d * endpt_anti;

					// It decided to rotate something it shouldn't oughta. Rotated tri 23600 = tri 2 for 11582.

					edge_normal.x = (endpt_anti.y - endpt_clock.y);
					edge_normal.y = (endpt_clock.x - endpt_anti.x);

					// SMARTY:
					if (TestDomainPos(pos_out))
					{

						kappa_parallel = 0.0;
						f64 nu;
						if ((izTri[iNeigh] >= StartMinor) && (izTri[iNeigh] < EndMinor))
						{
							kappa_parallel = 0.5*shared_kappa[izTri[iNeigh] - StartMinor];
							nu = 0.5*shared_nu[izTri[iNeigh] - StartMinor];
						}
						else {
							kappa_parallel = 0.5*p_kappa_e[izTri[iNeigh]];
							nu = 0.5*p_nu_e[izTri[iNeigh]];
						};

						{
							short iPrev = iNeigh - 1; if (iPrev < 0) iPrev = info.neigh_len - 1;
							if ((izTri[iPrev] >= StartMinor) && (izTri[iPrev] < EndMinor))
							{
								kappa_parallel += 0.5*shared_kappa[izTri[iPrev] - StartMinor];
								nu += 0.5*shared_nu[izTri[iPrev] - StartMinor];
							}
							else {
								kappa_parallel += 0.5*p_kappa_e[izTri[iPrev]];
								nu += 0.5*p_nu_e[izTri[iPrev]];
							}
						}

						indexneigh = Indexneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
						{
							B_out = shared_B[indexneigh - StartMajor];
						}
						else {
							f64_vec3 B_out3 = p_B_major[indexneigh];
							B_out = B_out3.xypart();
						}
						PBC = PBCneigh[MAXNEIGH_d*threadIdx.x + iNeigh];
						if (PBC == NEEDS_ANTI) 	B_out = Anticlock_rotate2(B_out);
						if (PBC == NEEDS_CLOCK)	B_out = Clockwise_rotate2(B_out);

						if ((!TestDomainPos(pos_clock)) || (!TestDomainPos(pos_anti)))
						{

							// Use longitudinal:

							f64 edgelen = edge_normal.modulus();
							f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));
							f64_vec3 omega = Make3(qoverMc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qoverMc);

							f64 long_contrib = TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
								(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
								/ (delta_out*edgelen*(nu * nu + omega.dot(omega)));

							ourrates.NeTe += long_contrib;

							if (TESTHEATFULL) printf("iVertex %d iNeigh %d long_contrib %1.14E T_out %1.9E ours %1.9E kappa_par %1.9E factor %1.9E\n",
								iVertex, iNeigh, long_contrib,
								T_out, shared_T[threadIdx.x], kappa_parallel, 
								(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
								/ (edgelen*(nu * nu + omega.dot(omega))));
						}
						else {
							f64 Area_quadrilateral = 0.5*(
								(info.pos.x + pos_anti.x)*(info.pos.y - pos_anti.y)
								+ (pos_clock.x + info.pos.x)*(pos_clock.y - info.pos.y)
								+ (pos_out.x + pos_clock.x)*(pos_out.y - pos_clock.y)
								+ (pos_anti.x + pos_out.x)*(pos_anti.y - pos_out.y)
								);
							grad_T.x = 0.5*(
								(shared_T[threadIdx.x] + T_anti)*(info.pos.y - pos_anti.y)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.y - info.pos.y)
								+ (T_out + T_clock)*(pos_out.y - pos_clock.y)
								+ (T_anti + T_out)*(pos_anti.y - pos_out.y)
								) / Area_quadrilateral;
							grad_T.y = -0.5*( // notice minus
								(shared_T[threadIdx.x] + T_anti)*(info.pos.x - pos_anti.x)
								+ (T_clock + shared_T[threadIdx.x])*(pos_clock.x - info.pos.x)
								+ (T_out + T_clock)*(pos_out.x - pos_clock.x)
								+ (T_anti + T_out)*(pos_anti.x - pos_out.x)
								) / Area_quadrilateral;

							//kappa.xx = kappa_parallel * (nu_eHeart*nu_eHeart + omega.x*omega.x) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.xy = kappa_parallel * (omega.x*omega.y - nu_eHeart *omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.yx = kappa_parallel * (omega.x*omega.y + nu_eHeart * omega.z) / (nu_eHeart * nu_eHeart + omega_sq);
							//kappa.yy = kappa_parallel * (omega.y*omega.y + nu_eHeart * nu_eHeart) / (nu_eHeart * nu_eHeart + omega_sq);

							{ // scoping brace
								f64_vec3 omega = Make3(qovermc * 0.5*(shared_B[threadIdx.x] + B_out), BZ_CONSTANT*qovermc);
								// PROBABLY ALWAYS SPILLED INTO GLOBAL -- WHAT CAN WE DO?

								f64 contrib = TWOTHIRDS * kappa_parallel *(
									edge_normal.x*((nu*nu + omega.x*omega.x)*grad_T.x + (omega.x*omega.y - nu * omega.z)*grad_T.y)
									+ edge_normal.y*((omega.x*omega.y + nu * omega.z)*grad_T.x + (omega.y*omega.y + nu * nu)*grad_T.y)
									) / (nu * nu + omega.dot(omega));

								// Rule 1. Not a greater flow than isotropic
								// Rule 2. Not the opposite direction to isotropic - minimum zero
								f64 iso_contrib = TWOTHIRDS * kappa_parallel *(edge_normal.x*grad_T.x + edge_normal.y*grad_T.y);


								if (TESTHEATFULL) printf(
									"iVertex %d iNeigh %d contrib %1.9E iso_contrib %1.9E \n"
									"edge_normal %1.8E %1.8E \n"
									"T %1.9E Tout %1.9E T_anti %1.9E T_clock %1.9E\n"
									"   kappa_par %1.9E nu %1.9E |omega| %1.9E Area %1.9E\n"
									"our_n %1.9E our n_n %1.9E nearby n %1.9E %1.9E\n"
									"pos %1.8E %1.8E opp %1.8E %1.8E anti %1.8E %1.8E clock %1.8E %1.8E\n"
									"omega %1.8E %1.8E grad_T %1.9E %1.9E \n"
									"=================================================\n"
									, iVertex, iNeigh,
									contrib, iso_contrib,
									edge_normal.x, edge_normal.y, shared_T[threadIdx.x], T_out, T_anti, T_clock,
									kappa_parallel, nu, sqrt(omega.dot(omega)),
									p_AreaMajor[iVertex],
									p_n_major[iVertex].n, p_n_major[iVertex].n_n, p_n_major[indexneigh].n, p_n_major[indexneigh].n_n,
									info.pos.x, info.pos.y, pos_out.x, pos_out.y, pos_anti.x, pos_anti.y, pos_clock.x, pos_clock.y,
									omega.x, omega.y, grad_T.x, grad_T.y);

								if (TESTHEATFULL) printf("shared B[threadIdx.x] %1.10E %1.10E B_out %1.10E %1.10E\n",
									shared_B[threadIdx.x].x, shared_B[threadIdx.x].y, B_out.x, B_out.y);

								if (contrib > 0.0) {
									if ((iso_contrib > 0.0) && (contrib > iso_contrib)) contrib = iso_contrib;
									if (iso_contrib < 0.0) contrib = 0.0;
								}
								else {
									if ((iso_contrib < 0.0) && (contrib < iso_contrib)) contrib = iso_contrib;
									if (iso_contrib > 0.0) contrib = 0.0;
								}

								if (bCheckWhetherToDoctorUp) {

									// Now ask if this flow is going uphill:
									bool b_out = p_bool_longi[indexneigh * 2 + 1];
									bool b_here = p_bool_longi[iVertex * 2 + 1];

									if (b_out || b_here) {
										// use longitudinal flows

										f64 edgelen = edge_normal.modulus();
										f64 delta_out = sqrt((info.pos.x - pos_out.x)*(info.pos.x - pos_out.x) + (info.pos.y - pos_out.y)*(info.pos.y - pos_out.y));

										f64 long_contrib = TWOTHIRDS * kappa_parallel *  (T_out - shared_T[threadIdx.x]) *
											(nu*nu*edgelen*edgelen + omega.dotxy(edge_normal)*omega.dotxy(edge_normal))
											/ (delta_out*edgelen*(nu * nu + omega.dot(omega)));
										//	printf("ELEC %d : %d T T_out %1.8E %1.8E T_put T_putout %1.8E %1.8E cont %1.9E long %1.9E\n",
										//		iVertex, indexneigh, shared_T[threadIdx.x], T_out, T_here2, Tout2, contrib, long_contrib);

											// if (((T_here2 < Tout2) && (contrib < 0.0)) || ((T_here2 > Tout2) && (contrib > 0.0))) {
											// Either we are less but shrinking or more but growing

										contrib = long_contrib;
										if (TESTHEATFULL) printf("contrib = long contrib %1.14E \n", contrib);
									};
								};

								if (TESTHEATFULL) printf("iVertex %d ourrates.NeTe before: %1.14E contrib %1.12E\n", iVertex, ourrates.NeTe, contrib);
								ourrates.NeTe += contrib;
								if (TESTHEATFULL) printf("iVertex %d ourrates.NeTe after: %1.14E \n", iVertex, ourrates.NeTe);

							}
						};
					} // if (pos_out.x*pos_out.x + pos_out.y*pos_out.y > ...)

					  // Now go round:	
					endpt_clock = endpt_anti;
					pos_clock = pos_out;
					pos_out = pos_anti;
#ifdef BWDSIDET
					T_clock = T_out;
					T_out = T_anti;
#else
					T_clock = T_outk;
					T_outk = T_anti;
#endif

				}; // next iNeigh

				memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));

			}; // was it DOMAIN_VERTEX? Do what otherwise?
		}; // was it OUTERMOST/INNERMOST?
	};
}

__global__ void kernelCreatePutativeT(
	f64 hsub,
	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_k,
	// T3 * __restrict__ p_T_putative,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	NTrates * __restrict__ NTadditionrates,

	bool * __restrict__ p_boolarray, // 2x NMAJOR
	bool * __restrict__ p_bFailedtest,
	bool * __restrict__ p_bMask3,
	bool * __restrict__ p_bMaskBlock, // do 1 for all species
	bool bUseMask
)
{
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	
	if (iVertex == VERTCHOSEN) printf("kernelCreatePutative %d : %d %d \n", iVertex,
		(bUseMask) ? 1 : 0, (p_bMaskBlock[blockIdx.x]) ? 1 : 0);

	if ((bUseMask) && (p_bMaskBlock[blockIdx.x] == 0)) return;
	
	
	bool bMask[3];
	if (bUseMask) {
		bMask[1] = p_bMask3[iVertex + NUMVERTICES];
		bMask[2] = p_bMask3[iVertex + 2 * NUMVERTICES];

		if (iVertex == VERTCHOSEN) printf("kernelCreatePutative %d : %d %d \n", iVertex,
			(bMask[1]) ? 1 : 0, (bMask[2]) ? 1 : 0);

		if ((bMask[1] == 0) && (bMask[2] == 0)) return; // we do nothing here with neutrals
	};
	
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	T3 T_k = p_T_k[iVertex];
	nvals n = p_n_major[iVertex];
	f64 AreaMajor = p_AreaMajor[iVertex];
	NTrates NT = NTadditionrates[iVertex];
	T3 T_put;
	// T_put.Tn = T_k.Tn + hsub* NT.NeTe / (n.n_n*AreaMajor); // serves no purpose...
	T_put.Ti = T_k.Ti + hsub*NT.NiTi / (n.n*AreaMajor);
	T_put.Te = T_k.Te + hsub*NT.NeTe / (n.n*AreaMajor);

	//if (iVertex == VERTCHOSEN) printf("%d T_e_k %1.8E NeTe %1.8E N %1.8E T_put %1.8E\n",
	//	iVertex, T_k.Te, NT.NeTe, (n.n*AreaMajor), T_put.Te);


	if (iVertex == VERTCHOSEN) printf("kernelCreatePutative %d : T_put.Te %1.10E NeTe %1.10E \n", iVertex,
		T_put.Te, NT.NeTe);

	bool ourbools[2];
	bool bAlert = false;
	memcpy(ourbools, p_boolarray + 2 * iVertex, sizeof(bool) * 2);

	//if (iVertex == VERTCHOSEN) printf("%d Te_putative %1.10E NT.NeTe %1.10E ourbool %d bAlert %d\n", iVertex, T_put.Te, NT.NeTe,
	//	ourbools[1]?1:0, bAlert?1:0);
	
	if (((bUseMask == 0) || (bMask[1] == true)) && (T_put.Ti < 0.0)) {
		if (ourbools[0] == 0) bAlert = true;
		ourbools[0] = true;
	};
	if (((bUseMask == 0) || (bMask[2] == true)) && (T_put.Te < 0.0)) {
		if (ourbools[1] == 0) bAlert = true;
		ourbools[1] = true;
	};

	if (iVertex == VERTCHOSEN) printf("kernelCreatePutative %d :ourbools[1] %d \n", iVertex,
		(ourbools[1]) ? 1 : 0);

	memcpy(p_boolarray + 2 * iVertex, ourbools, sizeof(bool) * 2);

	if (bAlert) p_bFailedtest[blockIdx.x] = true;
}


__global__ void kernelReturnNumberNegativeT(
	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T,
	long * __restrict__ p_sum
)
{
	__shared__ long sum[threadsPerTileMajorClever];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	sum[threadIdx.x] = 0;
	if (info.flag == DOMAIN_VERTEX) {
		T3 T = p_T[iVertex];
		if ((T.Tn < 0.0) || (T.Ti < 0.0) || (T.Te < 0.0))
		{
			printf("iVertex %d T %1.8E %1.8E %1.8E flag %d pos %1.10E %1.10E\n", iVertex, T.Tn, T.Ti, T.Te, info.flag,
				info.pos.x, info.pos.y);
			sum[threadIdx.x] = 1;

			// Really does find only 1 -- 19498 Ti. Could spit out more about
			// why it happened. 
			// So is there an alternative?

		}
		// worth it? can we easier/better just blitz the out-of-domain T to 0 and load it?
	};

	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sum[threadIdx.x] += sum[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sum[threadIdx.x] += sum[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sum[blockIdx.x] = sum[0];
	};
}
/*
__global__ void kernelSetNeighboursBwd(
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izNeigh_vert,
	bool * __restrict__ p_bMask3)
{
	Won't work because it needs separate src and dest memory.

	__shared__ bool bMask[threadsPerTileMajorClever][3];

	long const iVertex = threadIdx.x + blockIdx.x*blockDim.x;
	
	memcpy(bMask[threadIdx.x], p_bMask3 + 3 * iVertex, sizeof(bool) * 3);

	long const StartMajor = blockIdx.x*blockDim.x;
	long const EndMajor = StartMajor + blockDim.x;
	// check row-major meaning.

	__syncthreads();
	
	bool bMask3[3], bMaskNeigh[3];
	bMask3[0] = bMask[threadIdx.x][0];
	bMask3[1] = bMask[threadIdx.x][1];
	bMask3[2] = bMask[threadIdx.x][2];
	
	structural info = p_info_minor[iVertex];
	
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		long izNeigh[MAXNEIGH_d];

		memcpy(izNeigh, p_izNeigh_vert + MAXNEIGH_d*iVertex, sizeof(long)*MAXNEIGH_d);

#pragma unroll MAXNEIGH_d
		for (int i = 0; (i < info.neigh_len); i++)
		{
			long indexneigh = izNeigh[i];
			if ((indexneigh >= StartMajor) && (indexneigh < EndMajor))
			{
				memcpy(bMaskNeigh, bMask[indexneigh - StartMajor], sizeof(bool) * 3);
			} else {
				memcpy(bMaskNeigh, p_bMask3 + 3 * indexneigh, sizeof(bool) * 3);
			};
			if (bMaskNeigh[0]) bMask3[0] = true;
			if (bMaskNeigh[1]) bMask3[1] = true;
			if (bMaskNeigh[2]) bMask3[2] = true;
		};

		if ((bMaskNeigh[0]) || (bMaskNeigh[1]) || (bMaskNeigh[2])) {
			memcpy(p_bMask3 + 3 * iVertex, bMask, sizeof(bool) * 3);
			// otherwise, it was 0 to start with; let it still be 0.
		};
	}
}
*/

__global__ void kernelSetBlockMaskFlag_CountEquations_reset_Tk(
	bool * __restrict__ p_bMask3,
	bool * __restrict__ p_bMaskBlock,
	long * __restrict__ p_longblock3,
	T3 * __restrict__ p_T_k,
	T3 * __restrict__ p_T
)
{
	__shared__ bool bAlert[3];
	__shared__ long sum0[threadsPerTileMajorClever];
	__shared__ long sum1[threadsPerTileMajorClever];
	__shared__ long sum2[threadsPerTileMajorClever]; // need to save all 3 values

	if (threadIdx.x < 3) 
		bAlert[threadIdx.x] = 0;

	sum0[threadIdx.x] = 0.0;
	sum1[threadIdx.x] = 0.0;
	sum2[threadIdx.x] = 0.0;

	__syncthreads();

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	bool bMask[3];
	//memcpy(bMask, p_bMask3 + 3 * iVertex, sizeof(bool) * 3); 
	
	bMask[0] = p_bMask3[iVertex];
	bMask[1] = p_bMask3[iVertex + NUMVERTICES];
	bMask[2] = p_bMask3[iVertex + NUMVERTICES*2];
	
	if (bMask[0]) {
		bAlert[0] = true;
		sum0[threadIdx.x]++;
	};
	if (bMask[1]) {
		bAlert[1] = true;
		sum1[threadIdx.x]++;
		printf("Ion: %d\n", iVertex);
	};
	if (bMask[2]) {
		bAlert[2] = true;    // maybe this does not work.
		sum2[threadIdx.x]++;
		printf("Elec: %d  |  ", iVertex);
	};
	
	if ((bMask[0]) || (bMask[1]) || (bMask[2]))
	{
		T3 T = p_T[iVertex];
		T3 Tk = p_T_k[iVertex];
		if (bMask[0]) T.Tn = Tk.Tn;
		if (bMask[1]) T.Ti = Tk.Ti;
		if (bMask[2]) T.Te = Tk.Te;
		p_T[iVertex] = T;
	}

	__syncthreads();

//	if (iVertex == VERTCHOSEN) printf(" %d bAlert %d %d %d \n",
//		iVertex, bAlert[0] ? 1 : 0, bAlert[1] ? 1 : 0, bAlert[2] ? 1 : 0);

	if (threadIdx.x == 0) {
		p_bMaskBlock[blockIdx.x] = (bAlert[0] || bAlert[1] || bAlert[2]);

	}
	// all this doing but we want to split into species solves anyway.

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sum0[threadIdx.x] += sum0[threadIdx.x + k];
			sum1[threadIdx.x] += sum1[threadIdx.x + k];
			sum2[threadIdx.x] += sum2[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sum0[threadIdx.x] += sum0[threadIdx.x + s - 1];
			sum1[threadIdx.x] += sum1[threadIdx.x + s - 1];
			sum2[threadIdx.x] += sum2[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_longblock3[blockIdx.x*3] = sum0[0];
		p_longblock3[blockIdx.x*3+1] = sum1[0];
		p_longblock3[blockIdx.x*3+2] = sum2[0];
	};

}

__global__ void kernelCompareForStability_andSetFlag(
	structural * __restrict__ p_info_minor,
	NTrates * __restrict__ p_NTrates1,
	NTrates * __restrict__ p_NTrates2,
	long * __restrict__ p_sum,
	bool * __restrict__ p_bMask3
)
{
	__shared__ long sum[threadsPerTileMajorClever];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double
	bool bMask[3];
	sum[threadIdx.x] = 0;
	
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	
	if (info.flag == DOMAIN_VERTEX) {
		NTrates dNTdt1 = p_NTrates1[iVertex];
		NTrates dNTdt2 = p_NTrates2[iVertex];

		//memcpy(bMask, p_bMask3 + iVertex * 3, sizeof(bool) * 3);
		bMask[0] = p_bMask3[iVertex];
		bMask[1] = p_bMask3[iVertex + NUMVERTICES];
		bMask[2] = p_bMask3[iVertex + NUMVERTICES * 2];
		
		// we want to check if 2 is greater magnitude than 1 and reversed sign
		if ((dNTdt2.NnTn*dNTdt1.NnTn < 0.0)
			&& (fabs(dNTdt2.NnTn) > fabs(dNTdt1.NnTn)))
		{
			sum[threadIdx.x]++;
			bMask[0] = 1;
		}
		if ((dNTdt2.NiTi*dNTdt1.NiTi < 0.0)
			&& (fabs(dNTdt2.NiTi) > fabs(dNTdt1.NiTi))) {
			sum[threadIdx.x]++;
			bMask[1] = 1;
		}
		if ((dNTdt2.NeTe*dNTdt1.NeTe < 0.0)
			&& (fabs(dNTdt2.NeTe) > fabs(dNTdt1.NeTe))) {
			sum[threadIdx.x]++;
			bMask[2] = 1;
		};
		
		p_bMask3[iVertex] = bMask[0];
		p_bMask3[iVertex + NUMVERTICES] = bMask[1];
		p_bMask3[iVertex + NUMVERTICES * 2] = bMask[2];

	};
	// non domain mask flags already set to 0
	
	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sum[threadIdx.x] += sum[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sum[threadIdx.x] += sum[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sum[blockIdx.x] = sum[0];
	};
}

__global__ void kernelCreatePutativeTandsave(
	f64 hsub,
	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_k,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	NTrates * __restrict__ NTadditionrates,
	T3 * __restrict__ p_T_dest,
	bool * bMask3
)
{
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX	// 2.5 double

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];     // 3 double
	bool bMask[3];

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
		T3 T_k = p_T_k[iVertex];
		nvals n = p_n_major[iVertex];
		f64 AreaMajor = p_AreaMajor[iVertex];
		NTrates NT = NTadditionrates[iVertex];
		T3 T_put;
		T_put.Tn = T_k.Tn + hsub* NT.NeTe / (n.n_n*AreaMajor);
		T_put.Ti = T_k.Ti + hsub*NT.NiTi / (n.n*AreaMajor);
		T_put.Te = T_k.Te + hsub*NT.NeTe / (n.n*AreaMajor);

		p_T_dest[iVertex] = T_put;

		memset(bMask, 0, sizeof(bool) * 3);
		if (T_put.Tn < 0.0) bMask[0] = 1;
		if (T_put.Ti < 0.0) bMask[1] = 1;
		if (T_put.Te < 0.0) bMask[2] = 1;

		
	} else {
		memset(bMask, 0, sizeof(bool) * 3);
	}
	if (iVertex == 22351) printf("22351 info.flag %d bMask %d %d %d \n",
		info.flag, (bMask[0] ? 1 : 0), (bMask[1] ? 1 : 0), (bMask[2] ? 1 : 0));
	bMask3[iVertex] = bMask[0];
	bMask3[iVertex + NUMVERTICES] = bMask[1];
	bMask3[iVertex + 2 * NUMVERTICES] = bMask[2];

	//memcpy(bMask3 + iVertex * 3, bMask, sizeof(bool) * 3);

}


__global__ void kernelIonisationRates_Forward_Euler(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_major,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	NTrates * __restrict__ NTadditionrates,
	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,

	// We are in major cells so actually output this to a fresh temp array (9 scalars)
	// which we then share out into minor cells.

	v4 * __restrict__ p_v,
	f64_vec3 * __restrict__ p_v_n,
	T3 * __restrict__ p_T_use_major,
	bool b_useTuse
)
					// ** SIMPLIFIED VERSION **
{

#define SAFETY_FACTOR 1.2
#define LEEWAY        1.0e-23
#define vAC 218687393.0  // Alfven Critical velocity = sqrt(13.6*1.6e-12*2/me)

	long const iVertex = blockIdx.x*blockDim.x + threadIdx.x;
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
	NTrates ourrates;
	f64_vec3 MAR_neut, MAR_ion, MAR_elec;
	v4 v;
	f64_vec3 v_n;
	f64 T_use;

	if (info.flag == DOMAIN_VERTEX)
	{
		// case DOMAIN_VERTEX:

		f64 lambda;
		f64 AreaMajor = p_AreaMajor[iVertex];
		T3 T_k = p_T_major[iVertex];
		if (b_useTuse) {
			T3 T = p_T_use_major[iVertex];
			T_use = T.Te;
		}
		else {
			T_use = T_k.Te;
		}

		nvals our_n = p_n_major[iVertex];
		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));
		memcpy(&MAR_neut, p_MAR_neut + iVertex, sizeof(f64_vec3));   // are we passing stuff from central then?
		memcpy(&MAR_ion, p_MAR_ion + iVertex, sizeof(f64_vec3));
		memcpy(&MAR_elec, p_MAR_elec + iVertex, sizeof(f64_vec3));  // it does mean d/dt (Nv)
		memcpy(&v, p_v + iVertex, sizeof(v4));
		memcpy(&v_n, p_v_n + iVertex, sizeof(f64_vec3));

		// 0 . What is lambda?

		f64 oldT1;

		f64 n_k = our_n.n;
		f64 n_n_k = our_n.n_n;
		f64 n_kplus1, n_n_kplus1, n_kplus2;
		f64 Gamma_ion, Gamma_rec, hn, hnn, Delta_ionise, Delta_rec;
		// lambda = 0.5*reduced mass*w0.dot(w0) / T_k.Te;

		f64 w0z = v.vez - v_n.z;
		// What is capital Theta of T_k ?
		//f64 w = sqrt(w0z*w0z); // WE ARE ONLY USING Z DIMENSION FOR ABSORBING KINETIC ENERGY 

		// Check again: how did we come up with the following formulas?
		// Off of the lambda spreadsheet or the v spreadsheet? I think lambda.

		f64 T_use_theta = T_k.Te;
		if (T_use_theta < 1.0e-12) T_use_theta = 1.0e-12;
		f64 Theta = (1.1 + 0.4e-12 / T_use_theta);
		if (w0z < vAC - 0.4e-4 / T_use_theta) {
			//Theta *= exp(-w*(vC - 0.4e-4 / T_use_theta - w)*1.0e-12
			//	/ (0.25*(vC - 0.4e-4 / T_use_theta)*(vC - 0.4e-4 / T_use_theta)*T_use_theta));
			// Multiply through to save on divisions?:
			Theta *= exp(-w0z*((vAC - w0z)* T_use_theta - 0.4e-4)*1.0e-12 /
				(0.25*(vAC* T_use_theta - 0.4e-4)*(vAC* T_use_theta - 0.4e-4)));
		};

		// Available KE:
		f64 Kconv = 0.5*m_e*m_n*n_k*n_n_k*(w0z*w0z) / (m_e*n_k + m_n*n_n_k);

		f64 coeff_on_ionizing = 0.5*T_k.Tn - 2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv);

		// Now compute f(Tk) = T_k+1 given using T_k

		f64 w = sqrt(0.5*(w0z*w0z + (v.vxy.x - v_n.x)*(v.vxy.x - v_n.x) + (v.vxy.y - v_n.y)*(v.vxy.y - v_n.y))); // CORRECTION FACTOR 0.5 ...
						
		f64 T_image1, T2, T_image2, T_oldimage1, Tkplus2minus1;

		hn = h_use*n_k;
		hnn = h_use*n_k*n_k;

		f64 T1 = T_use;  // first go. = Tk if b_useTuse == false.
		{
			Gamma_ion = GetIonizationRates(T1, w, &Gamma_rec);

			Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
			n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

			T_image1 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;
		}
		T2 = T_image1;


		// Skip over algorithm:

		if (Delta_ionise != Delta_ionise) printf("Nandelta %d Tuse %1.10E w %1.8E Gamma %1.10E rec %1.10E w0z %1.10E Kconv %1.10E\n", iVertex, T_use, w, Gamma_ion, Gamma_rec, w0z, Kconv);

		//if (iVertex == 16700) printf("Delta_ionise %1.10E rec %1.10E \n", Delta_ionise, Delta_rec);

		//f64 TeeV = T1/ kB;
		//f64 Tesq = TeeV*TeeV;
		//f64 Te3 = TeeV*Tesq;
		//f64 Te4 = Tesq*Tesq;
		//f64 calc1 = (ionize_coeffs[0][0][4] + ionize_coeffs[0][0][3] * TeeV
		//	+ ionize_coeffs[0][0][2] * Tesq + ionize_coeffs[0][0][1] * Te3
		//	+ ionize_coeffs[0][0][0] * Te4);
		//f64 calc2 = (ionize_coeffs[0][0][0] + ionize_coeffs[0][0][1] * TeeV
		//	+ ionize_coeffs[0][0][2] * Tesq + ionize_coeffs[0][0][3] * Te3
		//	+ ionize_coeffs[0][0][4] * Te4);
		//if (iVertex == 16700) printf("ionize_coeffs[0][0] %1.12E %1.12E %1.12E %1.12E %1.12E \n"
		//	"TeeV %1.12E calc1  %1.12E calc2 %1.12E exp(calc1) %1.12E exp(calc2) %1.12E\n",
		//	ionize_coeffs[0][0][0], ionize_coeffs[0][0][1], ionize_coeffs[0][0][2], ionize_coeffs[0][0][3], ionize_coeffs[0][0][4],
		//	TeeV, calc1, calc2, exp(calc1), exp(calc2));


		f64 dNdt_ionise = AreaMajor*Delta_ionise / h_use;
		f64 dNdt_recombine = AreaMajor*Delta_rec / h_use;

		ourrates.N += dNdt_ionise - dNdt_recombine;
		ourrates.Nn += dNdt_recombine - dNdt_ionise;

		// Store existing energy density:
		f64 Energy_k = 1.5*(n_k*(T_k.Te + T_k.Ti) + n_n_k*T_k.Tn) +
			0.5*((m_e + m_i)*n_k*(v.vxy.dot(v.vxy)) + m_e*n_k*v.vez*v.vez + m_i*n_k*v.viz*v.viz + m_n*n_n_k*v_n.dot(v_n));


		// 1. Calculate kinetic energy absorption impact on vez, vnz
		// ie Ionization resistance to current

		n_kplus1 = n_k + Delta_ionise - Delta_rec;
		n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

		// Absorbed DKE:
		f64 deltaKE = -(2.0*Theta*Kconv / (3.0*n_k*T_k.Te + 2.0*Theta*Kconv))*Delta_ionise*13.6*kB;
		f64 new_vz_diff = sqrt(m_e*n_kplus1 + m_n*n_n_kplus1*
			((n_k*n_n_k / (m_e*n_k + m_n*n_n_k))*(w0z*w0z) + 2.0*deltaKE / (m_e*m_n)) /
			n_kplus1*n_n_kplus1);
		f64 delta_vez = m_n*n_n_kplus1*(w0z + new_vz_diff) /
			(m_n*n_n_kplus1 + m_e*n_kplus1);
		f64 delta_vnz = -m_e*n_kplus1*delta_vez / (m_n*n_n_kplus1);

		// Check: w0 = vez-vnz - tick
		// should change to scalar.

		MAR_neut.z += AreaMajor*n_n_kplus1*delta_vnz / h_use;
		MAR_elec.z += AreaMajor*n_kplus1*delta_vez / h_use;

		f64_vec3 ve_kplus1, vi_kplus1, vn_kplus1;

		// Store alongside: v_k+1 so that we can follow the anticipated change in energy,
		// to create energy balance:
		ve_kplus1.x = v.vxy.x*(n_k / n_kplus1);
		ve_kplus1.y = v.vxy.y*(n_k / n_kplus1);
		ve_kplus1.z = v.vez*(n_k / n_kplus1) + delta_vez; // we need to store v, we could also store nv if we wanted.

		vi_kplus1.x = v.vxy.x*(n_k / n_kplus1);
		vi_kplus1.y = v.vxy.y*(n_k / n_kplus1);
		vi_kplus1.z = v.viz*(n_k / n_kplus1);

		vn_kplus1 = v_n*(n_n_k / n_n_kplus1);
		vn_kplus1.z += delta_vnz;

		// 2. Add the effect of xfers on momenta:

		// What does MAR_neut mean? Nv?
		{
			f64_vec3 v_use;
			v_use.x = v.vxy.x;
			v_use.y = v.vxy.y;
			v_use.z = (m_e*v.vez + m_i*v.viz) / (m_e + m_i);
			MAR_neut += -dNdt_ionise*v_n + dNdt_recombine*v_use;
			MAR_ion += dNdt_ionise*v_n - dNdt_recombine*v_use;
			MAR_elec += dNdt_ionise*v_n - dNdt_recombine*v_use;

			vn_kplus1 -= (Delta_ionise*v_n - Delta_rec*v_use) / n_n_kplus1;
			// n_k+1 v_k+1 = n_k v_k + Delta_n*v_use => v_k+1 = (n_k/n_k+1) v_k + (Delta_n/n_k+1) v_use
			vi_kplus1 += (Delta_ionise*v_n - Delta_rec*v_use) / n_kplus1;
			ve_kplus1 += (Delta_ionise*v_n - Delta_rec*v_use) / n_kplus1;

		}
		if (MAR_elec.z != MAR_elec.z) printf("ivertex %d MAR_elec nan\n", iVertex);

		// . Ionization cooling & recombination heating
		//f64 coeff_on_ionizing = 0.5*T_k.Tn - 2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv);
		//		ourrates.NeTe +=
		//			dNdt_recombine*2.0*13.6*kB / 3.0
		//			- (2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv))*dNdt_ionise;
		// We can drop this: it will be accounted for by the final energy balance.

		// 3. Add to nT for x-fers due to species converting

		ourrates.NiTi += 0.5*dNdt_ionise*T_k.Tn;
		ourrates.NeTe += 0.5*dNdt_ionise*T_k.Tn;
		ourrates.NnTn -= dNdt_ionise*T_k.Tn;

		f64 nTe_kplus1 = T_k.Te*(n_k)+0.5*Delta_ionise*T_k.Tn;
		f64 nTi_kplus1 = T_k.Ti*(n_k)+0.5*Delta_ionise*T_k.Tn;
		f64 n_nTn_kplus1 = T_k.Tn*(n_n_k)-Delta_ionise*T_k.Tn;

		// 4. Energy balance through Te:
		// Maybe we should rather be seeking OVERALL energy balance where KE_result is from n_k+1, v_k+1
		// and we ensure that we have lost the right amount of energy overall.
		// That is the better way:

		f64 KE_result = 0.5*(m_e*n_kplus1*ve_kplus1.dot(ve_kplus1) + m_i*n_kplus1*vi_kplus1.dot(vi_kplus1)
			+ m_n*n_n_kplus1*vn_kplus1.dot(vn_kplus1));

		f64 Energy_density_kplus1 = KE_result + 1.5*(nTe_kplus1 + nTi_kplus1 + n_nTn_kplus1);
		f64 Energy_density_target = Energy_k - 13.6*kB*(Delta_ionise - Delta_rec);

		// Additional_heat = (KE_k + deltaKE) - KE_result; // usually positive
		// 1*1+3*3 > 2*2 + 2*2  so KE is generally decreasing by friction; KE_result < KE_k+deltaKE
		// KE_result + Added_heat + existing heat = desired total energy = KE_k + heat_k + deltaKE

		// 1.5 nT += Frictional_heating
		// NTe += (2/3) Area Frictional_heating
		ourrates.NeTe += 2.0*AreaMajor*
			(Energy_density_target - Energy_density_kplus1) / (3.0*h_use);



		// All this stuff is wrong - see full routine.




		// DEBUG:
		if (TEST_IONIZE) printf("iVertex %d n_k %1.9E N_k %1.9E Te_k %1.9E NeTe %1.9E h*NeTe %1.9E \n"
			"Ti_k %1.9E h*NiTi %1.9E Tn_k %1.9E h*NnTn %1.9E \n"
			"Delta_ionise %1.9E rec %1.9E \n",
			iVertex, n_k, n_k*AreaMajor, T_k.Te, ourrates.NeTe, h_use*ourrates.NeTe,
			T_k.Ti, h_use*ourrates.NiTi, T_k.Tn, h_use*ourrates.NnTn,
			Delta_ionise, Delta_rec
		);


		// DEBUG:
		if (n_k*AreaMajor*T_k.Te + ourrates.NeTe*h_use < 0.0)
			printf("%d Predicted Te %1.9E \n", iVertex, (n_k*AreaMajor*T_k.Te + ourrates.NeTe*h_use)/(n_k*AreaMajor));


		// DEBUG:
		if ((ourrates.NeTe != ourrates.NeTe)) printf("Nan NeTe %d \n", iVertex);
		if ((ourrates.NeTe != ourrates.NeTe)) printf("Nan NeTe %d \n", iVertex);
		if (MAR_elec.z != MAR_elec.z) printf("Nan MAR_elec.z %d \n", iVertex);
		if (MAR_elec.x != MAR_elec.x) printf("Nan MAR_elec.x %d \n", iVertex);
		if (MAR_neut.x != MAR_neut.x) printf("Nan MAR_neut.x %d \n", iVertex);
		if (MAR_ion.y != MAR_ion.y) printf("Nan MAR_ion.y %d \n", iVertex);


		memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));
		memcpy(p_MAR_neut + iVertex, &MAR_neut, sizeof(f64_vec3));
		memcpy(p_MAR_ion + iVertex, &MAR_ion, sizeof(f64_vec3));
		memcpy(p_MAR_elec + iVertex, &MAR_elec, sizeof(f64_vec3));


		//******************************************************************************************************


		//// f64 TeV = T.Te * one_over_kB; 
		//// We loaded in ourrates.NT which indicates the new heat available so we should include some of that.
		//// The main impact will be from heat conduction; dN/dt due to advection neglected here.
		//f64 TeV = one_over_kB * (T.Te*our_n.n*AreaMajor + h_use*ourrates.NeTe)/
		//	(our_n.n*AreaMajor + h_use*ourrates.N);
		//// Should be very careful here: ourrates.NeTe can soak to neutrals on timescale what? 1e-11?

		//if (TeV < 0.0) {
		//	printf("\n\niVertex %d : ourrates.N %1.14E denominator %1.14E \n"
		//		" AreaMajor %1.14E TeV %1.14E ourrates.NeTe %1.10E h %1.10E \n"
		//		"ourrates.Nn %1.10E n %1.10E n_n %1.10E Te %1.10E Tn %1.10E \n\n",
		//		iVertex, ourrates.N, 
		//		(our_n.n*AreaMajor + h_use*ourrates.N),
		//		AreaMajor, TeV, ourrates.NeTe, h_use,
		//		ourrates.Nn, our_n.n, our_n.n_n, T.Te, T.Tn);
		//	
		//}
		//f64 sqrtT = sqrt(TeV);
		//f64 temp = 1.0e-5*exp(-13.6 / TeV) / (13.6*(6.0*13.6 + TeV)); // = S / T^1/2
		//	// Let h n n_n S be the ionising amount,
		//	// h n S is the proportion of neutrals! Make sure we do not run out!

		////f64 hnS = (h_use*our_n.n*TeV*temp) / (sqrtT + h_use * our_n.n_n*temp*SIXTH*13.6);

		//	// d/dt (sqrtT) = 1/2 dT/dt T^-1/2. 
		//	// dT[eV]/dt = -TWOTHIRDS * 13.6* n_n* sqrtT *temp
		//	// d/dt (sqrtT) = -THIRD*13.6*n_n*temp;

		//// kind of midpoint, see SIXTH not THIRD:
		//f64 Model_of_T_to_half = TeV / (sqrtT + h_use*SIXTH*13.6*our_n.n_n*temp / (1.0 - h_use*(our_n.n_n - our_n.n)*temp*sqrtT));

		//f64 hS = h_use*temp*Model_of_T_to_half;
		//		
		//// NEW:
		//f64 ionise_rate = AreaMajor * our_n.n_n * our_n.n*hS / 
		//					(h_use*(1.0 + hS*(our_n.n-our_n.n_n)));   // dN/dt

		//ourrates.N += ionise_rate;
		//ourrates.Nn += -ionise_rate;


		//// Let nR be the recombining amount, R is the proportion.
		//TeV = T.Te * one_over_kB;
		//f64 Ttothe5point5 = sqrtT * TeV * TeV*TeV * TeV*TeV;
		//f64 hR = h_use * (our_n.n * our_n.n*8.75e-27*TeV) /
		//	(Ttothe5point5 + h_use * 2.25*TWOTHIRDS*13.6*our_n.n*our_n.n*8.75e-27);

		//// T/T^5.5 = T^-4.5
		//// T/(T^5.5+eps) < T^-4.5

		//// For some reason I picked 2.25 = 4.5/2 instead of 5.5/2.
		//// But basically it looks reasonable.

		//// Maybe the additional stuff is an estimate of the change in T[eV]^5.5??
		//// d/dt T^5.5 = 5.5 T^4.5 dT/dt 
		//// dT/dt = TWOTHIRDS * 13.6*( hR / h_use) = TWOTHIRDS * 13.6*( n^2 8.75e-27 T^-4.5) 
		//// d/dt T^5.5 = 5.5 TWOTHIRDS * 13.6*( n^2 8.75e-27 )  

		//f64 recomb_rate = AreaMajor * our_n.n * hR / h_use; // could reasonably again take hR/(1+hR) for n_k+1
		//ourrates.N -= recomb_rate;
		//ourrates.Nn += recomb_rate;

		//if (TEST) printf("%d recomb rate %1.10E ionise_rate %1.10E our_n.n %1.10E nn %1.10E hR %1.10E hS %1.10E\n"
		//	"h_use %1.8E sqrtTeV %1.10E Ttothe5point5 %1.9E Te %1.9E modelThalf %1.9E\n", iVertex,
		//	recomb_rate, ionise_rate, our_n.n, our_n.n_n, hR, hS, h_use, sqrtT, Ttothe5point5, T.Te, Model_of_T_to_half);

		//ourrates.NeTe += -TWOTHIRDS * 13.6*kB*(ionise_rate - recomb_rate) + 0.5*T.Tn*ionise_rate;
		//ourrates.NiTi += 0.5*T.Tn*ionise_rate;
		//ourrates.NnTn += (T.Te + T.Ti)*recomb_rate;
		//if (TEST) {
		//	printf("kernelIonisation %d NeTe %1.12E NiTi %1.12E NnTn %1.12E\n"
		//		"due to I+R : NeTe %1.12E NiTi %1.12E NnTn %1.12E\n"
		//		"d/dtNeTe/N %1.9E d/dtNiTi/N %1.9E d/dtNnTn/Nn %1.9E \n\n",
		//		iVertex, ourrates.NeTe, ourrates.NiTi, ourrates.NnTn,
		//		-TWOTHIRDS * 13.6*kB*(ionise_rate - recomb_rate) + 0.5*T.Tn*ionise_rate,
		//		0.5*T.Tn*ionise_rate,
		//		(T.Te + T.Ti)*recomb_rate,
		//		ourrates.NeTe / (our_n.n*AreaMajor), ourrates.NiTi / (our_n.n*AreaMajor), ourrates.NnTn / (our_n.n_n*AreaMajor));
		//};
		//memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));
	};
}
__global__ void kernelIonisationRates(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_major,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	NTrates * __restrict__ NTadditionrates,
	f64_vec3 * __restrict__ p_MAR_neut, 
	f64_vec3 * __restrict__ p_MAR_ion, 
	f64_vec3 * __restrict__ p_MAR_elec,

	// We are in major cells so actually output this to a fresh temp array (9 scalars)
	// which we then share out into minor cells.

	v4 * __restrict__ p_v,
	f64_vec3 * __restrict__ p_v_n,
	T3 * __restrict__ p_T_use_major,
	bool b_useTuse
	)
{

#define SAFETY_FACTOR 1.2
#define LEEWAY        1.0e-23
#define vAC 218687393.0  // Alfven Critical velocity = sqrt(13.6*1.6e-12*2/me)

	long const iVertex = blockIdx.x*blockDim.x + threadIdx.x;
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
	NTrates ourrates;
	f64_vec3 MAR_neut, MAR_ion, MAR_elec;
	v4 v;
	f64_vec3 v_n;
	f64 T_use;
	bool bZero_out = false;
	
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
	{
		// case DOMAIN_VERTEX:

		f64 lambda;
		f64 AreaMajor = p_AreaMajor[iVertex];
		T3 T_k = p_T_major[iVertex];
		
		if (b_useTuse) {
			T3 T = p_T_use_major[iVertex];
			T_use = T.Te;
		} else {
			T_use = T_k.Te;
		}

		nvals our_n = p_n_major[iVertex];
		f64 fac_uplift = ArtificialUpliftFactor(our_n.n, our_n.n_n);

		memcpy(&ourrates, NTadditionrates + iVertex, sizeof(NTrates));
		memcpy(&MAR_neut, p_MAR_neut + iVertex, sizeof(f64_vec3));   // are we passing stuff from central then?
		memcpy(&MAR_ion, p_MAR_ion + iVertex, sizeof(f64_vec3));
		memcpy(&MAR_elec, p_MAR_elec + iVertex, sizeof(f64_vec3));  // it does mean d/dt (Nv)
		memcpy(&v, p_v + iVertex, sizeof(v4));
		memcpy(&v_n, p_v_n + iVertex, sizeof(f64_vec3));

		if (TEST_IONIZE) printf("iVertex %d ourrates.NeTe original %1.10E \n", iVertex, ourrates.NeTe);

		// 0 . What is lambda?
		
		f64 oldT1;		
		f64 n_k = our_n.n;
		f64 n_n_k = our_n.n_n;
		f64 n_kplus1, n_n_kplus1, n_kplus2;
		f64 Gamma_ion, Gamma_rec, hn, hnn, Delta_ionise, Delta_rec;
		// lambda = 0.5*reduced mass*w0.dot(w0) / T_k.Te;

		f64 w0z = v.vez - v_n.z;
		// What is capital Theta of T_k ?
		//f64 w = sqrt(w0z*w0z); // WE ARE ONLY USING Z DIMENSION FOR ABSORBING KINETIC ENERGY 

		// Check again: how did we come up with the following formulas?
		// Off of the lambda spreadsheet or the v spreadsheet? I think lambda.

		f64 T_use_theta = T_k.Te;
		if (T_use_theta < 1.0e-12) T_use_theta = 1.0e-12;
		f64 Theta = (1.1 + 0.4e-12 / T_use_theta);
		if (w0z < vAC - 0.4e-4 / T_use_theta) {
			//Theta *= exp(-w*(vC - 0.4e-4 / T_use_theta - w)*1.0e-12
			//	/ (0.25*(vC - 0.4e-4 / T_use_theta)*(vC - 0.4e-4 / T_use_theta)*T_use_theta));
			// Multiply through to save on divisions?:
			Theta *= exp(-w0z*((vAC - w0z)* T_use_theta - 0.4e-4)*1.0e-12 /
				(0.25*(vAC* T_use_theta - 0.4e-4)*(vAC* T_use_theta - 0.4e-4)));
		};

		// Available KE:
		f64 Kconv = 0.5*m_e*m_n*n_k*n_n_k*(w0z*w0z) / (m_e*n_k + m_n*n_n_k);

		if (TEST_IONIZE) printf("iVertex %d w0z %1.10E Kconv %1.10E Theta %1.9E \n", iVertex,
			w0z, Kconv, Theta);

		f64 coeff_on_ionizing = 0.5*T_k.Tn - 2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv);

		// Now compute f(Tk) = T_k+1 given using T_k

		f64 w = sqrt(0.5*(w0z*w0z + (v.vxy.x - v_n.x)*(v.vxy.x - v_n.x) + (v.vxy.y - v_n.y)*(v.vxy.y - v_n.y))); // CORRECTION FACTOR 0.5 ...
																													 // ================
		// Made a mistake and saved data for v that is sqrt(2) times greater by missing 0.5 out of lambda
		// so
		// data for "1e7" is actually for 1.4e7. Thus pass 1/sqrt(2) times our velocity 

		f64 T_image1, T2, T_image2, T_oldimage1, Tkplus2minus1;

		hn = h_use*n_k;
		hnn = h_use*n_k*n_k;

		f64 T1 = T_use;  // first go. = Tk if b_useTuse == false.
		{
		//	if (TEST_IONIZE) {
		//		Gamma_ion = GetIonizationRatesDebug(T1, w, &Gamma_rec);
		//	} else {
				Gamma_ion = fac_uplift*GetIonizationRates(T1, w, &Gamma_rec);
		//	};

			Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
			n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

			T_image1 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec)/ n_kplus1;
		}
		T2 = T_image1;
		int sign_k = (T2 - T1 > 0.0) ? 1 : -1; // usually -1 , ie negative rate of change, net ionization
		//  Torigmove = T2 - T1; -- no, it's T_image-T_k that we wanna use.

 		if (TEST_IONIZE) printf("iVertex %d original move T2 %1.9E T1 %1.9E \n", iVertex, T2, T1);
		// X

		// it's ok to use sign_k for the sign of the T_use move
		// because if it's different sign to fwd move we never do overshooting test
		// But what about if it's T<0? so fwd is recombining but new shift of T
		// brings T_k+1<0.
		// In that case we should be detecting it right here.
		
		// First check if fwd next temperature would be negative:
		bool bAccept = false;		
		
		// Try allowing to access the b_test loop:
		bool b_test = b_useTuse;
		// check that this brings back the 77 - it doesn't

		if ((T2 < 0.0) && (b_useTuse) && (T_use > T_k.Te))
		{
			// in this case we should switch to T1 = T_k:

			f64 T1 = T_k.Te;  // first go. = Tk if b_useTuse == false.
			{
				Gamma_ion = fac_uplift*GetIonizationRates(T1, w, &Gamma_rec);

				Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
					((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
				Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
					((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
				n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
				n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

				T_image1 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;
			}
			T2 = T_image1;
			sign_k = (T2 - T1 > 0.0) ? 1 : -1;
			
			if (TEST_IONIZE) printf("iVertex %d switch to Tk \n", iVertex);
			// X

			// and turn off tests below involving assn of move?
			b_test = false;
		}
		 // DEBUG 2 -- it worked with this bit and b_test cut out
		


		if (T2 < 0.0) {
			
			while (T2 < 0.0) {

				oldT1 = T1;
				T1 *= 0.5;
				T_oldimage1 = T_image1; // save
				// Compute image of T1:
				{
					Gamma_ion = fac_uplift*GetIonizationRates(T1, w, &Gamma_rec);
					
					Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);

					n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
					n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

					T_image1 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;
				}
				T2 = T_image1;

				if (TEST_IONIZE) printf("iVertex %d T<0 loop: T2 %1.9E \n", iVertex, T2);
				// X

			};

			// 3(a) If 2^-i Tk is an acceptable point, accept it.

			if (T_image1 - T1 > 0.0) // T is now rising -- we crossed T_bwd from T_k
			{ // Note: bwd criterion: T_image(use T_use from T_k) element (0, fwd image of Tk)
				
				// 3.(c) If 2^-i T_k is lower than a bwd step, proceed to main loop with
				// 2^-i T_k as T_far and 2^-(i-1) T_k as T_near:
				
				T2 = T1; // "left point" (right if we were ascending)
				T1 *= 2.0; // "right point" -- may be T_k itself
				bAccept = false;
				T_image1 = T_oldimage1;
				T_image2 = T_image1;
				
				if (TEST_IONIZE) printf("iVertex %d T now rising: T1 %1.9E T2 %1.9E \n", iVertex, T1, T2);
				// X

			} else {
				// Test T1 for overshooting:
				// T2 is already defined as image of T1

				Gamma_ion = fac_uplift*GetIonizationRates(T2, w, &Gamma_rec);
				n_kplus2 = n_kplus1 + h_use*n_n_kplus1*n_kplus1*Gamma_ion -
										h_use*n_kplus1*n_kplus1*n_kplus1*Gamma_rec;
				Tkplus2minus1 = (n_kplus1 / n_kplus2 - 1.0)*T2 +
					coeff_on_ionizing*h_use*n_n_kplus1*Gamma_ion +
					TWOTHIRDS*13.6*kB*h_use*n_kplus1*n_kplus1*Gamma_rec;
				// compare this inflated difference of Tnext from T2
				// with the difference T2-Tk :
				
				if (Tkplus2minus1 < 0.0)// same sign as move from Tk to T2
				{
					bAccept = true; // accept this move Delta(T1)
					
				} else {
					// Test that the reversed magnitude is smaller.
					bAccept = (SAFETY_FACTOR*fabs(Tkplus2minus1) < fabs(T2-T_k.Te) + LEEWAY);
				};

				if (bAccept == false) {
					// Overshooting:

					if (TEST_IONIZE) printf("iVertex %d T overshooting 1\n", iVertex);
					// Y
					// No adjustment to T1, T2 needed.
					// Compute image of T2 under f_k:		
					Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
					T_image2 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;	
					// We want this defined when we enter secant loop.
				};
			};

		} else { // fwd T, or first step with T_use, didn't go below zero

			// 4. If we are so close to equilibrium that the proposed
			// change in temperature is tiny like 10^{-9}T then just set
			// the actual ionization to 0. We have only 10^{-6}/10^{-13}=10^{7} steps.
			
			// Changed factor to 1.0e-10
			if (fabs(T_image1 - T_k.Te) < 1.0e-10*T_k.Te) {
				// do nothing for a tiny move:
				Delta_ionise = 0.0;
				Delta_rec = 0.0;
				bAccept = true; 

				if (TEST_IONIZE) printf("iVertex %d small move accepted\n", iVertex);
				// Y

				bZero_out = true;

				// To move 10% would take 1e8 moves, we have only 1e-6/1e-13 = 1e7.
			} else {

				if (TEST_IONIZE) printf("%d b_test %d \n",iVertex, (b_test ? 1 : 0));
				// Y

				if (b_test) {
					// in this case we now want to check whether our move
					// is the same sign as the T_k move.

					// We can check the sign_k just by evaluating ionization rates at T_k
					// No, we pretty much need to work out which one is winning out.
					Gamma_ion = fac_uplift*GetIonizationRates(T_k.Te, w, &Gamma_rec);

					f64 Delta_ionise_k = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					f64 Delta_rec_k = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					n_kplus1 = n_k + Delta_ionise_k - Delta_rec_k; // Delta_rec is amount recombining.
					f64 T_image_k = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;
					
					if (TEST_IONIZE) printf("iVertex %d bTest was true; T_image_k %1.9E T_k %1.9E T_image1 %1.9E T1 %1.9E\n",
						iVertex, T_image_k, T_k.Te, T_image1, T1);
					// Y

					if (((T_image_k > T_k.Te) && (T_image1 < T1))
						||
						((T_image_k < T_k.Te) && (T_image1 > T1)))
					{
						// different sign:

						// If it's different sign, either accept it if the move
						// brings us the f(Tk) side of Tk, or set it to 0 
						// if the move would take us the opposite direction from Tk.
						bAccept = true;
						// We have not changed Delta_ionize

						if (((T_image_k > T_k.Te) && (T_image1 < T_k.Te))
							||
							((T_image_k < T_k.Te) && (T_image1 > T_k.Te)))
						{
							Delta_ionise = 0.0;
							Delta_rec = 0.0;
							// A better solution may exist.
							
							if (TEST_IONIZE) printf("iVertex %d setted Delta_ionise to 0\n", iVertex);
							// Z
						};
					} else {
						// If it's the same sign, pass to the following code
						// which asks if it is overshooting.
						// We have not changed T2 or T1 or T_image1
						
						if (TEST_IONIZE) printf("iVertex %d pass to secant loop\n", iVertex);
						// Z
					};
				};

				// putative Fwd Euler move neither had T < 0 nor was tiny.
				// Overshooting test for Fwd Euler:

				if (TEST_IONIZE) printf("iVertex %d overshooting test for Fwd move\n", iVertex);
				// Z
				bAccept = false;

				Gamma_ion = fac_uplift*GetIonizationRates(T2, w, &Gamma_rec);
				n_kplus2 = n_kplus1 + h_use*n_n_kplus1*n_kplus1*Gamma_ion -
					h_use*n_kplus1*n_kplus1*n_kplus1*Gamma_rec;
				// Comparator:
				Tkplus2minus1 = (n_kplus1 / n_kplus2 - 1.0)*T2 +
					coeff_on_ionizing*h_use*n_n_kplus1*Gamma_ion +
					TWOTHIRDS*13.6*kB*h_use*n_kplus1*n_kplus1*Gamma_rec;
				// compare this inflated difference of Tnext from T2 with the difference T2-T1:
				if (((Tkplus2minus1 > 0.0) && (sign_k > 0))
					||
					((Tkplus2minus1 < 0.0) && (sign_k < 0)))
				{
					bAccept = true; // Accept forward Euler move; Delta was set.
					// Or on main step, accept "T_k+1/2" move

					if (TEST_IONIZE) printf("iVertex %d comparator same sign; accept\n",iVertex);
					// Z
				} else {
					bAccept = (SAFETY_FACTOR*fabs(Tkplus2minus1) < fabs(T2-T_k.Te)+LEEWAY);
					// Accept only if the comparator is smaller in magnitude.

					if (TEST_IONIZE) printf("iVertex %d comparison %1.10E vs %1.10E\n", iVertex,
						SAFETY_FACTOR*fabs(Tkplus2minus1) , fabs(T2 - T_k.Te) + LEEWAY);
					// Z
				};
				// got rid of probs by commenting from here
				if (bAccept == false) {
					// construct f_k image of T2 for use in secant:
					f64 hnGamma_ion = h_use*Gamma_ion*n_k;
					f64 hnnGamma_rec = h_use*Gamma_rec*n_k*n_k;
					Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
						((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
					n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
					T_image2 = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;
				}
							
			}; // whether small move
			
		}; // whether fwd T < 0
		
		// Main loop:				

		
		int ctr = 0;

		while ((bAccept == false) && (ctr < 100)){
			++ctr; 
			// max 100 iterations but I don't see any reason hard limit will be needed.
			
			// Calculate secant from existing points:

			// We have T1, T2 coming in
			// T1 is the one closer to T_k, T2 is the other side of bwd T
			
			f64 T_sec = (T2*T_image1 - T1*T_image2) / (T2 - T_image2 - T1 + T_image1);
			// This approximates a backward step.
			// Try 'midpoint': we want to be on fwd side of bwd step
			f64 T_est = 0.5*(T_sec + T1);

			// Calculate image starting from T_k and using T_est
			
			//if (TEST_IONIZE) {
				//Gamma_ion = GetIonizationRatesDebug(T_est, w, &Gamma_rec);
			//} else {
				Gamma_ion = fac_uplift*GetIonizationRates(T_est, w, &Gamma_rec);
			//};

			Delta_ionise = (n_n_k*hn*Gamma_ion + (n_n_k + n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			Delta_rec = (n_k*hnn*Gamma_rec + (n_k + n_n_k)*hnn*Gamma_rec*hn*Gamma_ion) /
				((1.0 + hn*Gamma_ion)*(1.0 + hnn*Gamma_rec) - hnn*Gamma_rec*hn*Gamma_ion);
			// *** Set for the move we are testing ***
			n_kplus1 = n_k + Delta_ionise - Delta_rec; // Delta_rec is amount recombining.
			n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

			f64 T_image_est = (n_k*T_k.Te + coeff_on_ionizing*Delta_ionise + TWOTHIRDS*13.6*kB*Delta_rec) / n_kplus1;

			if (TEST_IONIZE) printf("iVertex %d secant loop T1 %1.9E T2 %1.9E T_est %1.9E T_image_est %1.9E Gamma_ion %1.9E Gamma_rec %1.9E\n",
				iVertex, T1, T2, T_est, T_image_est, Gamma_ion, Gamma_rec);
			// A - worked with

			bAccept = false;
			if ((T_image_est < 0.0) && (sign_k < 0)) // If T goes negative it does count as overshooting, supposing original dT was decreasing.
			{
				// Can only get here if non-monotonic?
				// overshooting:
				T1 = T_est;
				T_image1 = T_image_est;				
				bAccept = false;

			} else {

				// between bwd & fwd: T_image_est - T_est between 0 and Torigmove.
				if (
					((sign_k > 0) && (T_image_est - T_est < 0.0)) 
					|| // going wrong way starting from T_k: therefore beyond bwd
					((sign_k < 0) && (T_image_est - T_est > 0.0))
					)
				{
					T2 = T_est;
					T_image2 = T_image_est;
					bAccept = false;

					if (TEST_IONIZE) printf("iVertex %d : T_est beyond bwd\n", iVertex);
					// A - works with

					// This is beyond bwd but do we need to check that we are not exceeding a fwd.
					// .. If we got here then the fwd step is overshooting so that greater move would presumably be overshooting also.

					// We now defined sign_k based on the requested T's move
					// Therefore can we say that 
					// 1. T2 is on the opposite side of T_use from T_k
					// 2. Moving back towards T_use will achieve the same sign as T_use
					// ... it is possible that we were given T_use the other side of bwd so then 
					// we are stuck? can it be the other side of bwd, overshoot T_k away from itself ---
					// we need to address separate cases.


				} else {
					// Test overshooting:
					bAccept = false;

					if (TEST_IONIZE) printf("iVertex %d : T_est overshooting test\n", iVertex);
					// A - works with

					// T_image_est is > 0 if we got here.
					Gamma_ion = fac_uplift*GetIonizationRates(T_image_est, w, &Gamma_rec);
					// Comparator:
					n_kplus2 = n_kplus1 + h_use*n_n_kplus1*n_kplus1*Gamma_ion -
						h_use*n_kplus1*n_kplus1*n_kplus1*Gamma_rec;
					// Comparator:
					Tkplus2minus1 = (n_kplus1 / n_kplus2 - 1.0)*T2 +
						coeff_on_ionizing*h_use*n_n_kplus1*Gamma_ion +
						TWOTHIRDS*13.6*kB*h_use*n_kplus1*n_kplus1*Gamma_rec;
					// compare this inflated difference of Tnext from T2 with the difference T2-T1:
					if (((Tkplus2minus1 > 0.0) && (T2-T_k.Te > 0.0))
						||
						((Tkplus2minus1 < 0.0) && (T2-T_k.Te < 0.0)))
					{
						bAccept = true; // same sign onward => not overshooting eqm

						if (TEST_IONIZE) printf("iVertex %d comparator same sign\n", iVertex);
						// A
					} else {
						bAccept = (SAFETY_FACTOR*fabs(Tkplus2minus1) < fabs(T2-T_k.Te)+LEEWAY);

						if (TEST_IONIZE) printf("iVertex %d comparison %1.10E vs %1.10E\n",
							SAFETY_FACTOR*fabs(Tkplus2minus1), fabs(T2 - T_k.Te) + LEEWAY);
							
						// A

						// Accept only if the comparator is smaller in magnitude.
						if (bAccept == false) {
							T1 = T_est; // still overshooting
							T_image1 = T_image_est;
						};
					};
				};
			};
		}; // end while: bAccept == true
		
		// Now calculate what to do, given this move:
		// ==========================================

		// Aim in the above:
		// Delta_ionise and delta_rec should be already set.

		// This bit is not perfect. It breaks down the changes into steps.

		// 0. What is ROC of N:

		if (bZero_out) {
			// do nothing to MAR_elec, ourrates etc
		} else {
			f64 dNdt_ionise = AreaMajor*Delta_ionise / h_use;
			f64 dNdt_recombine = AreaMajor*Delta_rec / h_use;

			ourrates.N += dNdt_ionise - dNdt_recombine;
			ourrates.Nn += dNdt_recombine - dNdt_ionise;
			
			if (TEST_IONIZE) printf("Delta_ionise %1.10E Delta_rec %1.10E ourrates.N %1.10E \n",
				Delta_ionise, Delta_rec, ourrates.N);
				

			// Store existing energy density:
		//	f64 Energy_k = 1.5*(n_k*(T_k.Te + T_k.Ti) + n_n_k*T_k.Tn) +
		//		0.5*((m_e + m_i)*n_k*(v.vxy.dot(v.vxy)) + m_e*n_k*v.vez*v.vez + m_i*n_k*v.viz*v.viz + m_n*n_n_k*v_n.dot(v_n));


			// 1. Calculate kinetic energy absorption impact on vez, vnz
			// ie Ionization resistance to current

			n_kplus1 = n_k + Delta_ionise - Delta_rec;
			n_n_kplus1 = n_n_k - Delta_ionise + Delta_rec;

			// Absorbed DKE:
			f64 deltaKE = -(2.0*Theta*Kconv / (3.0*n_k*T_k.Te + 2.0*Theta*Kconv))*Delta_ionise*13.6*kB;
			
			f64 safe_argument = (m_e*n_kplus1 + m_n*n_n_kplus1)*
				((n_k*n_n_k / (m_e*n_k + m_n*n_n_k))*(w0z*w0z) + 2.0*deltaKE / (m_e*m_n)) /
				(n_kplus1*n_n_kplus1);
			f64 new_vz_diff;
			if (safe_argument >= 0.0) {
				new_vz_diff = sqrt(safe_argument);
			} else {
				new_vz_diff = 0.0;   // This is just to stop it crashing, the theory of why new_vz_diff sqrt should have +ve argument, I have not revisited.
			};

			// I guess it can be -ve.
			
			// Choose new_vz_diff to have same sign as w0z = diff_k:
			if (w0z < 0.0) new_vz_diff = -new_vz_diff;

			f64 delta_vez = m_n*n_n_kplus1*(-w0z + new_vz_diff) /
				(m_n*n_n_kplus1 + m_e*n_kplus1);
			f64 delta_vnz = -m_e*n_kplus1*delta_vez / (m_n*n_n_kplus1);

			
			if (TEST_IONIZE) printf("deltaKE %1.10E v.vez %1.10E w0z %1.9E new_vz_diff %1.9E delta_vez %1.9E\n",
				deltaKE, v.vez, w0z, new_vz_diff, delta_vez);
				


			// Check: w0 = vez-vnz - tick
			// should change to scalar.

			MAR_neut.z += AreaMajor*n_n_kplus1*delta_vnz / h_use;
			MAR_elec.z += AreaMajor*n_kplus1*delta_vez / h_use;

			f64_vec3 ve_kplus1, vi_kplus1, vn_kplus1;
			f64_vec3 v_use;
			v_use.x = v.vxy.x;
			v_use.y = v.vxy.y;
			v_use.z = (m_e*v.vez + m_i*v.viz) / (m_e + m_i);

			// Store alongside: v_k+1 so that we can follow the anticipated change in energy,
			// to create energy balance:
			ve_kplus1.x = v.vxy.x*(n_k / n_kplus1);
			ve_kplus1.y = v.vxy.y*(n_k / n_kplus1);
			ve_kplus1.z = v.vez*(n_k / n_kplus1) + delta_vez; // we need to store v, we could also store nv if we wanted.

			vi_kplus1.x = v.vxy.x*(n_k / n_kplus1);
			vi_kplus1.y = v.vxy.y*(n_k / n_kplus1);
			vi_kplus1.z = v.viz*(n_k / n_kplus1);

			vn_kplus1 = v_n*(n_n_k / n_n_kplus1); // Check in accel routine to be sure this will ever actually happen
			vn_kplus1.z += delta_vnz;
			
			// Does it happen automatically or do we need to include the n_k+1/n_k effect in MAR_ ????
			
			// It does NOT work automatically !!!
			// We have to include the effect here ---- stupid us, don't know why doing it this way.

			// Where used: v0.viz = vie_k.viz + h_use * MAR.z / (n_use.n*AreaMinor);
			// and n_use is the target n_kplus1.

		//	MAR_neut += AreaMajor*n_n_kplus1*(v_n*(n_n_k/ n_n_kplus1) - v_n) / h_use;
			MAR_neut += AreaMajor*(v_n*(n_n_k - n_n_kplus1)) / h_use;
			MAR_ion += AreaMajor*(Make3(v.vxy,v.viz)*(n_k - n_kplus1)) / h_use; 
			MAR_elec += AreaMajor*(Make3(v.vxy,v.vez)*(n_k - n_kplus1)) / h_use;
			// diluting v..

// 2. Add the effect of xfers on momenta:

			// Let's think about this clearly:
			// v_k+1 = (1/n_k+1) (n_k v_k + delta_ionize v_n - delta_rec v_use)
			// = v_k + h * MAR / (n_k+1 * Area);

			// MAR(h/area) = (v_k+1 - v_k)(n_k+1)
			// = ( n_k v_k + delta_ionize v_n - delta_rec v_use - v_k n_k+1)

			// We split the change into local minor cells, hopefully correctly.
			
			vn_kplus1 -= (Delta_ionise*v_n - Delta_rec*v_use) / n_n_kplus1;
			// n_k+1 v_k+1 = n_k v_k + Delta_n*v_use => v_k+1 = (n_k/n_k+1) v_k + (Delta_n/n_k+1) v_use
			vi_kplus1 += (Delta_ionise*v_n - Delta_rec*v_use) / n_kplus1;
			ve_kplus1 += (Delta_ionise*v_n - Delta_rec*v_use) / n_kplus1;

			MAR_neut += -dNdt_ionise*v_n + dNdt_recombine*v_use; // area*delta_n/h * v_use
			MAR_ion += dNdt_ionise*v_n - dNdt_recombine*v_use;
			MAR_elec += dNdt_ionise*v_n - dNdt_recombine*v_use;
						
			if (TEST_IONIZE) printf("__ ve_kplus1 %1.9E %1.9E %1.9E vi_plus1 %1.9E %1.9E %1.9E vn_plus1 %1.9E %1.9E %1.9E delta_vez %1.9E\n",
				ve_kplus1.x, ve_kplus1.y, ve_kplus1.z, vi_kplus1.x, vi_kplus1.y, vi_kplus1.z,
				vn_kplus1.x, vn_kplus1.y, vn_kplus1.z, delta_vez);
				
			// . Ionization cooling & recombination heating
			//f64 coeff_on_ionizing = 0.5*T_k.Tn - 2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv);
	//		ourrates.NeTe +=
	//			dNdt_recombine*2.0*13.6*kB / 3.0
	//			- (2.0*T_k.Te*13.6*kB*n_k / (3.0*T_k.Te*n_k + 2.0*Theta*Kconv))*dNdt_ionise;
			// We can drop this: it will be accounted for by the final energy balance.

			// 3. Add to nT for x-fers due to species converting

	//		ourrates.NiTi += 0.5*dNdt_ionise*T_k.Tn;
	//		ourrates.NeTe += 0.5*dNdt_ionise*T_k.Tn;
	//		ourrates.NnTn -= dNdt_ionise*T_k.Tn;  // no longer need this, it is incorporated below

	//		f64 nTe_kplus1 = T_k.Te*(n_k)+0.5*Delta_ionise*T_k.Tn;
	//		f64 nTi_kplus1 = T_k.Ti*(n_k)+0.5*Delta_ionise*T_k.Tn;
	//		f64 n_nTn_kplus1 = T_k.Tn*(n_n_k)-Delta_ionise*T_k.Tn;   

		  // without any change to NeTe it would stay just the same as at timeslice k.

			// 4. Energy balance through Te:
			// Maybe we should rather be seeking OVERALL energy balance where KE_result is from n_k+1, v_k+1
			// and we ensure that we have lost the right amount of energy overall.
			// That is the better way:

	//		f64 KE_result = 0.5*(m_e*n_kplus1*ve_kplus1.dot(ve_kplus1) + m_i*n_kplus1*vi_kplus1.dot(vi_kplus1)
	//			+ m_n*n_n_kplus1*vn_kplus1.dot(vn_kplus1));

	//		if (TEST_IONIZE) printf("n_kplus1 %1.12E ve_kplus1 %1.9E %1.9E %1.9E vi_plus1 %1.9E %1.9E %1.9E vn_plus1 %1.9E %1.9E %1.9E\n",
	//			n_kplus1, ve_kplus1.x, ve_kplus1.y, ve_kplus1.z, vi_kplus1.x, vi_kplus1.y, vi_kplus1.z,
	//			vn_kplus1.x, vn_kplus1.y, vn_kplus1.z);

			f64 Energy_density_k_n = 0.5*m_n*n_n_k*v_n.dot(v_n) + 1.5*n_n_k*T_k.Tn;
			f64 Energy_density_k_i = 0.5*m_i*n_k*(v.vxy.dot(v.vxy) + v.viz*v.viz) + 1.5*n_k*T_k.Ti;
			f64 Energy_density_k_e = 0.5*m_e*n_k*(v.vxy.dot(v.vxy) + v.vez*v.vez) + 1.5*n_k*T_k.Te;

			// The energy density given the change in velocity but zero change in heat:
			f64 Energy_density_kplus1_e = 0.5*m_e*n_kplus1*ve_kplus1.dot(ve_kplus1) + 1.5*T_k.Te*n_k;
			// without any change to NeTe it would stay just the same as at timeslice k.
			f64 Energy_density_kplus1_i = 0.5*m_i*n_kplus1*vi_kplus1.dot(vi_kplus1) + 1.5*T_k.Ti*n_k;
			f64 Energy_density_kplus1_n = 0.5*m_n*n_n_kplus1*vn_kplus1.dot(vn_kplus1) + 1.5*T_k.Tn*n_n_k;
			
			f64 Energy_density_target_n = Energy_density_k_n + 0.5*m_n*Delta_rec*v_use.dot(v_use) + 1.5*Delta_rec*(T_k.Ti + T_k.Te)
															 - 0.5*m_n*Delta_ionise*v_n.dot(v_n) - 1.5*Delta_ionise*T_k.Tn;
			
			f64 Energy_density_target_i = Energy_density_k_i + 0.5*m_i*Delta_ionise*v_n.dot(v_n) + 1.5*(m_i / m_n)*Delta_ionise*T_k.Tn
															 - 0.5*m_i*Delta_rec*v_use.dot(v_use) - 1.5*Delta_rec*T_k.Ti;
			
			f64 Energy_density_target_e = Energy_density_k_e + 0.5*m_e*Delta_ionise*v_n.dot(v_n) + 1.5*(m_e / m_n)*Delta_ionise*T_k.Tn
															 - 0.5*m_e*Delta_rec*v_use.dot(v_use) - 1.5*Delta_rec*T_k.Te
															 - 13.6*kB*(Delta_ionise - Delta_rec);





	//		f64 Energy_density_target = Energy_k - 13.6*kB*(Delta_ionise - Delta_rec);

			// Additional_heat = (KE_k + deltaKE) - KE_result; // usually positive
			// 1*1+3*3 > 2*2 + 2*2  so KE is generally decreasing by friction; KE_result < KE_k+deltaKE
			// KE_result + Added_heat + existing heat = desired total energy = KE_k + heat_k + deltaKE

			// 1.5 nT += Frictional_heating
			// NTe += (2/3) Area Frictional_heating



			if (TEST_IONIZE) printf("AreaMajor %1.9E h_use %1.9E 0.6666 13.6 kB %1.9E \n"
				"Energy_density_kplus1_i %1.12E target_i %1.12E k_i %1.12E \n"
				"vi_k %1.10E %1.10E %1.10E kplus1 %1.10E %1.10E %1.10E\n"

				"0.5*m_i*Delta_ionise*v_n.dot(v_n) %1.9E 1.5*(m_i / m_n)*Delta_ionise*T_k.Tn %1.9E\n"
				"-0.5*m_i*Delta_rec*v_use.dot(v_use) %1.9E -1.5*Delta_rec*T_k.Ti %1.9E\n",
				AreaMajor, h_use, 0.666667*13.6*kB, 
				Energy_density_kplus1_i, Energy_density_target_i, Energy_density_k_i,
				v.vxy.x, v.vxy.y, v.viz, vi_kplus1.x, vi_kplus1.y, vi_kplus1.z,
				0.5*m_i*Delta_ionise*v_n.dot(v_n), 1.5*(m_i / m_n)*Delta_ionise*T_k.Tn,				
				-0.5*m_i*Delta_rec*v_use.dot(v_use) , -1.5*Delta_rec*T_k.Ti
			);


			if (TEST_IONIZE) printf("ourrates.NiTi before: %1.11E \n", ourrates.NiTi);
			
			ourrates.NeTe += 2.0*AreaMajor*
				(Energy_density_target_e - Energy_density_kplus1_e) / (3.0*h_use);
			ourrates.NiTi += 2.0*AreaMajor*
				(Energy_density_target_i - Energy_density_kplus1_i) / (3.0*h_use);
			ourrates.NnTn += 2.0*AreaMajor*
				(Energy_density_target_n - Energy_density_kplus1_n) / (3.0*h_use);

			if (TEST_IONIZE) printf("ourrates.NiTi after: %1.11E Area*n_kplus1 %1.9E \n\n", ourrates.NiTi,
				AreaMajor*n_kplus1);
				
//
//			if ((Energy_density_target_e - Energy_density_kplus1_e) / (n_k*AreaMajor) > 1.0e-8)
//			{
//				printf("Vertex %d vez(k+1) %1.9E vezk %1.9E delta_vez %1.9E\n"
//					"iVertex %d n_k %1.9E N_k %1.9E Te_k %1.9E NeTe %1.9E h*NeTe %1.9E \n"
//					"Ti_k %1.9E h*NiTi %1.9E Tn_k %1.9E h*NnTn %1.9E \n"
//					"Delta_ionise %1.9E rec %1.9E deltaKE %1.9E deltavez %1.9E\n"
//					"Predicted Te %1.12E Theta %1.12E \n"
//					"Energy_k %1.12E w0z %1.9E energy_kplus1 %1.12E energy_target %1.12E \n"
//					"KEk %1.12E Heat_k %1.12E  \n",
//					iVertex, 
//					ve_kplus1.z, v.vez, delta_vez,
//					iVertex, n_k, n_k*AreaMajor, T_k.Te, ourrates.NeTe, h_use*ourrates.NeTe,
//					T_k.Ti, h_use*ourrates.NiTi, T_k.Tn, h_use*ourrates.NnTn,
//					Delta_ionise, Delta_rec, deltaKE, delta_vez,
//					(n_k*AreaMajor*T_k.Te + ourrates.NeTe*h_use) / (n_k*AreaMajor),
//					Theta, Energy_k, w0z, Energy_density_kplus1_e, Energy_density_target_e,
//					0.5*((m_e + m_i)*n_k*(v.vxy.dot(v.vxy)) + m_e*n_k*v.vez*v.vez + m_i*n_k*v.viz*v.viz + m_n*n_n_k*v_n.dot(v_n)),
//					1.5*(n_k*T_k.Te + n_k*T_k.Ti + n_n_k*T_k.Tn));
//				printf("iVertex %d "
//					"ve_kplus1 %1.9E %1.9E %1.9E vi_plus1 %1.9E %1.9E %1.9E vn_plus1 %1.9E %1.9E %1.9E\n"
//					"v_n %1.9E %1.9E %1.9E n_n_k %1.9E  n_n_kplus1 %1.9E n_k %1.9E n_kplus1 %1.9E \n"
//					"Theta %1.10E Kconv %1.10E deltaKE %1.10E ppnK %1.10E full_loss %1.10E\n",
//					iVertex,
//					ve_kplus1.x, ve_kplus1.y, ve_kplus1.z,
//					vi_kplus1.x, vi_kplus1.y, vi_kplus1.z,
//					vn_kplus1.x, vn_kplus1.y, vn_kplus1.z,
//					v_n.x, v_n.y, v_n.z, n_n_k, n_n_kplus1, n_k, n_kplus1,
//					Theta, Kconv, deltaKE,
//					(2.0*Theta*Kconv / (3.0*n_k*T_k.Te + 2.0*Theta*Kconv)),Delta_ionise*13.6*kB
//				);
//			}

			// DEBUG:
			if (TEST_IONIZE) printf("iVertex %d n_k %1.9E n_n_k %1.9E N_k %1.9E Te_k %1.9E NeTe %1.9E \n h*NeTe %1.9E "
				"Ti_k %1.9E h*NiTi %1.9E Tn_k %1.9E h*NnTn %1.9E \n"
				"Delta_ionise %1.9E rec %1.9E \n",
				iVertex, n_k, n_n_k, n_k*AreaMajor, T_k.Te, ourrates.NeTe, h_use*ourrates.NeTe,
				T_k.Ti, h_use*ourrates.NiTi, T_k.Tn, h_use*ourrates.NnTn,
				Delta_ionise, Delta_rec
			);

			// DEBUG:
			if (TEST_IONIZE) //n_k*AreaMajor*T_k.Te + ourrates.NeTe*h_use < 0.0)
				printf("%d Predicted Te %1.9E \n", iVertex, (n_k*AreaMajor*T_k.Te + ourrates.NeTe*h_use) / (n_k*AreaMajor));
				
			// Try to get rid of 77


		}
		memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));
		memcpy(p_MAR_neut + iVertex, &MAR_neut, sizeof(f64_vec3));
		memcpy(p_MAR_ion + iVertex, &MAR_ion, sizeof(f64_vec3));
		memcpy(p_MAR_elec + iVertex, &MAR_elec, sizeof(f64_vec3));
		

// ****************************************************************************************************


		//// f64 TeV = T.Te * one_over_kB; 
		//// We loaded in ourrates.NT which indicates the new heat available so we should include some of that.
		//// The main impact will be from heat conduction; dN/dt due to advection neglected here.
		//f64 TeV = one_over_kB * (T.Te*our_n.n*AreaMajor + h_use*ourrates.NeTe)/
		//	(our_n.n*AreaMajor + h_use*ourrates.N);
		//// Should be very careful here: ourrates.NeTe can soak to neutrals on timescale what? 1e-11?

		//if (TeV < 0.0) {
		//	printf("\n\niVertex %d : ourrates.N %1.14E denominator %1.14E \n"
		//		" AreaMajor %1.14E TeV %1.14E ourrates.NeTe %1.10E h %1.10E \n"
		//		"ourrates.Nn %1.10E n %1.10E n_n %1.10E Te %1.10E Tn %1.10E \n\n",
		//		iVertex, ourrates.N, 
		//		(our_n.n*AreaMajor + h_use*ourrates.N),
		//		AreaMajor, TeV, ourrates.NeTe, h_use,
		//		ourrates.Nn, our_n.n, our_n.n_n, T.Te, T.Tn);
		//	
		//}
		//f64 sqrtT = sqrt(TeV);
		//f64 temp = 1.0e-5*exp(-13.6 / TeV) / (13.6*(6.0*13.6 + TeV)); // = S / T^1/2
		//	// Let h n n_n S be the ionising amount,
		//	// h n S is the proportion of neutrals! Make sure we do not run out!

		////f64 hnS = (h_use*our_n.n*TeV*temp) / (sqrtT + h_use * our_n.n_n*temp*SIXTH*13.6);

		//	// d/dt (sqrtT) = 1/2 dT/dt T^-1/2. 
		//	// dT[eV]/dt = -TWOTHIRDS * 13.6* n_n* sqrtT *temp
		//	// d/dt (sqrtT) = -THIRD*13.6*n_n*temp;

		//// kind of midpoint, see SIXTH not THIRD:
		//f64 Model_of_T_to_half = TeV / (sqrtT + h_use*SIXTH*13.6*our_n.n_n*temp / (1.0 - h_use*(our_n.n_n - our_n.n)*temp*sqrtT));

		//f64 hS = h_use*temp*Model_of_T_to_half;
		//		
		//// NEW:
		//f64 ionise_rate = AreaMajor * our_n.n_n * our_n.n*hS / 
		//					(h_use*(1.0 + hS*(our_n.n-our_n.n_n)));   // dN/dt

		//ourrates.N += ionise_rate;
		//ourrates.Nn += -ionise_rate;


		//// Let nR be the recombining amount, R is the proportion.
		//TeV = T.Te * one_over_kB;
		//f64 Ttothe5point5 = sqrtT * TeV * TeV*TeV * TeV*TeV;
		//f64 hR = h_use * (our_n.n * our_n.n*8.75e-27*TeV) /
		//	(Ttothe5point5 + h_use * 2.25*TWOTHIRDS*13.6*our_n.n*our_n.n*8.75e-27);

		//// T/T^5.5 = T^-4.5
		//// T/(T^5.5+eps) < T^-4.5

		//// For some reason I picked 2.25 = 4.5/2 instead of 5.5/2.
		//// But basically it looks reasonable.

		//// Maybe the additional stuff is an estimate of the change in T[eV]^5.5??
		//// d/dt T^5.5 = 5.5 T^4.5 dT/dt 
		//// dT/dt = TWOTHIRDS * 13.6*( hR / h_use) = TWOTHIRDS * 13.6*( n^2 8.75e-27 T^-4.5) 
		//// d/dt T^5.5 = 5.5 TWOTHIRDS * 13.6*( n^2 8.75e-27 )  

		//f64 recomb_rate = AreaMajor * our_n.n * hR / h_use; // could reasonably again take hR/(1+hR) for n_k+1
		//ourrates.N -= recomb_rate;
		//ourrates.Nn += recomb_rate;

		//if (TEST) printf("%d recomb rate %1.10E ionise_rate %1.10E our_n.n %1.10E nn %1.10E hR %1.10E hS %1.10E\n"
		//	"h_use %1.8E sqrtTeV %1.10E Ttothe5point5 %1.9E Te %1.9E modelThalf %1.9E\n", iVertex,
		//	recomb_rate, ionise_rate, our_n.n, our_n.n_n, hR, hS, h_use, sqrtT, Ttothe5point5, T.Te, Model_of_T_to_half);

		//ourrates.NeTe += -TWOTHIRDS * 13.6*kB*(ionise_rate - recomb_rate) + 0.5*T.Tn*ionise_rate;
		//ourrates.NiTi += 0.5*T.Tn*ionise_rate;
		//ourrates.NnTn += (T.Te + T.Ti)*recomb_rate;
		//if (TEST) {
		//	printf("kernelIonisation %d NeTe %1.12E NiTi %1.12E NnTn %1.12E\n"
		//		"due to I+R : NeTe %1.12E NiTi %1.12E NnTn %1.12E\n"
		//		"d/dtNeTe/N %1.9E d/dtNiTi/N %1.9E d/dtNnTn/Nn %1.9E \n\n",
		//		iVertex, ourrates.NeTe, ourrates.NiTi, ourrates.NnTn,
		//		-TWOTHIRDS * 13.6*kB*(ionise_rate - recomb_rate) + 0.5*T.Tn*ionise_rate,
		//		0.5*T.Tn*ionise_rate,
		//		(T.Te + T.Ti)*recomb_rate,
		//		ourrates.NeTe / (our_n.n*AreaMajor), ourrates.NiTi / (our_n.n*AreaMajor), ourrates.NnTn / (our_n.n_n*AreaMajor));
		//};
		//memcpy(NTadditionrates + iVertex, &ourrates, sizeof(NTrates));
	};
}

__global__ void kernelAdvanceDensityAndTemperature(
	f64 h_use,
	structural * __restrict__ p_info_major,
	nvals * p_n_major,
	T3 * p_T_major,
	NTrates * __restrict__ NTadditionrates,

	// Think we see the mistake here: are these to be major or minor values?
	// Major, right? Check code:

	nvals * p_n_use,
	T3 * p_T_use,
	v4 * __restrict__ p_vie_use,
	f64_vec3 * __restrict__ p_v_n_use,

	f64 * __restrict__ p_div_v_neut,
	f64 * __restrict__ p_div_v,
	f64 * __restrict__ p_Integrated_div_v_overall,
	f64 * __restrict__ p_AreaMajor, // hmm

	nvals * __restrict__ p_n_major_dest,
	T3 * __restrict__ p_T_major_dest
)
{
	// runs for major tile
	// nu would have been a better choice to go in shared as it coexists with the 18 doubles in "LHS","inverted".
	// Important to set 48K L1 for this routine.

	__shared__ nvals n_src_or_use[threadsPerTileMajor];
	__shared__ f64 AreaMajor[threadsPerTileMajor];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // iVertex OF VERTEX
	structural info = p_info_major[iVertex];
	//	if (iVertex == CHOSEN) printf("GPU iVertex %d info.flag %d \n", CHOSEN, info.flag);

	if ((info.flag == DOMAIN_VERTEX)) {

		n_src_or_use[threadIdx.x] = p_n_major[iVertex];  // used throughout so a good candidate to stick in shared mem
		AreaMajor[threadIdx.x] = p_AreaMajor[iVertex]; // ditto

		NTrates newdata;
		{
			NTrates AdditionNT = NTadditionrates[iVertex];
			newdata.N = n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] + h_use * AdditionNT.N;
			newdata.Nn = n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] + h_use * AdditionNT.Nn;
			newdata.NnTn = h_use * AdditionNT.NnTn; // start off without knowing 'factor' so we can ditch AdditionNT
			newdata.NiTi = h_use * AdditionNT.NiTi;
			newdata.NeTe = h_use * AdditionNT.NeTe;

			if (TEST)
				printf("Advance_nT  %d : nsrc %1.12E nn %1.12E *AreaMajor %1.12E %1.12E\n"
					"newdata.Nn %1.12E newdata.Ni %1.12E AreaMajor %1.10E \n"
					"h*additionNiTi %1.12E for e %1.12E for n %1.12E \n"
					"AdditionNT.e %1.10E h_use %1.10E\n"
					, iVertex,
					n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n,
					n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x],
					n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x],
					newdata.Nn, newdata.N, AreaMajor[threadIdx.x],
					newdata.NiTi, newdata.NeTe, newdata.NnTn,
					AdditionNT.NeTe, h_use);
		}

		// So at this vertex, near the insulator, NiTi that comes in is NaN. Is that advection or diffusion?
		// Have to go to bed tonight...

		{
			nvals n_dest;
			f64 Div_v_overall_integrated = p_Integrated_div_v_overall[iVertex];
			n_dest.n = newdata.N / (AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated); // Do have to worry whether advection steps are too frequent.
			n_dest.n_n = newdata.Nn / (AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated); // What could do differently: know ROC area as well as mass flux through walls
			p_n_major_dest[iVertex] = n_dest;

			//		if (iVertex == CHOSEN) printf("GPU %d n_dest.n_n %1.14E  Area_used %1.14E \n\n", iVertex, n_dest.n_n,
			//			(AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated));
		}

		// roughly right ; maybe there are improvements.

		// --------------------------------------------------------------------------------------------
		// Simple way of doing area ratio for exponential growth of T: 
		// (1/(1+h div v)) -- v outward grows the area so must be + here. 

		// Compressive heating:
		// USE 1 iteration of Halley's method for cube root:
		// cu_root Q =~~= x0(x0^3+2Q)/(2x0^3+Q) .. for us x0 = 1, Q is (1+eps)^-2
		// Thus (1+2(1+eps)^-2)/(2+(1+eps)^-2)
		// Multiply through by (1+eps)^2:
		// ((1+eps)^2+2)/(1+2*(1+eps)^2) .. well of course it is
		// eps = h div v

		// Way to get reasonable answer without re-doing equations:
		// Take power -1/3 and multiply once before interspecies and once after.

		f64 factor, factor_neut; // used again at end
		{
			f64 Div_v = p_div_v[iVertex];
			f64 Div_v_n = p_div_v_neut[iVertex];
			factor = (3.0 + h_use * Div_v) /
				(3.0 + 2.0* h_use * Div_v);
			factor_neut = (3.0 + h_use * Div_v_n) /
				(3.0 + 2.0*h_use * Div_v_n);
		}
		// gives (1+ h div v)^(-1/3), roughly

		// Alternate version: 
		// factor = pow(pVertex->AreaCell / pVertDest->AreaCell, 2.0 / 3.0);
		// pVertDest->Ion.heat = pVertex->Ion.heat*factor;
		// but the actual law is with 5/3 
		// Comp htg dT/dt = -2/3 T div v_fluid 
		// factor (1/(1+h div v))^(2/3) --> that's same
		{
			T3 T_src = p_T_major[iVertex];
			newdata.NnTn += n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] * T_src.Tn*factor_neut;
			newdata.NiTi += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Ti*factor;
			newdata.NeTe += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Te*factor;
			//
			if (TEST) {
				printf("\nAdvance_nT %d : n %1.12E Area %1.12E compressfac %1.10E \n"
					"newdate.NiTi %1.12E Ti_k %1.12E newdata.NeTe %1.10E Te_k %1.10E\n",
					iVertex, n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], factor,
					newdata.NiTi, T_src.Ti, newdata.NeTe, T_src.Te);
			}
		}

		f64 nu_ne_MT, nu_en_MT, nu_ni_MT, nu_in_MT, nu_ei; // optimize after
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal, lnLambda, s_in_MT, s_en_MT, s_en_visc;

			n_src_or_use[threadIdx.x] = p_n_use[iVertex];
			T3 T_use = p_T_use[iVertex];

			sqrt_Te = sqrt(T_use.Te); // should be "usedata"
			ionneut_thermal = sqrt(T_use.Ti / m_ion + T_use.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_src_or_use[threadIdx.x].n, T_use.Te);

			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T_use.Ti*one_over_kB,
					&s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T_use.Te*one_over_kB, // call with T in electronVolts
				&s_en_MT,
				&s_en_visc);
			//s_en_MT = Estimate_Ion_Neutral_MT_Cross_section(T_use.Te*one_over_kB);
			//s_en_visc = Estimate_Ion_Neutral_Viscosity_Cross_section(T_use.Te*one_over_kB);

			if (n_src_or_use[threadIdx.x].n_n > ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n) {
				s_en_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				s_in_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			}

			// Need nu_ne etc to be defined:

			nu_ne_MT = s_en_MT * n_src_or_use[threadIdx.x].n * electron_thermal; // have to multiply by n_e for nu_ne_MT
			nu_ni_MT = s_in_MT * n_src_or_use[threadIdx.x].n * ionneut_thermal;
			nu_en_MT = s_en_MT * n_src_or_use[threadIdx.x].n_n*electron_thermal;
			nu_in_MT = s_in_MT * n_src_or_use[threadIdx.x].n_n*ionneut_thermal;

			//	
			//	if (iVertex == CHOSEN) {
			//		printf("nu_en_MT components GPU : %1.8E %1.8E %1.8E \n",
			//			s_en_MT, n_src_or_use[threadIdx.x].n_n, electron_thermal);
			//		f64 T = T_use.Te*one_over_kB;
			//		int j;
			//		printf("T = %1.10E\n", T);
			//		for (j = 0; j < 10; j++)
			//			printf("%d : cross_T_vals_d %1.10E cross_s_vals_MT %1.10E \n",
			//				j, cross_T_vals_d[j], cross_s_vals_MT_ni_d[j]);
			//		int i = 1;
			//		if (T > cross_T_vals_d[5]) {
			//			if (T > cross_T_vals_d[7]) {
			//				if (T > cross_T_vals_d[8])
			//				{
			//					i = 9; // top of interval
			//				}
			//				else {
			//					i = 8;
			//				};
			//			}
			//			else {
			//				if (T > cross_T_vals_d[6]) {
			//					i = 7;
			//				}
			//				else {
			//					i = 6;
			//				};
			//			};
			//		}
			//		else {
			//			if (T > cross_T_vals_d[3]) {
			//				if (T > cross_T_vals_d[4]) {
			//					i = 5;
			//				}
			//				else {
			//					i = 4;
			//				};
			//			}
			//			else {
			//				if (T > cross_T_vals_d[2]) {
			//					i = 3;
			//				}
			//				else {
			//					if (T > cross_T_vals_d[1]) {
			//						i = 2;
			//					}
			//					else {
			//						i = 1;
			//					};
			//				};
			//			};
			//		};
			//		// T lies between i-1,i
			//		printf("i = %d\n\n", i);
			//	}

			nu_ei = nu_eiBarconst * kB_to_3halves*n_src_or_use[threadIdx.x].n*lnLambda /
				(T_use.Te*sqrt_Te);

			//		nu_ie = nu_ei;

			//		nu_eHeart = 1.87*nu_eiBar + data_k.n_n*s_en_visc*electron_thermal;
		}


		// For now doing velocity-independent resistive heating.
		// Because although we have a magnetic correction Upsilon_zz involved, we ignored it
		// since we are also squashing the effect of velocity-dependent collisions on vx and vy (which
		// would produce a current in the plane) and this squashing should create heat, which
		// maybe means it adds up to the velocity-independent amount of heating. 
		{
			f64_vec3 v_n = p_v_n_use[iVertex];
			v4 vie = p_vie_use[iVertex];

			newdata.NeTe += h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_en_MT*m_en*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

				+ AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ei*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz));

			newdata.NiTi += h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_in_MT*M_in*m_n*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			newdata.NnTn += h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ni_MT*M_in*m_i*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			if (TEST)
				printf("%d v_n.z %1.9E vie_use.viz %1.9E vie_use.vez %1.9E \n areamajor %1.8E\n"
					"nu_in %1.10E nu_en %1.8E \n"
					"Frictional htg (NT+=): n i e %1.10E %1.10E %1.10E\n",
					iVertex, v_n.z, vie.viz, vie.vez, AreaMajor[threadIdx.x],
					nu_in_MT, nu_en_MT,
					h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ni_MT*M_in*m_i*(
					(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
						+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
						+ (v_n.z - vie.viz)*(v_n.z - vie.viz))),
					h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_in_MT*M_in*m_n*(
					(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
						+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
						+ (v_n.z - vie.viz)*(v_n.z - vie.viz))),
					h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_en_MT*m_en*(
					(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
						+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
						+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

						+ AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ei*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz))
				);
		}
		f64_tens3 inverted;
		{
			f64_tens3 LHS;
			// x = neutral
			// y = ion
			// z = elec
			// This is for NT
			f64 nu_ie = nu_ei;

			LHS.xx = 1.0 - h_use * (-M_en * nu_ne_MT - M_in * nu_ni_MT);
			LHS.xy = -h_use * (M_in * nu_in_MT);
			LHS.xz = -h_use *(M_en * nu_en_MT);

			LHS.yx = -h_use *  M_in * nu_ni_MT;
			LHS.yy = 1.0 - h_use * (-M_in * nu_in_MT - M_ei * nu_ie);
			LHS.yz = -h_use * M_ei * nu_ei;

			LHS.zx = -h_use * M_en * nu_ne_MT;
			LHS.zy = -h_use * M_ei * nu_ie;
			LHS.zz = 1.0 - h_use * (-M_en * nu_en_MT - M_ei * nu_ei);

			// some indices appear reversed because NT not T.

			if (TEST) {
				printf("%d LHS | \n %1.14E %1.14E %1.14E |\n %1.14E %1.14E %1.14E |  \n %1.14E %1.14E %1.14E | \n",
					iVertex, LHS.xx, LHS.xy, LHS.xz,
					LHS.yx, LHS.yy, LHS.yz,
					LHS.zx, LHS.zy, LHS.zz);
				printf("GPU %d : NnTn %1.14E NeTe %1.14E nu_en_MT %1.12E \n", iVertex, newdata.NnTn, newdata.NeTe, nu_en_MT);
			}
			LHS.Inverse(inverted);
		}

		f64_vec3 RHS;
		f64 nu_ie = nu_ei;
		RHS.x = newdata.NnTn - h_use * (nu_ni_MT*M_in + nu_ne_MT * M_en)*newdata.NnTn
			+ h_use * nu_in_MT*M_in*newdata.NiTi + h_use * nu_en_MT*M_en*newdata.NeTe;

		RHS.y = newdata.NiTi - h_use * (nu_in_MT*M_in + nu_ie * M_ei)*newdata.NiTi
			+ h_use * nu_ni_MT*M_in*newdata.NnTn + h_use * nu_ei*M_ei*newdata.NeTe;

		RHS.z = newdata.NeTe - h_use * (nu_en_MT*M_en + nu_ei * M_ei)*newdata.NeTe
			+ h_use * nu_ie*M_ei*newdata.NiTi + h_use * nu_ne_MT*M_en*newdata.NnTn;

		f64_vec3 NT;
		NT = inverted * RHS;
		newdata.NnTn = NT.x;
		newdata.NiTi = NT.y;
		newdata.NeTe = NT.z;

		T3 T_dest;
		T_dest.Tn = newdata.NnTn* factor_neut / newdata.Nn;
		T_dest.Ti = newdata.NiTi* factor / newdata.N;
		T_dest.Te = newdata.NeTe* factor / newdata.N;

		if (TEST) {
			printf("\ninverted %d | RHS \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n"
				"NnTn %1.14E NiTi %1.14E NeTe %1.14E \n"
				"Tn Ti Te %1.14E %1.14E %1.14E\n",
				iVertex,
				inverted.xx, inverted.xy, inverted.xz, RHS.x,
				inverted.yx, inverted.yy, inverted.yz, RHS.y,
				inverted.zx, inverted.zy, inverted.zz, RHS.z,
				newdata.NnTn, newdata.NiTi, newdata.NeTe, T_dest.Tn, T_dest.Ti, T_dest.Te);
		} // This came out with a value.

		if (T_dest.Te != T_dest.Te) {
			printf("Advance_n_T %d : Te NaN factor %1.8E newdata.N %1.10E flag %d \n"
				"n %1.10E Area %1.10E hd/dtNT %1.10E\n",
				iVertex, factor, newdata.N, info.flag,
				n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], h_use * NTadditionrates[iVertex].N);
		}

		p_T_major_dest[iVertex] = T_dest;

	}
	else {
		// nothing to do ??
		if (info.flag == OUTERMOST) {
			p_n_major_dest[iVertex] = p_n_major[iVertex];
			p_T_major_dest[iVertex] = p_T_major[iVertex];
		}
		else {
			memset(p_n_major_dest + iVertex, 0, sizeof(nvals));
			memset(p_T_major_dest + iVertex, 0, sizeof(T3));
		};
	};
}

__global__ void kernelAdvanceDensityAndTemperature_nosoak_etc(
	f64 h_use,
	structural * __restrict__ p_info_major,
	nvals * p_n_major,
	T3 * p_T_major,
	NTrates * __restrict__ NTadditionrates,

	nvals * p_n_use,
	T3 * p_T_use,
	v4 * __restrict__ p_vie_use,
	f64_vec3 * __restrict__ p_v_n_use,

	f64 * __restrict__ p_div_v_neut,
	f64 * __restrict__ p_div_v,
	f64 * __restrict__ p_Integrated_div_v_overall,
	f64 * __restrict__ p_AreaMajor, // hmm

	nvals * __restrict__ p_n_major_dest,
	T3 * __restrict__ p_T_major_dest
)
{
	// runs for major tile
	// nu would have been a better choice to go in shared as it coexists with the 18 doubles in "LHS","inverted".
	// Important to set 48K L1 for this routine.

	__shared__ nvals n_src_or_use[threadsPerTileMajor];
	__shared__ f64 AreaMajor[threadsPerTileMajor];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // iVertex OF VERTEX
	structural info = p_info_major[iVertex];
	//	if (iVertex == CHOSEN) printf("GPU iVertex %d info.flag %d \n", CHOSEN, info.flag);

	if ((info.flag == DOMAIN_VERTEX)) {

		n_src_or_use[threadIdx.x] = p_n_major[iVertex];  // used throughout so a good candidate to stick in shared mem
		AreaMajor[threadIdx.x] = p_AreaMajor[iVertex]; // ditto

		NTrates newdata;
		{
			NTrates AdditionNT = NTadditionrates[iVertex];
			newdata.N = n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] + h_use * AdditionNT.N;
			newdata.Nn = n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] + h_use * AdditionNT.Nn;
			newdata.NnTn = h_use * AdditionNT.NnTn; // start off without knowing 'factor' so we can ditch AdditionNT
			newdata.NiTi = h_use * AdditionNT.NiTi;
			newdata.NeTe = h_use * AdditionNT.NeTe;

			if (TEST1)
				printf("Advance_nT NOSOAK %d : nsrc %1.12E nn %1.12E *AreaMajor %1.12E %1.12E\n"
					"newdata.Nn %1.12E newdata.Ni %1.12E AreaMajor %1.14E h_use %1.10E AdditionNT N Nn %1.10E %1.10E\n"
					, iVertex,
					n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n,
					n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x],
					n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x],
					newdata.Nn, newdata.N, AreaMajor[threadIdx.x], h_use,
					AdditionNT.N, AdditionNT.Nn);
		
			nvals n_dest;
			f64 Div_v_overall_integrated = p_Integrated_div_v_overall[iVertex];
			n_dest.n = newdata.N / (AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated); // Do have to worry whether advection steps are too frequent.
			n_dest.n_n = newdata.Nn / (AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated); // What could do differently: know ROC area as well as mass flux through walls
			p_n_major_dest[iVertex] = n_dest;

			if (iVertex == VERTCHOSEN) printf("\n %d n_dest.n_n %1.14E  Area_used %1.14E Div_v_overall_integ %1.13E\n\n", 
				iVertex, n_dest.n_n,
				(AreaMajor[threadIdx.x] + h_use*Div_v_overall_integrated),
				Div_v_overall_integrated);

		}

		// roughly right ; maybe there are improvements.

		// --------------------------------------------------------------------------------------------
		// Simple way of doing area ratio for exponential growth of T: 
		// (1/(1+h div v)) -- v outward grows the area so must be + here. 

		// Compressive heating:
		// USE 1 iteration of Halley's method for cube root:
		// cu_root Q =~~= x0(x0^3+2Q)/(2x0^3+Q) .. for us x0 = 1, Q is (1+eps)^-2
		// Thus (1+2(1+eps)^-2)/(2+(1+eps)^-2)
		// Multiply through by (1+eps)^2:
		// ((1+eps)^2+2)/(1+2*(1+eps)^2) .. well of course it is
		// eps = h div v

		// Way to get reasonable answer without re-doing equations:
		// Take power -1/3 and multiply once before interspecies and once after.

		f64 factor, factor_neut; // used again at end
		{
			f64 Div_v = p_div_v[iVertex];
			f64 Div_v_n = p_div_v_neut[iVertex];
			factor = (3.0 + h_use * Div_v) /
				(3.0 + 2.0* h_use * Div_v);
			factor_neut = (3.0 + h_use * Div_v_n) /
				(3.0 + 2.0*h_use * Div_v_n);
		}
		// gives (1+ h div v)^(-1/3), roughly

		// Alternate version: 
		// factor = pow(pVertex->AreaCell / pVertDest->AreaCell, 2.0 / 3.0);
		// pVertDest->Ion.heat = pVertex->Ion.heat*factor;
		// but the actual law is with 5/3 
		// Comp htg dT/dt = -2/3 T div v_fluid 
		// factor (1/(1+h div v))^(2/3) --> that's same
		{
			T3 T_src = p_T_major[iVertex];
			newdata.NnTn += n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] * T_src.Tn*factor_neut;
			newdata.NiTi += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Ti*factor;
			newdata.NeTe += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Te*factor;
			//
			if (TEST) {
				printf("\nAdvance_nT NOSOAK %d : n %1.12E Area %1.12E compressfac %1.10E \n"
					"newdata.NiTi (the new Ni Ti) %1.12E Ti_k %1.12E newdata.NeTe %1.10E Te_k %1.10E\n"
					"newdata.NnTn (the new Nn Tn) %1.12E Tn_k %1.12E \n",
					iVertex, n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], factor,
					newdata.NiTi, T_src.Ti, newdata.NeTe, T_src.Te,
					newdata.NnTn, T_src.Tn);
			}
		}		
		T3 T_dest;
		T_dest.Tn = newdata.NnTn* factor_neut / newdata.Nn;
		T_dest.Ti = newdata.NiTi* factor / newdata.N;
		T_dest.Te = newdata.NeTe* factor / newdata.N;
		if (TEST) {
			printf("\nAdvance_nT NOSOAK %d : newdata.N %1.9E T_dest.Ti %1.10E Nn %1.9E T_dest.Tn %1.10E\n",
				iVertex, newdata.N, T_dest.Ti, newdata.Nn, T_dest.Tn);
		}
		if (T_dest.Te != T_dest.Te) {
			printf("Advance_n_T %d : Te NaN factor %1.8E newdata.N %1.10E flag %d \n"
				"n %1.10E Area %1.10E hd/dtNT %1.10E\n",
				iVertex, factor, newdata.N, info.flag,
				n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], h_use * NTadditionrates[iVertex].N);
		}
		p_T_major_dest[iVertex] = T_dest;
	}
	else {
		// nothing to do ??
		if (info.flag == OUTERMOST) {
			p_n_major_dest[iVertex] = p_n_major[iVertex];
			p_T_major_dest[iVertex] = p_T_major[iVertex];
		}
		else {
			memset(p_n_major_dest + iVertex, 0, sizeof(nvals));
			memset(p_T_major_dest + iVertex, 0, sizeof(T3));
		};
	};
}


__global__ void kernelAdvanceDensityAndTemperature_noadvectioncompression(
	f64 h_use,
	structural * __restrict__ p_info_major,
	nvals * p_n_major,
	T3 * p_T_major,
	NTrates * __restrict__ NTadditionrates,
	nvals * p_n_use,
	T3 * p_T_use,
	v4 * __restrict__ p_vie_use,
	f64_vec3 * __restrict__ p_v_n_use, 
	f64 * __restrict__ p_AreaMajor, 
	nvals * __restrict__ p_n_major_dest,
	T3 * __restrict__ p_T_major_dest,
	f64_vec3 * __restrict__ p_B_major
)
{
	// runs for major tile
	// nu would have been a better choice to go in shared as it coexists with the 18 doubles in "LHS","inverted".
	// Important to set 48K L1 for this routine.

	__shared__ nvals n_src_or_use[threadsPerTileMajor];
	__shared__ f64 AreaMajor[threadsPerTileMajor];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // iVertex OF VERTEX
	structural info = p_info_major[iVertex];
//	if (iVertex == CHOSEN) printf("GPU iVertex %d info.flag %d \n", CHOSEN, info.flag);

	if ((info.flag == DOMAIN_VERTEX)) {

		n_src_or_use[threadIdx.x] = p_n_major[iVertex];  // used throughout so a good candidate to stick in shared mem
		AreaMajor[threadIdx.x] = p_AreaMajor[iVertex]; // ditto

		NTrates newdata;
		{
		NTrates AdditionNT = NTadditionrates[iVertex];
		newdata.N = n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] + h_use * AdditionNT.N;
		newdata.Nn = n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] + h_use * AdditionNT.Nn;
		newdata.NnTn = h_use * AdditionNT.NnTn; // start off without knowing 'factor' so we can ditch AdditionNT
		newdata.NiTi = h_use * AdditionNT.NiTi;
		newdata.NeTe = h_use * AdditionNT.NeTe;


		nvals n_dest;
		n_dest.n = newdata.N / (AreaMajor[threadIdx.x]);
		n_dest.n_n = newdata.Nn / (AreaMajor[threadIdx.x]);
		p_n_major_dest[iVertex] = n_dest;

		if (TEST)
			printf("Bdvance_nT  %d : nsrc %1.13E nn %1.13E *AreaMajor %1.13E %1.13E\n"
				"newdata.Nn %1.12E newdata.Ni %1.12E AreaMajor %1.14E n_n_k+1 %1.14E \n"
				"h*additionNT.N %1.14E h*additionNT.Nn %1.14E h %1.14E h*addNT.NeTe %1.14E\n"


				, iVertex,
				n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n,

				n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x],
				n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x],

				newdata.Nn, newdata.N, AreaMajor[threadIdx.x], n_dest.n_n,
				h_use*AdditionNT.N, h_use*AdditionNT.Nn, h_use,
				h_use*AdditionNT.NeTe);
		
		}

		f64 factor, factor_neut; // used again at end
		{
			T3 T_src = p_T_major[iVertex];
			newdata.NnTn += n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] * T_src.Tn;
			newdata.NiTi += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Ti;
			newdata.NeTe += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Te;
			
			//
			if (TEST_T) {
				printf("\nCdvance_nT %d : n %1.12E Area %1.12E compressfac %1.10E \n"
					"newdata.NiTi %1.12E Ti_k %1.12E newdata.NeTe %1.10E Te_k %1.10E\n"
					"newdata.NnTn %1.12E Tn_k %1.12E \n"
					, 
					iVertex, n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], factor,
					newdata.NiTi,T_src.Ti, newdata.NeTe, T_src.Te,
					newdata.NnTn, T_src.Tn);
			}
		}
		
		f64 nu_ne_MT, nu_en_MT, nu_ni_MT, nu_in_MT, nu_ei_effective; // optimize after
		f64 nu_eiBar;
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal, lnLambda, s_in_MT, s_en_MT, s_en_visc;

			n_src_or_use[threadIdx.x] = p_n_use[iVertex];
			T3 T_use = p_T_use[iVertex];

			sqrt_Te = sqrt(T_use.Te); // should be "usedata"
			ionneut_thermal = sqrt(T_use.Ti / m_ion + T_use.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_src_or_use[threadIdx.x].n, T_use.Te);
			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T_use.Ti*one_over_kB,
					&s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T_use.Te*one_over_kB, // call with T in electronVolts
				&s_en_MT,
				&s_en_visc);
			//s_en_MT = Estimate_Ion_Neutral_MT_Cross_section(T_use.Te*one_over_kB);
			//s_en_visc = Estimate_Ion_Neutral_Viscosity_Cross_section(T_use.Te*one_over_kB);
			
			if (n_src_or_use[threadIdx.x].n_n > ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n) {
				s_en_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				s_in_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			}
			// ARTIFICIAL CHANGE TO STOP HAVING TO WORRY ABOUT SILLY VALUES IN AREAS THAT DON'T MATTER MUCH :
			s_en_MT *= ArtificialUpliftFactor(n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n);
			s_in_MT *= ArtificialUpliftFactor(n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n); // returns factor 1.0 if n+nn > 1.0e14.
			// Send heat into neutrals if there's not much stuff here total.

			// Need nu_ne etc to be defined:

			nu_ne_MT = s_en_MT * n_src_or_use[threadIdx.x].n * electron_thermal; // have to multiply by n_e for nu_ne_MT
			nu_ni_MT = s_in_MT * n_src_or_use[threadIdx.x].n * ionneut_thermal;
			nu_en_MT = s_en_MT * n_src_or_use[threadIdx.x].n_n*electron_thermal;
			nu_in_MT = s_in_MT * n_src_or_use[threadIdx.x].n_n*ionneut_thermal;

			nu_eiBar = nu_eiBarconst * kB_to_3halves*max(MINIMUM_NU_EI_DENSITY,n_src_or_use[threadIdx.x].n)*lnLambda / (T_use.Te*sqrt_Te);
			f64 nu_eHeart = 1.87*nu_eiBar + n_src_or_use[threadIdx.x].n_n*s_en_visc*electron_thermal;

			f64_vec3 omega = p_B_major[iVertex] * qovermc;
			// Confusing, why does this say that? We used visc en in nu_eHeart, explanation?

			f64 nu_ei_effective = nu_eiBar * (1.0 - 0.9*nu_eiBar*
				(nu_eHeart*nu_eHeart + omega.z*omega.z) /
				(nu_eHeart*(nu_eHeart*nu_eHeart + omega.dot(omega))));

			if (TEST) printf("%d nu_eiBar: %1.10E n %1.10E lnLambda %1.10E T_use %1.10E \n"
				"nu_eHeart %1.10E omega %1.8E %1.8E %1.8E qovermc %1.8E nu_eiBar/nu_eHeart %1.8E \n"
				"nunuomegaomegafac %1.9E ratio %1.9E  1.0-0.9* = %1.9E nu_ei_effective %1.9E\n",
				iVertex, nu_eiBar, n_src_or_use[threadIdx.x].n, lnLambda, T_use.Te,
				nu_eHeart, omega.x, omega.y, omega.z, qovermc,
				nu_eiBar / nu_eHeart,
				(nu_eHeart*nu_eHeart + omega.z*omega.z) / (nu_eHeart*nu_eHeart + omega.dot(omega)),
				nu_eiBar*
				(nu_eHeart*nu_eHeart + omega.z*omega.z) /
				(nu_eHeart*(nu_eHeart*nu_eHeart + omega.dot(omega))),
				(1.0 - 0.9*nu_eiBar*
				(nu_eHeart*nu_eHeart + omega.z*omega.z) /
					(nu_eHeart*(nu_eHeart*nu_eHeart + omega.dot(omega)))),
				nu_ei_effective
			);
			//		nu_ie = nu_ei;
			//		nu_eHeart = 1.87*nu_eiBar + data_k.n_n*s_en_visc*electron_thermal;
		}
		
		// For now doing velocity-independent resistive heating.
		// Because although we have a magnetic correction Upsilon_zz involved, we ignored it
		// since we are also squashing the effect of velocity-dependent collisions on vx and vy (which
		// would produce a current in the plane) and this squashing should create heat, which
		// maybe means it adds up to the velocity-independent amount of heating. 
		{
			f64_vec3 v_n = p_v_n_use[iVertex];
			v4 vie = p_vie_use[iVertex];
			
			newdata.NeTe += h_use*(AreaMajor[threadIdx.x]*n_src_or_use[threadIdx.x].n * TWOTHIRDS*nu_en_MT*m_en*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

				+ AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n* TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz));
			
			// I see that I did resistive heating for nu_ei but did something much more complicated in the acceleration routine.
			// That isn't quite right then.
			
			newdata.NiTi += h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n * TWOTHIRDS*nu_in_MT*M_in*m_n*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			newdata.NnTn += h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n_n * TWOTHIRDS*nu_ni_MT*M_in*m_i*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			if (TEST) {
		
				printf(
					"%d v_n.z %1.9E vie_use.viz %1.9E vie_use.vez %1.9E Frictional htg (NT+=):e %1.10E\n"
					"elec e-n z htg: %1.10E i-e z htg: %1.10E \n",
					iVertex, v_n.z, vie.viz, vie.vez,
					h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n* TWOTHIRDS*nu_en_MT*m_en*(
					(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
						+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
						+ (v_n.z - vie.vez)*(v_n.z - vie.vez))
						+ AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n*TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz)),
					h_use*AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n*TWOTHIRDS*nu_en_MT*m_en*(v_n.z - vie.vez)*(v_n.z - vie.vez),
					h_use*AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n*TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz)
				);
			};
	
		}
		f64_tens3 inverted;
		{
			f64_tens3 LHS;
			// x = neutral
			// y = ion
			// z = elec
			// This is for NT
			f64 nu_ie = nu_eiBar;
	
			// gonna have to change to Backward Euler :/
			// 6th Nov 2019 : add 2* so that it all goes here.

			LHS.xx = 1.0 - 2.0*h_use * (-M_en * nu_ne_MT - M_in * nu_ni_MT);
			LHS.xy = -2.0*h_use * (M_in * nu_in_MT);
			LHS.xz = -2.0*h_use *(M_en * nu_en_MT);

			LHS.yx = -2.0*h_use *  M_in * nu_ni_MT;
			LHS.yy = 1.0 - 2.0*h_use * (-M_in * nu_in_MT - M_ei * nu_ie);
			LHS.yz = -2.0*h_use * M_ei * nu_eiBar;

			LHS.zx = -2.0*h_use * M_en * nu_ne_MT;
			LHS.zy = -2.0*h_use * M_ei * nu_ie; 
			LHS.zz = 1.0 - 2.0*h_use * (-M_en * nu_en_MT - M_ei * nu_eiBar);
			
			if (TEST) {
				printf("%d LHS | \n %1.14E %1.14E %1.14E |\n %1.14E %1.14E %1.14E |  \n %1.14E %1.14E %1.14E | \n"
					,
					iVertex, LHS.xx, LHS.xy, LHS.xz, 
					LHS.yx, LHS.yy, LHS.yz, 
					LHS.zx, LHS.zy, LHS.zz);				
			} 
			LHS.Inverse(inverted);
		}

		f64_vec3 RHS;
		f64 nu_ie = nu_eiBar;

		// gonna have to change to Backward Euler :/

		//RHS.x = newdata.NnTn - h_use * (nu_ni_MT*M_in + nu_ne_MT * M_en)*newdata.NnTn
		//	+ h_use * nu_in_MT*M_in*newdata.NiTi + h_use * nu_en_MT*M_en*newdata.NeTe;

		//RHS.y = newdata.NiTi - h_use * (nu_in_MT*M_in + nu_ie * M_ei)*newdata.NiTi
		//	+ h_use * nu_ni_MT*M_in*newdata.NnTn + h_use * nu_ei*M_ei*newdata.NeTe;

		//RHS.z = newdata.NeTe - h_use * (nu_en_MT*M_en + nu_ei * M_ei)*newdata.NeTe
		//	+ h_use * nu_ie*M_ei*newdata.NiTi + h_use * nu_ne_MT*M_en*newdata.NnTn;
		
		RHS.x = newdata.NnTn;
		RHS.y = newdata.NiTi;
		RHS.z = newdata.NeTe;

		f64_vec3 NT;
		NT = inverted * RHS;
		newdata.NnTn = NT.x;
		newdata.NiTi = NT.y;
		newdata.NeTe = NT.z;

		T3 T_dest;
		T_dest.Tn = newdata.NnTn / newdata.Nn;
		T_dest.Ti = newdata.NiTi / newdata.N;
		T_dest.Te = newdata.NeTe/ newdata.N;

		if (TEST) {
			printf("\ninverted %d | RHS \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n"
				" NnTn %1.14E NiTi %1.14E NeTe %1.14E \n"
				"Tn Ti Te %1.14E %1.14E %1.14E\n\n"
				,
				iVertex, inverted.xx, inverted.xy, inverted.xz, RHS.x, 
				inverted.yx, inverted.yy, inverted.yz, RHS.y, 
				inverted.zx, inverted.zy, inverted.zz, RHS.z,
				newdata.NnTn, newdata.NiTi, newdata.NeTe,
				T_dest.Tn, T_dest.Ti, T_dest.Te);
		} // This came out with a value.
		
	//	if (T_dest.Te != T_dest.Te) {
	//		printf("Advance_n_T %d : Te NaN factor %1.8E newdata.N %1.10E flag %d \n"
	//			"n %1.10E Area %1.10E hd/dtNT %1.10E\n",
	//			iVertex, factor, newdata.N, info.flag,
	//			n_src_or_use[threadIdx.x].n,AreaMajor[threadIdx.x] , h_use * NTadditionrates[iVertex].N);
	//	}

		p_T_major_dest[iVertex] = T_dest;

	} else {
		// nothing to do ??
		if (info.flag == OUTERMOST) {
			p_n_major_dest[iVertex] = p_n_major[iVertex];
			p_T_major_dest[iVertex] = p_T_major[iVertex];
		}
		else {
			memset(p_n_major_dest + iVertex, 0, sizeof(nvals));
			memset(p_T_major_dest + iVertex, 0, sizeof(T3));
		};
	};
}

__global__ void kernelAdvanceDensityAndTemperature_noadvectioncompression_Copy(
	f64 h_use,
	structural * __restrict__ p_info_major,
	nvals * p_n_major,
	T3 * p_T_major,
	NTrates * __restrict__ NTadditionrates,
	nvals * p_n_use,
	T3 * p_T_use,
	v4 * __restrict__ p_vie_use,
	f64_vec3 * __restrict__ p_v_n_use,
	f64 * __restrict__ p_AreaMajor,
	nvals * __restrict__ p_n_major_dest,
	T3 * __restrict__ p_T_major_dest,
	f64_vec3 * __restrict__ p_B_major,
	f64 * __restrict__ p_Tgraph_resistive,
	f64 * __restrict__ p_Tgraph_other,
	f64 * __restrict__ p_Tgraph_total,
	f64 * __restrict__ p_Tgraph_dNT
)
{
	// runs for major tile
	// nu would have been a better choice to go in shared as it coexists with the 18 doubles in "LHS","inverted".
	// Important to set 48K L1 for this routine.

	__shared__ nvals n_src_or_use[threadsPerTileMajor];
	__shared__ f64 AreaMajor[threadsPerTileMajor];

	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // iVertex OF VERTEX
	structural info = p_info_major[iVertex];
	//	if (iVertex == CHOSEN) printf("GPU iVertex %d info.flag %d \n", CHOSEN, info.flag);

	if ((info.flag == DOMAIN_VERTEX)) {

		n_src_or_use[threadIdx.x] = p_n_major[iVertex];  // used throughout so a good candidate to stick in shared mem
		AreaMajor[threadIdx.x] = p_AreaMajor[iVertex]; // ditto

		NTrates newdata;
		{
			NTrates AdditionNT = NTadditionrates[iVertex];
			newdata.N = n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] + h_use * AdditionNT.N;
			newdata.Nn = n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] + h_use * AdditionNT.Nn;
			newdata.NnTn = h_use * AdditionNT.NnTn; // start off without knowing 'factor' so we can ditch AdditionNT
			newdata.NiTi = h_use * AdditionNT.NiTi;
			newdata.NeTe = h_use * AdditionNT.NeTe;

			nvals n_dest;
			n_dest.n = newdata.N / (AreaMajor[threadIdx.x]);
			n_dest.n_n = newdata.Nn / (AreaMajor[threadIdx.x]);
			p_n_major_dest[iVertex] = n_dest;

			if (TEST)
				printf("Bdvance_nT  %d : nsrc %1.13E nn %1.13E *AreaMajor %1.13E %1.13E\n"
					"newdata.Nn %1.12E newdata.Ni %1.12E AreaMajor %1.14E n_n_k+1 %1.14E \n"
					"h*additionNT.N %1.14E h*additionNT.Nn %1.14E h %1.14E \n"


					, VERTCHOSEN,
					n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n,

					n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x],
					n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x],

					newdata.Nn, newdata.N, AreaMajor[threadIdx.x], n_dest.n_n,
					h_use*AdditionNT.N, h_use*AdditionNT.Nn, h_use);

		}

		f64 factor, factor_neut; // used again at end
		
		T3 T_src = p_T_major[iVertex];
			newdata.NnTn += n_src_or_use[threadIdx.x].n_n*AreaMajor[threadIdx.x] * T_src.Tn;
			newdata.NiTi += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Ti;
			newdata.NeTe += n_src_or_use[threadIdx.x].n*AreaMajor[threadIdx.x] * T_src.Te;

			//
			if (TEST) {
				printf("\nAdvance_nT %d : n %1.12E Area %1.12E compressfac %1.10E \n"
					"newdata.NiTi %1.12E Ti_k %1.12E newdata.NeTe %1.10E Te_k %1.10E\n"
					"newdata.NnTn %1.12E Tn_k %1.12E \n"
					,
					VERTCHOSEN, n_src_or_use[threadIdx.x].n, AreaMajor[threadIdx.x], factor,
					newdata.NiTi, T_src.Ti, newdata.NeTe, T_src.Te,
					newdata.NnTn, T_src.Tn);
			}
		

		f64 nu_ne_MT, nu_en_MT, nu_ni_MT, nu_in_MT, nu_ei_effective; // optimize after
		f64 nu_eiBar;
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal, lnLambda, s_in_MT, s_en_MT, s_en_visc;

			n_src_or_use[threadIdx.x] = p_n_use[iVertex];
			T3 T_use = p_T_use[iVertex];

			sqrt_Te = sqrt(T_use.Te); // should be "usedata"
			ionneut_thermal = sqrt(T_use.Ti / m_ion + T_use.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_src_or_use[threadIdx.x].n, T_use.Te);

			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T_use.Ti*one_over_kB,
					&s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T_use.Te*one_over_kB, // call with T in electronVolts
				&s_en_MT,
				&s_en_visc);
			//s_en_MT = Estimate_Ion_Neutral_MT_Cross_section(T_use.Te*one_over_kB);
			//s_en_visc = Estimate_Ion_Neutral_Viscosity_Cross_section(T_use.Te*one_over_kB);

			if (n_src_or_use[threadIdx.x].n_n > ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n) {
				s_en_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				s_in_MT *= n_src_or_use[threadIdx.x].n_n / (ARTIFICIAL_RELATIVE_THRESH_HEAT *n_src_or_use[threadIdx.x].n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			}
			// ARTIFICIAL CHANGE TO STOP HAVING TO WORRY ABOUT SILLY VALUES IN AREAS THAT DON'T MATTER MUCH :
			s_en_MT *= ArtificialUpliftFactor(n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n);
			s_in_MT *= ArtificialUpliftFactor(n_src_or_use[threadIdx.x].n, n_src_or_use[threadIdx.x].n_n); // returns factor 1.0 if n+nn > 1.0e14.
			 // Send heat into neutrals if there's not much stuff here total.

			  // Need nu_ne etc to be defined:

			nu_ne_MT = s_en_MT * n_src_or_use[threadIdx.x].n * electron_thermal; // have to multiply by n_e for nu_ne_MT
			nu_ni_MT = s_in_MT * n_src_or_use[threadIdx.x].n * ionneut_thermal;
			nu_en_MT = s_en_MT * n_src_or_use[threadIdx.x].n_n*electron_thermal;
			nu_in_MT = s_in_MT * n_src_or_use[threadIdx.x].n_n*ionneut_thermal;

			nu_eiBar = nu_eiBarconst * kB_to_3halves*max(MINIMUM_NU_EI_DENSITY, n_src_or_use[threadIdx.x].n)*lnLambda / (T_use.Te*sqrt_Te);

			if (TEST) {
				printf("nu_eiBar %1.12E n %1.12E lnLambda %1.10E \n\n", nu_eiBar, n_src_or_use[threadIdx.x].n, lnLambda);

				real Te_eV = T_use.Te*one_over_kB;
				real Te_eV2 = Te_eV*Te_eV;
				real Te_eV3 = Te_eV*Te_eV2;

				if (n_src_or_use[threadIdx.x].n*Te_eV3 > 0.0) {

					f64	lnLambda1 = 23.0 - 0.5*log(n_src_or_use[threadIdx.x].n / Te_eV3);
					f64	lnLambda2 = 24.0 - 0.5*log(n_src_or_use[threadIdx.x].n / Te_eV2);
					// smooth between the two:
					f64	factorxx = 2.0*fabs(Te_eV - 10.0)*(Te_eV - 10.0) / (1.0 + 4.0*(Te_eV - 10.0)*(Te_eV - 10.0));
					lnLambda = lnLambda1*(0.5 - factorxx) + lnLambda2*(0.5 + factorxx);

					printf("lnLambda1 2 %1.14E %1.14E lnLambda %1.14E Te_eV %1.12E factorxx %1.12E \n", lnLambda1, lnLambda2, lnLambda, Te_eV, factorxx);

					// floor at 2 just in case, but it should not get near:
				f64	lnLambda_sq = lnLambda*lnLambda;
					factorxx = 1.0 + 0.5*lnLambda + 0.25*lnLambda_sq + 0.125*lnLambda*lnLambda_sq + 0.0625*lnLambda_sq*lnLambda_sq;
					lnLambda += 2.0 / factorxx;
					printf("lnLambda %1.14E after floor at 2 ... \n", lnLambda);
					if (lnLambda < 2.0) lnLambda = 2.0;
				};
			};
			
			f64 nu_eHeart = 1.87*nu_eiBar + n_src_or_use[threadIdx.x].n_n*s_en_visc*electron_thermal;

			f64_vec3 omega = p_B_major[iVertex] * qovermc;
			// Confusing, why does this say that? We used visc en in nu_eHeart, explanation?

			f64 nu_ei_effective = nu_eiBar * (1.0 - 0.9*nu_eiBar*
				(nu_eHeart*nu_eHeart + omega.z*omega.z) /
				(nu_eHeart*(nu_eHeart*nu_eHeart + omega.dot(omega))));

			//		nu_ie = nu_ei;

			//		nu_eHeart = 1.87*nu_eiBar + data_k.n_n*s_en_visc*electron_thermal;
		}


		// For now doing velocity-independent resistive heating.
		// Because although we have a magnetic correction Upsilon_zz involved, we ignored it
		// since we are also squashing the effect of velocity-dependent collisions on vx and vy (which
		// would produce a current in the plane) and this squashing should create heat, which
		// maybe means it adds up to the velocity-independent amount of heating. 
		{
			f64_vec3 v_n = p_v_n_use[iVertex];
			v4 vie = p_vie_use[iVertex];


			newdata.NeTe += h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n * TWOTHIRDS*nu_en_MT*m_en*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

				+ AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n* TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz));

			p_Tgraph_resistive[iVertex] = TWOTHIRDS*nu_en_MT*m_en*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

				+ TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz);



			// I see that I did resistive heating for nu_ei but did something much more complicated in the acceleration routine.
			// That isn't quite right then.



			newdata.NiTi += h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n * TWOTHIRDS*nu_in_MT*M_in*m_n*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			newdata.NnTn += h_use*(AreaMajor[threadIdx.x] * n_src_or_use[threadIdx.x].n_n * TWOTHIRDS*nu_ni_MT*M_in*m_i*(
				(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
				+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
				+ (v_n.z - vie.viz)*(v_n.z - vie.viz)));

			//if (TEST) 
			//	printf("%d v_n.z %1.9E vie_use.viz %1.9E vie_use.vez %1.9E \n areamajor %1.8E\n"
			//		"nu_in %1.10E nu_en %1.8E \n"
			//		"Frictional htg (NT+=): n i e %1.10E %1.10E %1.10E\n",
			//		VERTCHOSEN, v_n.z, vie.viz, vie.vez, AreaMajor[threadIdx.x],
			//		nu_in_MT, nu_en_MT,
			//		h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ni_MT*M_in*m_i*(
			//		(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
			//			+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
			//			+ (v_n.z - vie.viz)*(v_n.z - vie.viz))),
			//		h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_in_MT*M_in*m_n*(
			//		(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
			//			+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
			//			+ (v_n.z - vie.viz)*(v_n.z - vie.viz))),
			//		h_use*(AreaMajor[threadIdx.x] * TWOTHIRDS*nu_en_MT*m_en*(
			//		(v_n.x - vie.vxy.x)*(v_n.x - vie.vxy.x)
			//			+ (v_n.y - vie.vxy.y)*(v_n.y - vie.vxy.y)
			//			+ (v_n.z - vie.vez)*(v_n.z - vie.vez))

			//			+ AreaMajor[threadIdx.x] * TWOTHIRDS*nu_ei_effective*m_ei*(vie.vez - vie.viz)*(vie.vez - vie.viz))
			//	);			
		}
		f64_tens3 inverted;
		{
			f64_tens3 LHS;
			// x = neutral
			// y = ion
			// z = elec
			// This is for NT
			f64 nu_ie = nu_eiBar;

			// gonna have to change to Backward Euler :/
			// 6th Nov 2019 : add 2* so that it all goes here.

			LHS.xx = 1.0 - 2.0*h_use * (-M_en * nu_ne_MT - M_in * nu_ni_MT);
			LHS.xy = -2.0*h_use * (M_in * nu_in_MT);
			LHS.xz = -2.0*h_use *(M_en * nu_en_MT);

			LHS.yx = -2.0*h_use *  M_in * nu_ni_MT;
			LHS.yy = 1.0 - 2.0*h_use * (-M_in * nu_in_MT - M_ei * nu_ie);
			LHS.yz = -2.0*h_use * M_ei * nu_eiBar;

			LHS.zx = -2.0*h_use * M_en * nu_ne_MT;
			LHS.zy = -2.0*h_use * M_ei * nu_ie;
			LHS.zz = 1.0 - 2.0*h_use * (-M_en * nu_en_MT - M_ei * nu_eiBar);

			// some indices appear reversed because NT not T.
			if (TEST) printf("LHS.zz %1.10E h_use %1.10E M_en %1.10E nu_en_MT %1.10E nu_eiBar %1.10E\n",
				LHS.zz, h_use, M_en, nu_en_MT, nu_eiBar);

			if (TEST) {
				printf("LHS | \n %1.14E %1.14E %1.14E |\n %1.14E %1.14E %1.14E |  \n %1.14E %1.14E %1.14E | \n",
					LHS.xx, LHS.xy, LHS.xz,
					LHS.yx, LHS.yy, LHS.yz,
					LHS.zx, LHS.zy, LHS.zz);
				printf("GPU %d : NnTn %1.14E NeTe %1.14E \n", VERTCHOSEN, newdata.NnTn, newdata.NeTe);
				printf("GPU nu_en_MT %1.14E\n", nu_en_MT);
				
			}
			LHS.Inverse(inverted);
		}

		f64_vec3 RHS;
		f64 nu_ie = nu_eiBar;

		// gonna have to change to Backward Euler :/

		//RHS.x = newdata.NnTn - h_use * (nu_ni_MT*M_in + nu_ne_MT * M_en)*newdata.NnTn
		//	+ h_use * nu_in_MT*M_in*newdata.NiTi + h_use * nu_en_MT*M_en*newdata.NeTe;

		//RHS.y = newdata.NiTi - h_use * (nu_in_MT*M_in + nu_ie * M_ei)*newdata.NiTi
		//	+ h_use * nu_ni_MT*M_in*newdata.NnTn + h_use * nu_ei*M_ei*newdata.NeTe;

		//RHS.z = newdata.NeTe - h_use * (nu_en_MT*M_en + nu_ei * M_ei)*newdata.NeTe
		//	+ h_use * nu_ie*M_ei*newdata.NiTi + h_use * nu_ne_MT*M_en*newdata.NnTn;

		RHS.x = newdata.NnTn;
		RHS.y = newdata.NiTi;
		RHS.z = newdata.NeTe;

		f64_vec3 NT;
		NT = inverted * RHS;
		newdata.NnTn = NT.x;
		newdata.NiTi = NT.y;
		newdata.NeTe = NT.z;

		T3 T_dest;
		T_dest.Tn = newdata.NnTn / newdata.Nn;
		T_dest.Ti = newdata.NiTi / newdata.N;
		T_dest.Te = newdata.NeTe / newdata.N;

		if (TEST) {
			printf("\ninverted | RHS \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n %1.14E %1.14E %1.14E | %1.14E \n",
				inverted.xx, inverted.xy, inverted.xz, RHS.x,
				inverted.yx, inverted.yy, inverted.yz, RHS.y,
				inverted.zx, inverted.zy, inverted.zz, RHS.z);
			printf("GPU %d : NnTn %1.14E NiTi %1.14E NeTe %1.14E \n"
				"Tn Ti Te %1.14E %1.14E %1.14E\n", VERTCHOSEN, newdata.NnTn, newdata.NiTi, newdata.NeTe,
				T_dest.Tn, T_dest.Ti, T_dest.Te);
		} // This came out with a value.

		if (TEST) printf("%d : T_dest %1.8E %1.8E %1.8E \n"
			"newdata .NeTe %1.10E .N %1.10E factor %1.10E\n\n",
			iVertex, T_dest.Tn, T_dest.Ti, T_dest.Te,
			newdata.NeTe, newdata.N, factor
		);

		p_T_major_dest[iVertex] = T_dest;

		p_Tgraph_other[iVertex] = 2.0 * M_en * nu_en_MT*(T_dest.Tn - T_dest.Te)
								+ 2.0 * M_ei * nu_eiBar*(T_dest.Ti - T_dest.Te);
		p_Tgraph_total[iVertex] = (T_dest.Te - T_src.Te) / h_use;
		p_Tgraph_dNT[iVertex] = (T_dest.Te - T_src.Te)* newdata.N / (AreaMajor[threadIdx.x] * h_use);
	} else {
		// nothing to do ??
		if (info.flag == OUTERMOST) {
			p_n_major_dest[iVertex] = p_n_major[iVertex];
			p_T_major_dest[iVertex] = p_T_major[iVertex];
		}
		else {
			memset(p_n_major_dest + iVertex, 0, sizeof(nvals));
			memset(p_T_major_dest + iVertex, 0, sizeof(T3));
		};
	};
}
/*
__global__ void kernelCalculateUpwindDensity_tris(
	structural * __restrict__ p_info_minor,
	ShardModel * __restrict__ p_n_shard_n_major,
	ShardModel * __restrict__ p_n_shard_major,
	v4 * __restrict__ p_vie_minor,
	f64_vec3 * __restrict__ p_v_n_minor,
	f64_vec2 * __restrict__ p_overall_v_minor,
	LONG3 * __restrict__ p_tricornerindex,
	LONG3 * __restrict__ p_trineighindex,
	LONG3 * __restrict__ p_which_iTri_number_am_I,
	CHAR4 * __restrict__ p_szPBCneigh_tris,
	nvals * __restrict__ p_n_upwind_minor, // result 

	T3 * __restrict__ p_T_minor,
	T3 * __restrict__ p_T_upwind_minor // result
)
{
	// The idea is to take the upwind n on each side of each
	// major edge through this tri, weighted by |v.edge_normal|
	// to produce an average.
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor]; // 4 doubles/vertex
	__shared__ f64_12 shared_shards[threadsPerTileMajor];  // + 12
														   // 15 doubles right there. Max 21 for 288 vertices. 16 is okay.
														   // Might as well stick 1 more double  in there if we get worried about registers.
						
														   // #############################################%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%###############
														   // We need a reverse index: this triangle carry 3 indices to know who it is to its corners.
	long const iTri = blockDim.x*blockIdx.x + threadIdx.x;
	structural const info = p_info_minor[iTri];
	nvals result;
	T3 upwindT;

	shared_pos[threadIdx.x] = info.pos;
	long const StartMajor = blockIdx.x*threadsPerTileMajor;
	long const EndMajor = StartMajor + threadsPerTileMajor;
	long const StartMinor = blockIdx.x*threadsPerTileMinor;
	long const EndMinor = StartMinor + threadsPerTileMinor;

	if (threadIdx.x < threadsPerTileMajor)
	{
		memcpy(&(shared_shards[threadIdx.x].n), &(p_n_shard_major[threadsPerTileMajor*blockIdx.x + threadIdx.x].n), MAXNEIGH * sizeof(f64));
		// efficiency vs memcpy? We only need 12 here, not the centre.
	}
	__syncthreads();

	f64 n0, n1, n2;
	T3 T0, T1, T2;
	f64_vec2 edge_normal0, edge_normal1, edge_normal2;
	LONG3 tricornerindex, trineighindex;
	LONG3 who_am_I;
	f64_vec2 v_overall;
	char szPBC_triminor[6];
	CHAR4 szPBC_neighs;
	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)
		|| (info.flag == CROSSING_CATH))
	{
		// Several things we need to collect:
		// . v in this triangle and mesh v at this triangle centre.
		// . edge_normal going each way
		// . n that applies from each corner

		// How to get n that applies from each corner:
		tricornerindex = p_tricornerindex[iTri];
		who_am_I = p_which_iTri_number_am_I[iTri];
		szPBC_neighs = p_szPBCneigh_tris[iTri];

		// Wasteful:
		T0 = p_T_minor[tricornerindex.i1 + BEGINNING_OF_CENTRAL];
		T1 = p_T_minor[tricornerindex.i2 + BEGINNING_OF_CENTRAL];
		T2 = p_T_minor[tricornerindex.i3 + BEGINNING_OF_CENTRAL];

		if ((tricornerindex.i1 >= StartMajor) && (tricornerindex.i1 < EndMajor))
		{
			n0 = shared_shards[tricornerindex.i1 - StartMajor].n[who_am_I.i1]; // whoa, be careful with data type / array
		}
		else {
			n0 = p_n_shard_major[tricornerindex.i1].n[who_am_I.i1];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}
		if ((tricornerindex.i2 >= StartMajor) && (tricornerindex.i2 < EndMajor))
		{
			n1 = shared_shards[tricornerindex.i2 - StartMajor].n[who_am_I.i2];
		}
		else {
			n1 = p_n_shard_major[tricornerindex.i2].n[who_am_I.i2];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}
		if ((tricornerindex.i3 >= StartMajor) && (tricornerindex.i3 < EndMajor))
		{
			n2 = shared_shards[tricornerindex.i3 - StartMajor].n[who_am_I.i3];
		}
		else {
			n2 = p_n_shard_major[tricornerindex.i3].n[who_am_I.i3];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}

		v_overall = p_overall_v_minor[iTri];
		f64_vec2 relv = p_vie_minor[iTri].vxy - v_overall;
		
		// So this relies on the assumption that n = 0 outside of domain.

		if ((info.flag == CROSSING_INS) || (info.flag == CROSSING_CATH)) {
			int number_within = (n0 > 0.0) ? 1 : 0 + (n1 > 0.0) ? 1 : 0 + (n2 > 0.0) ? 1 : 0;
			if (number_within == 1) {
				result.n = n0 + n1 + n2;
				upwindT.Te = T0.Te + T1.Te + T2.Te;
				upwindT.Tn = T0.Tn + T1.Tn + T2.Tn;
				upwindT.Ti = T0.Ti + T1.Ti + T2.Ti; 
			}
			else {
				// quick way not upwind: 
				result.n = 0.5*(n0 + n1 + n2);
				upwindT.Te = 0.5*(T0.Te + T1.Te + T2.Te);
				upwindT.Tn = 0.5*(T0.Tn + T1.Tn + T2.Tn);
				upwindT.Ti = 0.5*(T0.Ti + T1.Ti + T2.Ti); // watch out for heat evacuating CROSSING_INS tris.
			}
			//if (iTri == 23400) printf("\n23400 was an insulator tri, T012 %1.8E %1.8E %1.8E upwind %1.8E\n"
			//	"indexcorner %d %d %d\n\n",
			//	T0.Te,T1.Te,T2.Te,upwindT.Te,
			//	tricornerindex.i1, tricornerindex.i1, tricornerindex.i3);

			if (info.flag == CROSSING_CATH) {
				// set n = 0 if position is within cathode rod:
				if (!TestDomainPos(info.pos)) {
					result.n = 0.0;					
				}
			}

		} else {

			trineighindex = p_trineighindex[iTri];

		//	if (iTri == CHOSEN) printf("%d GPU: n0 %1.14E n1 %1.14E n2 %1.14E \n"
		//		"relv GPU %1.14E %1.14E \n",
		//		CHOSEN, n0, n1, n2, relv.x, relv.y);

			f64_vec2 nearby_pos;
			if ((trineighindex.i1 >= StartMinor) && (trineighindex.i1 < EndMinor)) {
				nearby_pos = shared_pos[trineighindex.i1 - StartMinor];
			}
			else {
				nearby_pos = p_info_minor[trineighindex.i1].pos;
			}
			if (szPBC_neighs.per0 == ROTATE_ME_CLOCKWISE) {
				nearby_pos = Clockwise_d*nearby_pos;
			}
			if (szPBC_neighs.per0 == ROTATE_ME_ANTICLOCKWISE) {
				nearby_pos = Anticlockwise_d*nearby_pos;
			}
			// Slightly puzzled why we don't just take difference of 2 corners of our triangle.
			// Why dealing with tri positions instead of vertex positions? Because tri positions
			// are the corners of the major cell.

			edge_normal0.x = nearby_pos.y - info.pos.y;
			edge_normal0.y = info.pos.x - nearby_pos.x;
			// CAREFUL : which side is which???
			// tri centre 2 is on same side of origin as corner 1 -- I think
			// We don't know if the corners have been numbered anticlockwise?
			// Could arrange it though.
			// So 1 is anticlockwise for edge 0.

			f64 numerator = 0.0;
			f64 dot1, dot2;
			f64 dot0 = relv.dot(edge_normal0);

			if ((trineighindex.i2 >= StartMinor) && (trineighindex.i2 < EndMinor)) {
				nearby_pos = shared_pos[trineighindex.i2 - StartMinor];
			}
			else {
				nearby_pos = p_info_minor[trineighindex.i2].pos;
			}
			if (szPBC_neighs.per1 == ROTATE_ME_CLOCKWISE) {
				nearby_pos = Clockwise_d*nearby_pos;
			}
			if (szPBC_neighs.per1 == ROTATE_ME_ANTICLOCKWISE) {
				nearby_pos = Anticlockwise_d*nearby_pos;
			}
			edge_normal1.x = nearby_pos.y - info.pos.y;
			edge_normal1.y = info.pos.x - nearby_pos.x;

			dot1 = relv.dot(edge_normal1);
			if ((trineighindex.i3 >= StartMinor) && (trineighindex.i3 < EndMinor)) {
				nearby_pos = shared_pos[trineighindex.i3 - StartMinor];
			}
			else {
				nearby_pos = p_info_minor[trineighindex.i3].pos;
			}
			if (szPBC_neighs.per2 == ROTATE_ME_CLOCKWISE) {
				nearby_pos = Clockwise_d*nearby_pos;
			}
			if (szPBC_neighs.per2 == ROTATE_ME_ANTICLOCKWISE) {
				nearby_pos = Anticlockwise_d*nearby_pos;
			}

			edge_normal2.x = nearby_pos.y - info.pos.y;
			edge_normal2.y = info.pos.x - nearby_pos.x;

			dot2 = relv.dot(edge_normal2);

			bool b0, b1, b2; // is this n012 legit?
			if (dot0 > 0.0) { b2 = 1; }
			else { b1 = 1; };
			if (dot1 > 0.0) { b0 = 1; }
			else { b2 = 1; };
			if (dot2 > 0.0) { b1 = 1; }
			else { b0 = 1; };
			
			//Usually now only one of b012 is false.

			if (b0 == 0) {
				if (b1 == 0) {
					result.n = n2; // how idk
					memcpy(&upwindT, &T2, sizeof(T3));
				} else {
					if (b2 == 0) { 
						result.n = n1;
						memcpy(&upwindT, &T1, sizeof(T3));
					} else {
						result.n = min(n1, n2);
						upwindT.Te = min(T1.Te, T2.Te);
						upwindT.Ti = min(T1.Ti, T2.Ti);
					}
				}
			} else {
				if ((b1 == 0) && (b2 == 0)) {
					result.n = n0;
					memcpy(&upwindT, &T0, sizeof(T3));
				} else {
					if (b1 == 0) {
						result.n = min(n0, n2);
						memcpy(&upwindT, &T2, sizeof(T3));
					} else {
						if (b2 == 0)
						{
							result.n = min(n0, n1);
							upwindT.Te = min(T1.Te, T0.Te);
							upwindT.Ti = min(T1.Ti, T0.Ti);
						} else {
							result.n = min(min(n0, n1), n2);
							upwindT.Te = min(T0.Te, min(T1.Te, T2.Te));
							upwindT.Ti = min(T0.Ti, min(T1.Ti, T2.Ti));
						}
					}
				}
			}
		//	if (iTri == 23435) printf("CALC UPWIND n\n"
		//		"tricornerindex %d %d %d\n"
		//		"n0 n1 n2 %1.12E %1.12E %1.12E\n"
		//		"relv %1.9E %1.9E \n"
		//		"edge_nml %1.9E %1.9E | %1.9E %1.9E | %1.9E %1.9E \n"
		//		"dot %1.9E %1.9E %1.9E\n"
		//		"b0 b1 b2 %d %d %d \n"
		//		"result.n %1.9E\n\n",
		//		tricornerindex.i1, tricornerindex.i2, tricornerindex.i3,
		//		n0, n1, n2,
		//		relv.x, relv.y,
		//		edge_normal0.x, edge_normal0.y, edge_normal1.x, edge_normal1.y, edge_normal2.x, edge_normal2.y,
		//		dot0, dot1, dot2,
		//		(b0 ? 1 : 0), (b1 ? 1 : 0), (b2 ? 1 : 0),
		//		result.n);
		//	
//
//			if (iTri == 23400) printf("\n23400 was a domain tri, T012 %1.8E %1.8E %1.8E upwind %1.8E\n"
//				"relv %1.8E %1.8E b012 %d %d %d \n\n",
//				T0.Te, T1.Te, T2.Te, upwindT.Te,
//				relv.x, relv.y, (int)b0, (int)b1, (int)b2);
						// Alternative way: try using squared weights of upwind n for v.dot(edgenormal).

			// This old, doesn't work when JxB force empties out near ins:
			
			// Argument against fabs in favour of squared weights?
		};
		// Think carefully / debug how it goes for CROSSING_INS.
	} else {
		result.n = 0.0;
		memset(&upwindT, 0, sizeof(T3));
	};

	// Now same for upwind neutral density:
	// In order to use syncthreads we had to come out of the branching.

	if (threadIdx.x < threadsPerTileMajor)
	{
		memcpy(&(shared_shards[threadIdx.x].n), 
			&(p_n_shard_n_major[threadsPerTileMajor*blockIdx.x + threadIdx.x].n),
			sizeof(f64)*MAXNEIGH);
		// efficiency vs memcpy? We only need 12 here, not the centre.
	}
	__syncthreads();

	// &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)
		|| (info.flag == CROSSING_CATH))
	{
		if ((tricornerindex.i1 >= StartMajor) && (tricornerindex.i1 < EndMajor))
		{
			n0 = shared_shards[tricornerindex.i1 - StartMajor].n[who_am_I.i1];
		}
		else {
			n0 = p_n_shard_n_major[tricornerindex.i1].n[who_am_I.i1];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}
		if ((tricornerindex.i2 >= StartMajor) && (tricornerindex.i2 < EndMajor))
		{
			n1 = shared_shards[tricornerindex.i2 - StartMajor].n[who_am_I.i2];
		} else {
			n1 = p_n_shard_n_major[tricornerindex.i2].n[who_am_I.i2];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}
		if ((tricornerindex.i3 >= StartMajor) && (tricornerindex.i3 < EndMajor))
		{
			n2 = shared_shards[tricornerindex.i3 - StartMajor].n[who_am_I.i3];
		} else {
			n2 = p_n_shard_n_major[tricornerindex.i3].n[who_am_I.i3];
			// at least it's 1 bus journey this way instead of 2 to fetch n_shards.
		}

		f64_vec2 relv = p_v_n_minor[iTri].xypart() - v_overall;

		if ((info.flag == CROSSING_INS) || (info.flag == CROSSING_CATH)) {
			int number_within = ((n0 > 0.0) ? 1 : 0) + ((n1 > 0.0) ? 1 : 0) + ((n2 > 0.0) ? 1 : 0);
			if (number_within == 1) {
				result.n_n = n0 + n1 + n2;
				upwindT.Tn = T0.Tn + T1.Tn + T2.Tn;
				
		//		if ((iTri == 51243) || (iTri == 43048)) printf("%d : INS-1 nn012 %1.8E %1.8E %1.8E nn %1.10E\n",
		//			iTri, n0, n1, n2, result.n_n);

			} else {
				// quick way not upwind:
				result.n_n = 0.5*(n0 + n1 + n2);
				upwindT.Tn = 0.5*(T0.Tn + T1.Tn + T2.Tn);

//				if ((iTri == 51243) || (iTri == 43048)) printf("%d : INS-2 nn012 %1.8E %1.8E %1.8E nn %1.10E\n",
//					iTri, n0, n1, n2, result.n_n);

			};

			if (info.flag == CROSSING_CATH) {
				// set n = 0 if position is within cathode rod:
				if (!TestDomainPos(info.pos)) {
					result.n_n = 0.0;
				}
			}
		} else {

			f64 numerator = 0.0;
			f64 dot1, dot2;
			f64 dot0 = relv.dot(edge_normal0);
			dot1 = relv.dot(edge_normal1);
			dot2 = relv.dot(edge_normal2);
			
			bool b0, b1, b2; // is this n012 legit?
			if (dot0 > 0.0) { b2 = 1; }
			else { b1 = 1; };
			if (dot1 > 0.0) { b0 = 1; }
			else { b2 = 1; };
			if (dot2 > 0.0) { b1 = 1; }
			else { b0 = 1; };

			//Usually now only one of b012 is false.

			if (b0 == 0) {
				if (b1 == 0) {
					result.n_n = n2; // how idk

					upwindT.Tn = T2.Tn;
				}
				else {
					if (b2 == 0) { result.n = n1; }
					else {
						result.n_n = min(n1, n2);

						upwindT.Tn = min(T1.Tn, T2.Tn);
					}
				}
			}
			else {
				if ((b1 == 0) && (b2 == 0)) {
					result.n_n = n0;

					upwindT.Tn = T0.Tn;
				}
				else {
					if (b1 == 0) {
						result.n_n = min(n0, n2);

						upwindT.Tn = min(T2.Tn, T0.Tn);
					}
					else {
						if (b2 == 0)
						{
							result.n_n = min(n0, n1);

							upwindT.Tn = min(T1.Tn, T0.Tn);
						} else {
							result.n_n = min(min(n0, n1), n2);

							upwindT.Tn = min(min(T1.Tn, T0.Tn), T2.Tn);
						}
					}
				}
			}

		//	if ((iTri == 51243) || (iTri == 43048)) printf("%d : DOMAIN n012 %1.8E %1.8E %1.8E nn %1.10E\n",
		//		iTri, n0, n1,n2, result.n_n);


			// Look carefully at what happens for CROSSING_INS.
			// relv should be horizontal, hence it doesn't give a really low density? CHECK IT IN PRACTICE.
		};
	} else {
		result.n_n = 0.0;
		upwindT.Tn = 0.0;
	};

	p_n_upwind_minor[iTri] = result;
	p_T_upwind_minor[iTri] = upwindT;

}*/
/*
__global__ void kernelAccumulateAdvectiveMassHeatRate(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBCtri_verts,

	nvals * __restrict__ p_n_src_major,
	T3 * __restrict__ p_T_src_major,

	nvals * __restrict__ p_n_upwind_minor,
	v4 * __restrict__ p_vie_minor,
	f64_vec3 * __restrict__ p_v_n_minor,
	f64_vec2 * __restrict__ p_v_overall_minor,
	//T3 * __restrict__ p_T_minor, // may or may not overlap source: don't we only use from tris? so not overlap

	T3 * __restrict__ p_T_upwind_minor,

	NTrates * __restrict__ p_NTadditionrates,
	f64 * __restrict__ p_div_v,
	f64 * __restrict__ p_div_v_n,
	f64 * __restrict__ p_Integrated_div_v_overall
)
{
	// Use the upwind density from tris together with v_tri.
	// Seems to include a factor h

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor]; // only reused what, 3 times?  2*2 per major thread
	__shared__ nvals shared_n_upwind[threadsPerTileMinor]; 
	__shared__ f64_vec2 shared_vxy[threadsPerTileMinor]; // 2*2 per major thread
	__shared__ f64_vec2 shared_v_n[threadsPerTileMinor]; // could split routine; no good reason not to.
	//__shared__ f64_vec2 v_overall[threadsPerTileMinor];
	// choosing just to load it ad hoc
	__shared__ T3 shared_T[threadsPerTileMinor];          // 2*2 ... rightly or wrongly.



	// Do neutral after? Necessitates doing all the random loads again.
	// Is that worse than loading for each point at the time, a 2-vector v_overall?
	// About 6 bus journeys per external point. About 1/4 as many external as internal?
	// ^ only 6 because doing ion&neutral together. Changing to do sep could make sense.

	// 2* (2+2+2+2+3) = 22
	// Max viable threads at 26: 236
	// Max viable threads at 24: 256

	// Can't store rel v: we use div v of each v in what follows.

	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	{
		structural info[2];
		memcpy(info, p_info_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(structural) * 2);
		shared_pos[2 * threadIdx.x] = info[0].pos;
		shared_pos[2 * threadIdx.x + 1] = info[1].pos;
		
		memcpy(&(shared_n_upwind[2 * threadIdx.x]), 
			p_n_upwind_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(nvals) * 2);
		
		v4 vie[2];
		memcpy(&vie, p_vie_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(v4) * 2);
		shared_vxy[2 * threadIdx.x] = vie[0].vxy;
		shared_vxy[2 * threadIdx.x + 1] = vie[1].vxy;
		f64_vec3 v_n[2];
		memcpy(v_n, p_v_n_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(f64_vec3) * 2);
		shared_v_n[2 * threadIdx.x] = v_n[0].xypart();
		shared_v_n[2 * threadIdx.x + 1] = v_n[1].xypart();
		memcpy(&(shared_T[2 * threadIdx.x]), p_T_upwind_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(T3) * 2);
	}
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const EndMinor = threadsPerTileMinor + StartMinor;

	__syncthreads();

	// What happens for abutting ins?
	// T defined reasonably at insulator-crossing tri, A defined, v defined reasonably

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

	if (info.flag == DOMAIN_VERTEX) {

		// T3 Tsrc = p_T_src_major[iVertex]; // UNUSED!
		nvals nsrc = p_n_src_major[iVertex];
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		memcpy(izTri, p_izTri + iVertex * MAXNEIGH, sizeof(long) * MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_verts + iVertex*MAXNEIGH, sizeof(char)*MAXNEIGH);
		// Now we are assuming what? Neigh 0 is below tri 0, so 0 1 are on neigh 0
		// Check in debug. Looks true from comments.
		short tri_len = info.neigh_len;

		f64_vec2 edge_normal, endpt0, endpt1;
		f64_vec2 vxy_prev, vxy_next;
		f64_vec2 v_n_prev, v_n_next;
		f64 n_next, n_prev, nn_next, nn_prev;
		f64_vec2 v_overall_prev, v_overall_next;
		f64 Te_next, Te_prev, Ti_next, Ti_prev, Tn_next, Tn_prev;

		short inext, i = 0;
		long iTri = izTri[0];
		v_overall_prev = p_v_overall_minor[iTri];
		if ((iTri >= StartMinor) && (iTri < EndMinor)) {
			endpt0 = shared_pos[iTri - StartMinor];
			nvals nvls = shared_n_upwind[iTri - StartMinor];
			n_prev = nvls.n;
			nn_prev = nvls.n_n;
			vxy_prev = shared_vxy[iTri - StartMinor];
			v_n_prev = shared_v_n[iTri - StartMinor];
			Te_prev = shared_T[iTri - StartMinor].Te;
			Ti_prev = shared_T[iTri - StartMinor].Ti;
			Tn_prev = shared_T[iTri - StartMinor].Tn;

		} else {
			// The volume of random bus accesses means that we would have been better off making a separate
			// neutral routine even though it looks efficient with the shared loading. nvm
			endpt0 = p_info_minor[iTri].pos;
			nvals n_upwind = p_n_upwind_minor[iTri];
			n_prev = n_upwind.n;
			nn_prev = n_upwind.n_n;
			vxy_prev = p_vie_minor[iTri].vxy;
			v_n_prev = p_v_n_minor[iTri].xypart();
			T3 Tuse = p_T_upwind_minor[iTri];
			Te_prev = Tuse.Te;
			Ti_prev = Tuse.Ti;
			Tn_prev = Tuse.Tn;
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
			endpt0 = Clockwise_d*endpt0;
			vxy_prev = Clockwise_d*vxy_prev;
			v_n_prev = Clockwise_d*v_n_prev;
			v_overall_prev = Clockwise_d*v_overall_prev;
		};
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
			endpt0 = Anticlockwise_d*endpt0;
			vxy_prev = Anticlockwise_d*vxy_prev;
			v_n_prev = Anticlockwise_d*v_n_prev;
			v_overall_prev = Anticlockwise_d*v_overall_prev;
		};

		nvals totalmassflux_out;
		memset(&totalmassflux_out, 0, sizeof(nvals));
		T3 totalheatflux_out;
		memset(&totalheatflux_out, 0, sizeof(T3));
		f64 Integrated_div_v = 0.0;
		f64 Integrated_div_v_n = 0.0;
		f64 Integrated_div_v_overall = 0.0;
		f64 AreaMajor = 0.0;

#pragma unroll MAXNEIGH
		for (i = 0; i < tri_len; i++)
		{
			inext = i + 1; if (inext == tri_len) inext = 0;

			long iTri = izTri[inext];
			f64_vec2 v_overall_next = p_v_overall_minor[iTri];
			if ((iTri >= StartMinor) && (iTri < EndMinor)) {
				endpt1 = shared_pos[iTri - StartMinor];
				nvals nvls = shared_n_upwind[iTri - StartMinor];
				n_next = nvls.n;
				nn_next = nvls.n_n;
				vxy_next = shared_vxy[iTri - StartMinor];
				v_n_next = shared_v_n[iTri - StartMinor];
				Te_next = shared_T[iTri - StartMinor].Te;
				Ti_next = shared_T[iTri - StartMinor].Ti;
				Tn_next = shared_T[iTri - StartMinor].Tn;
			} else {
				// The volume of random bus accesses means that we would have been better off making a separate
				// neutral routine even though it looks efficient with the shared loading. nvm
				endpt1 = p_info_minor[iTri].pos;
				nvals n_upwind = p_n_upwind_minor[iTri];
				n_next = n_upwind.n;
				nn_next = n_upwind.n_n;
				vxy_next = p_vie_minor[iTri].vxy;
				v_n_next = p_v_n_minor[iTri].xypart();
				T3 Tuse = p_T_upwind_minor[iTri];
				Te_next = Tuse.Te;
				Ti_next = Tuse.Ti;
				Tn_next = Tuse.Tn;
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
				endpt1 = Clockwise_d*endpt1;
				vxy_next = Clockwise_d*vxy_next;
				v_n_next = Clockwise_d*v_n_next;
				v_overall_next = Clockwise_d*v_overall_next;
			};
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
				endpt1 = Anticlockwise_d*endpt1;
				vxy_next = Anticlockwise_d*vxy_next;
				v_n_next = Anticlockwise_d*v_n_next;
				v_overall_next = Anticlockwise_d*v_overall_next;
			};
			
			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			AreaMajor += 0.5*edge_normal.x*(endpt0.x + endpt1.x);

		//	if (iVertex == CHOSEN) printf("GPU %d : AreaMajor %1.9E edge_nml.x %1.6E endpt0.x %1.6E endpt1.x %1.6E \n",
		//		iVertex,
		//		AreaMajor, edge_normal.x, endpt0.x, endpt1.x);
			
			if ((n_prev != 0.0) && (n_next != 0.0)) {

				Integrated_div_v_overall += 0.5*(v_overall_prev + v_overall_next).dot(edge_normal); // Average outward velocity of edge...
				// The area CAN be changing because of other vertices dragging on it.
				// However we can ignore it as n,T should be locally constant near the rod
				// anyway.

				Integrated_div_v += 0.5*(vxy_prev + vxy_next).dot(edge_normal);
				totalmassflux_out.n += 0.5*(n_prev*(vxy_prev - v_overall_prev)
					+ n_next*(vxy_next - v_overall_next)).dot(edge_normal);
				totalheatflux_out.Ti += 0.5*(n_prev*Ti_prev*(vxy_prev - v_overall_prev)
					+ n_next*Ti_next*(vxy_next - v_overall_next)).dot(edge_normal);
				totalheatflux_out.Te += 0.5*(n_prev*Te_prev*(vxy_prev - v_overall_prev)
					+ n_next*Te_next*(vxy_next - v_overall_next)).dot(edge_normal);
			};

			if ((nn_prev != 0.0) && (nn_next != 0.0)) {
				Integrated_div_v_n += 0.5*(v_n_prev + v_n_next).dot(edge_normal);
				totalmassflux_out.n_n += 0.5*(nn_prev*(v_n_prev - v_overall_prev)
					+ nn_next*(v_n_next - v_overall_next)).dot(edge_normal);
				totalheatflux_out.Tn += 0.5*(nn_prev*Tn_prev*(v_n_prev - v_overall_prev)
					+ nn_next*Tn_next*(v_n_next - v_overall_next)).dot(edge_normal);
			};

			if (TEST1) printf("advect GPU %d : "
			"i %d iTri %d heatfluxout_contrib e %1.14E \n"
				"nprev %1.14E nnext %1.14E\n"
				"Te_prev next %1.14E %1.14E \nrel vxy %1.14E %1.14E ; %1.14E %1.14E\n"
				"edge_normal %1.14E %1.14E \n"
				"-------------------------\n",
				iVertex, i, iTri,
				0.5*(n_prev*Te_prev*(vxy_prev - v_overall_prev)
					+ n_next*Te_next*(vxy_next - v_overall_next)).dot(edge_normal),
				n_prev, n_next,
				Te_prev, Te_next, (vxy_prev - v_overall_prev).x, (vxy_prev - v_overall_prev).y,
				(vxy_next - v_overall_next).x, (vxy_next - v_overall_next).y,
				edge_normal.x, edge_normal.y);

			if (TESTADVECT) printf("AccumulateAdvectiveMassHeatRate iVertex %d : inext %d iTri %d \n"
				"NTiflux %1.9E cumu %1.9E n_prev %1.9E n_next %1.9E vxyprev %1.7E %1.7E\n"
				"vxy_prev.edgenml %1.9E v_overall_prev. %1.9E vxy_next. %1.9E v_overall_next. %1.9E\n"
				"Ti_prev %1.9E Ti_next %1.9E prev contrib %1.9E nex cntrib %1.9E\n"
				"v_overall_next %1.9E %1.9E  |  \n"
				"------------------------------------------------\n",
				iVertex, i, iTri,
				0.5*(n_prev*Ti_prev*(vxy_prev - v_overall_prev)
					+ n_next*Ti_next*(vxy_next - v_overall_next)).dot(edge_normal),
				totalheatflux_out.Ti, n_prev, n_next, vxy_prev.x, vxy_prev.y,
				vxy_prev.dot(edge_normal),
				v_overall_prev.dot(edge_normal),
				vxy_next.dot(edge_normal),
				v_overall_next.dot(edge_normal),
				Ti_prev, Ti_next,
				0.5*n_prev*Ti_prev*(vxy_prev - v_overall_prev).dot(edge_normal),
				0.5*n_next*Ti_next*(vxy_next - v_overall_next).dot(edge_normal),
				v_overall_next.x, v_overall_next.y
			);

			if (TESTADVECTNEUT) printf("AccumulateAdvectiveMassHeatRate iVertex %d : inext %d iTri %d \n"
				"NnTnflux %1.9E cumu %1.9E nn_prev %1.9E nn_next %1.9E vxyprev %1.7E %1.7E\n"
				"vxy_prev.edgenml %1.9E v_overall_prev. %1.9E vxy_next. %1.9E v_overall_next. %1.9E\n"
				"Tn_prev %1.9E Tn_next %1.9E prev contrib %1.9E nex cntrib %1.9E\n"
				"v_overall_next %1.9E %1.9E\n"
				"------------------------------------------------\n",
				iVertex, i, iTri,
				0.5*(nn_prev*Tn_prev*(v_n_prev - v_overall_prev)
					+ nn_next*Tn_next*(v_n_next - v_overall_next)).dot(edge_normal),
				totalheatflux_out.Tn, nn_prev, nn_next, v_n_prev.x, v_n_prev.y,
				v_n_prev.dot(edge_normal),
				v_overall_prev.dot(edge_normal),
				v_n_next.dot(edge_normal),
				v_overall_next.dot(edge_normal),
				Tn_prev, Tn_next,
				0.5*nn_prev*Tn_prev*(v_n_prev - v_overall_prev).dot(edge_normal),
				0.5*nn_next*Tn_next*(v_n_next - v_overall_next).dot(edge_normal),
				v_overall_next.x, v_overall_next.y
			);
		//	if (TEST) printf("advect GPU %d : "
		//		"i %d iTri %d heatfluxout_contrib %1.14E \n"
		//		"nprev %1.14E nnext %1.14E\n"
		//		"Ti_prev next %1.14E %1.14E \nrel vxy %1.14E %1.14E ; %1.14E %1.14E\n"
		//		"edge_normal %1.14E %1.14E \n"
		//		"-------------------------\n",
		//		iVertex, i, iTri,
		//		0.5*(n_prev*Ti_prev*(vxy_prev - v_overall_prev)
		//			+ n_next*Ti_next*(vxy_next - v_overall_next)).dot(edge_normal),
		//		n_prev, n_next,
		//		Ti_prev, Ti_next, (vxy_prev-v_overall_prev).x, (vxy_prev - v_overall_prev).y,
		//		(vxy_next - v_overall_next).x, (vxy_next - v_overall_next).y,
		//		edge_normal.x, edge_normal.y);
			if (TEST1) printf("advect GPU %d : "
				"i %d iTri %d heatfluxout_contrib e %1.14E \n"
				"nprev %1.14E nnext %1.14E\n"
				"Te_prev next %1.14E %1.14E \nrel vxy %1.14E %1.14E ; %1.14E %1.14E\n"
				"edge_normal %1.14E %1.14E \n"
				"-------------------------\n",
				iVertex, i, iTri,
				0.5*(n_prev*Te_prev*(vxy_prev - v_overall_prev)
					+ n_next*Te_next*(vxy_next - v_overall_next)).dot(edge_normal)	,
				n_prev, n_next,
				Te_prev, Te_next, (vxy_prev - v_overall_prev).x, (vxy_prev - v_overall_prev).y,
				(vxy_next - v_overall_next).x, (vxy_next - v_overall_next).y,
				edge_normal.x, edge_normal.y);


//
			endpt0 = endpt1;
			n_prev = n_next;
			nn_prev = nn_next;
			vxy_prev = vxy_next;
			v_n_prev = v_n_next;
			v_overall_prev = v_overall_next;
			Ti_prev = Ti_next;
			Te_prev = Te_next;
			Tn_prev = Tn_next;
		};

		NTrates NTplus;

		NTplus.N = -totalmassflux_out.n;
		NTplus.Nn = -totalmassflux_out.n_n;
		NTplus.NeTe = -totalheatflux_out.Te;
		NTplus.NiTi = -totalheatflux_out.Ti;
		NTplus.NnTn = -totalheatflux_out.Tn;
//
//		if (TEST) printf("\n%d : NTplus.NiTi %1.10E NTplus.N %1.10E Tsrc.i %1.10E nsrc.n %1.10E\n"
//			"NTplus.NiTi/NTplus.N (avg temp of those coming/going) %1.10E\n"
//			"NTplus.NiTi/N (ROC Ti) %1.10E\n"
//			"NTplus.NiTi/NiTi (elasticity of T?) %1.10E \n"
//			"NTplus.N/N (elasticity of N) %1.10E \n\n",
//			CHOSEN, NTplus.NiTi, NTplus.N,
//			Tsrc.Ti, nsrc.n,
//			NTplus.NiTi/NTplus.N,
//			NTplus.NiTi/(AreaMajor*nsrc.n),
//			NTplus.NiTi/(AreaMajor*nsrc.n*Tsrc.Ti),
//			NTplus.N/(AreaMajor*nsrc.n)
//			);

		memcpy(p_NTadditionrates + iVertex, &NTplus, sizeof(NTrates));

		// What we need now: 
		//	* Cope with non-domain vertex
		p_div_v[iVertex] = Integrated_div_v / AreaMajor;
		p_div_v_n[iVertex] = Integrated_div_v_n / AreaMajor;
		p_Integrated_div_v_overall[iVertex] = Integrated_div_v_overall;

	//	if (iVertex == CHOSEN) printf(
	//			"Chosen: %d Integrated_div_v_n %1.9E p_div_v_n %1.9E \n",
	//			iVertex, Integrated_div_v_n, p_div_v_n[iVertex]);

		// 3 divisions -- could speed up by creating 1.0/AreaMajor. Except it's bus time anyway.
	} else {
		p_div_v[iVertex] = 0.0;
		p_div_v_n[iVertex] = 0.0;
		p_Integrated_div_v_overall[iVertex] = 0.0;
	};
}*/
__global__ void kernelAccumulateAdvectiveMassHeatRateNew(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBCtri_verts,

	long * __restrict__ p_izNeigh_vert, 

	short * __restrict__ p_who_am_I_to_my_neighbours,

	nvals * __restrict__ p_n_src_major,
	T3 * __restrict__ p_T_src_major,   // use T vertex itself to infer what T to use.

	v4 * __restrict__ p_vie_minor,
	// f64_vec3 * __restrict__ p_v_n_minor,
	f64_vec2 * __restrict__ p_v_overall_minor,
	//T3 * __restrict__ p_T_minor, // may or may not overlap source: don't we only use from tris? so not overlap

	// ShardModel * __restrict__ p_n_shard_n_major,
	ShardModel * __restrict__ p_n_shard_major,
	
	NTrates * __restrict__ p_NTadditionrates,
	f64 * __restrict__ p_div_v,
//	f64 * __restrict__ p_div_v_n, // write ion & electron routine only first; re-do as neutral.
	f64 * __restrict__ p_Integrated_div_v_overall,

	NTrates * __restrict__ p_store_flux
)
{
	// Use the upwind density from tris together with v_tri.
	// Seems to include a factor h

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];   // 4 -- assume we still use this.
	// only reused what, 3 times?  2*2 per major thread
	// do we 

	__shared__ f64_vec2 shared_vxy[threadsPerTileMinor]; // +2*2 per major thread
	// __shared__ f64_vec2 shared_v_n[threadsPerTileMinor]; // +4
	
				// could split routine; no good reason not to.
								 //__shared__ f64_vec2 v_overall[threadsPerTileMinor];
								 // choosing just to load it ad hoc
		
	__shared__ T3 shared_T[threadsPerTileMajor];          // +3  ... = 15 total. Can run 128 threads.

	// Should we just pre-average this on tris to make life easier for ourselves? No, because we also need T_opp.

	__shared__ f64_12 shared_shards[threadsPerTileMajor];

	// probably stick with loading in tri positions if we can.
	// Probably can't manage to run this routine with 256 threads at a time. Can't fit 8 doubles / thread to shared.

	// 24 doubles/thread to get 256 threads so we still need to chuck some out!
	// 48K shared is default.
	// scrap v_n, do neutral in sequence.

	/////////////////////////////////////////////////////////////////////////////
	// Can't store rel v: we use div v of each v in what follows.

	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	{
		structural info[2];
		memcpy(info, p_info_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(structural) * 2);
		shared_pos[2 * threadIdx.x] = info[0].pos;
		shared_pos[2 * threadIdx.x + 1] = info[1].pos;
				
		v4 vie[2];
		memcpy(&vie, p_vie_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(v4) * 2);
		shared_vxy[2 * threadIdx.x] = vie[0].vxy;
		shared_vxy[2 * threadIdx.x + 1] = vie[1].vxy;
		
		//f64_vec3 v_n[2];
		//memcpy(v_n, p_v_n_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(f64_vec3) * 2);
		//shared_v_n[2 * threadIdx.x] = v_n[0].xypart();
		//shared_v_n[2 * threadIdx.x + 1] = v_n[1].xypart();
		
		// memcpy(&(shared_T[2 * threadIdx.x]), p_T_upwind_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(T3) * 2);

		shared_T[threadIdx.x] = p_T_src_major[iVertex];

	}
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const EndMinor = threadsPerTileMinor + StartMinor;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMajor = StartMajor + threadsPerTileMajor;
	
	__syncthreads();

	// What happens for abutting ins?
	// T defined reasonably at insulator-crossing tri, A defined, v defined reasonably

	// Now that we have T from vertices, we'll need to define it on INS tri -- gulp. Just use avg of here and opp.
	
	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

	if ((info.flag == DOMAIN_VERTEX)) {

		// We do not use for Outermost.
		// Trouble is that sometimes a wall is moving and we want outermost to send US the mass.
		// Solution: never shift vertices that lie outside a certain radius.

		memcpy(&(shared_shards[threadIdx.x].n), &(p_n_shard_major[iVertex].n), MAXNEIGH * sizeof(f64));

		// T3 Tsrc = p_T_src_major[iVertex]; // UNUSED!
		nvals nsrc = p_n_src_major[iVertex]; 
				// is this USED?

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		memcpy(izTri, p_izTri + iVertex * MAXNEIGH, sizeof(long) * MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_verts + iVertex*MAXNEIGH, sizeof(char)*MAXNEIGH);
		// Now we are assuming what? Neigh 0 is below tri 0, so 0 1 are on neigh 0
		// Check in debug. Looks true from comments.
		short tri_len = info.neigh_len;

		f64_vec2 edge_normal, endpt0, endpt1;
		f64_vec2 vxy_prev, vxy_next;
		f64_vec2 v_n_prev, v_n_next;
		f64 n_next, n_prev, nn_next, nn_prev;
		f64_vec2 v_overall_prev, v_overall_next;
		f64 Te_next, Te_prev, Ti_next, Ti_prev, Tn_next, Tn_prev;

		short inext, i = 0;
		
		// Initial scenario: use triangle 0 & triangle 1. These face at neighbour 1. prev neigh = 0.

		// Notice that for OUTERMOST we can make no such assumption --- the opposite holds.
		

		long iNeigh = p_izNeigh_vert[iVertex*MAXNEIGH + 0]; 
		if ((iNeigh >= StartMajor) && (iNeigh < EndMajor)) {
			Ti_prev = shared_T[iNeigh - StartMajor].Ti;
			Te_prev = shared_T[iNeigh - StartMajor].Te;
		} else {
			T3 Tload = p_T_src_major[iNeigh];
			Ti_prev = Tload.Ti;
			Te_prev = Tload.Te;
		};
		if (Ti_prev == 0.0) Ti_prev = shared_T[threadIdx.x].Ti;
		if (Te_prev == 0.0) Te_prev = shared_T[threadIdx.x].Te;
		

		long iTri = izTri[0];
		v_overall_prev = p_v_overall_minor[iTri];
		if ((iTri >= StartMinor) && (iTri < EndMinor)) {
			endpt0 = shared_pos[iTri - StartMinor];
			//nvals nvls = shared_n_upwind[iTri - StartMinor];
			//n_prev = nvls.n;
			//nn_prev = nvls.n_n;
			vxy_prev = shared_vxy[iTri - StartMinor];
			//v_n_prev = shared_v_n[iTri - StartMinor];
			//Te_prev = shared_T[iTri - StartMinor].Te;
			//Ti_prev = shared_T[iTri - StartMinor].Ti;
			//Tn_prev = shared_T[iTri - StartMinor].Tn;
			
		} else {
			// The volume of random bus accesses means that we would have been better off making a separate
			// neutral routine even though it looks efficient with the shared loading. nvm
			endpt0 = p_info_minor[iTri].pos;
			//nvals n_upwind = p_n_upwind_minor[iTri];
			//n_prev = n_upwind.n;
			//nn_prev = n_upwind.n_n;
			vxy_prev = p_vie_minor[iTri].vxy;
			//v_n_prev = p_v_n_minor[iTri].xypart();
			//T3 Tuse = p_T_upwind_minor[iTri];
			//Te_prev = Tuse.Te;
			//Ti_prev = Tuse.Ti;
			//Tn_prev = Tuse.Tn;
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
			endpt0 = Clockwise_d*endpt0;
			vxy_prev = Clockwise_d*vxy_prev;
			//v_n_prev = Clockwise_d*v_n_prev;
			v_overall_prev = Clockwise_d*v_overall_prev;
		};
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
			endpt0 = Anticlockwise_d*endpt0;
			vxy_prev = Anticlockwise_d*vxy_prev;
			//v_n_prev = Anticlockwise_d*v_n_prev;
			v_overall_prev = Anticlockwise_d*v_overall_prev;
		};

		// We're going to need position of out vertex?

		nvals totalmassflux_out;
		memset(&totalmassflux_out, 0, sizeof(nvals));
		T3 totalheatflux_out;
		memset(&totalheatflux_out, 0, sizeof(T3));  // we're only going to use ion and electron

		f64 Integrated_div_v = 0.0;
		// f64 Integrated_div_v_n = 0.0;
		f64 Integrated_div_v_overall = 0.0;
		f64 AreaMajor = 0.0;

		f64 Ti_opp, Te_opp;

		iNeigh = p_izNeigh_vert[iVertex*MAXNEIGH + 1]; // neigh 0 is between 0 and 1, neigh -1 is before tri 0.
		if ((iNeigh >= StartMajor) && (iNeigh < EndMajor)) {
			Ti_opp = shared_T[iNeigh - StartMajor].Ti;
			Te_opp = shared_T[iNeigh - StartMajor].Te;
		}
		else {
			T3 Tload = p_T_src_major[iNeigh];
			Ti_opp = Tload.Ti;
			Te_opp = Tload.Te;
		};
		if (Ti_opp == 0.0) Ti_opp = shared_T[threadIdx.x].Ti;
		if (Te_opp == 0.0) Te_opp = shared_T[threadIdx.x].Te;
		

#pragma unroll MAXNEIGH
		for (i = 0; i < tri_len; i++)
		{
			inext = i + 1; if (inext == tri_len) inext = 0; // i,inext are the triangle indices

			// Let's assume inext is the index of iNeigh but we should spit out lists to check this.
			short inext2 = inext + 1; if (inext2 == tri_len) inext2 = 0;

			long iNeighNext = p_izNeigh_vert[iVertex*MAXNEIGH + inext2]; // neigh0 is between 0 and 1, neigh -1 is before tri 0.
			if ((iNeighNext >= StartMajor) && (iNeighNext < EndMajor)) {
				Ti_next = shared_T[iNeighNext - StartMajor].Ti;
				Te_next = shared_T[iNeighNext - StartMajor].Te;
			}
			else {
				T3 Tload = p_T_src_major[iNeighNext]; // it's actually use not src.
				Ti_next = Tload.Ti;
				Te_next = Tload.Te;
			};
			if (Ti_next == 0.0) Ti_next = shared_T[threadIdx.x].Ti;
			if (Te_next == 0.0) Te_next = shared_T[threadIdx.x].Te;
			

			long iTri = izTri[inext];
			f64_vec2 v_overall_next = p_v_overall_minor[iTri];

			if ((iTri >= StartMinor) && (iTri < EndMinor)) {
				endpt1 = shared_pos[iTri - StartMinor];
				
				vxy_next = shared_vxy[iTri - StartMinor];

				// We are going to need a separate attempt to get at T from vertices, to use in Simpson.
				
			} else {
				// The volume of random bus accesses means that we would have been better off making a separate
				// neutral routine even though it looks efficient with the shared loading. nvm
				endpt1 = p_info_minor[iTri].pos;

				vxy_next = p_vie_minor[iTri].vxy;
			};
			
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
				endpt1 = Clockwise_d*endpt1;
				vxy_next = Clockwise_d*vxy_next;
				v_n_next = Clockwise_d*v_n_next;
				v_overall_next = Clockwise_d*v_overall_next;
			};
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
				endpt1 = Anticlockwise_d*endpt1;
				vxy_next = Anticlockwise_d*vxy_next;
				v_n_next = Anticlockwise_d*v_n_next;
				v_overall_next = Anticlockwise_d*v_overall_next;
			};

			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			AreaMajor += 0.5*edge_normal.x*(endpt0.x + endpt1.x);

			//	if (iVertex == CHOSEN) printf("GPU %d : AreaMajor %1.9E edge_nml.x %1.6E endpt0.x %1.6E endpt1.x %1.6E \n",
			//		iVertex,
			//		AreaMajor, edge_normal.x, endpt0.x, endpt1.x);

			n_prev = shared_shards[threadIdx.x].n[i];
			n_next = shared_shards[threadIdx.x].n[inext];
			

			// Totally smash up the runtime:
			char neighflag = p_info_minor[iNeigh + BEGINNING_OF_CENTRAL].flag;

			if ((neighflag == DOMAIN_VERTEX) ||
				((info.flag == DOMAIN_VERTEX) && (neighflag == OUTERMOST)))
			{
				// Note: changes of cell area looking into ins / cath are not valid changes of area.


				if ((n_prev != 0.0) && (n_next != 0.0)) {

					Integrated_div_v_overall += 0.5*(v_overall_prev + v_overall_next).dot(edge_normal); // Average outward velocity of edge...
																										// The area CAN be changing because of other vertices dragging on it.
																										// However we can ignore it as n,T should be locally constant near the rod anyway.
					Integrated_div_v += 0.5*(vxy_prev + vxy_next).dot(edge_normal);

					f64 prev_relv = (vxy_prev - v_overall_prev).dot(edge_normal);
					f64 next_relv = (vxy_next - v_overall_next).dot(edge_normal);

					if (iVertex == VERTCHOSEN) printf("%d : i = %d %d , contrib to integ div v overall %1.9E v_overall prev %1.9E %1.9E\n"
						"prev next dot normal %1.9E %1.9E\n",
						iVertex, i, iNeigh, 0.5*(v_overall_prev + v_overall_next).dot(edge_normal),
						v_overall_prev.x, v_overall_prev.y, v_overall_prev.dot(edge_normal),
						v_overall_next.dot(edge_normal));

					// Insulator: If 1 or 2 of the vertices comes out at T=0 then what?
					// Fill in with our own value. Upwind.
					// But absolutely ensure we are not looking out of domain at vertex! And we are throwing away anything that flowed into OUTERMOST. Too bad about that, leave it.
					// Without loading info for the vertex we look at, we do not know if it's out of domain. So we have to rely on vr=0 at insulator.

					// upwind? :
					if (prev_relv + next_relv > 0.0) {

						// For now:
						f64 Ti_prevavg = THIRD*(shared_T[threadIdx.x].Ti + Ti_prev + Ti_opp);
						f64 Ti_nextavg = THIRD*(shared_T[threadIdx.x].Ti + Ti_next + Ti_opp);
						f64 Te_prevavg = THIRD*(shared_T[threadIdx.x].Te + Te_prev + Te_opp);
						f64 Te_nextavg = THIRD*(shared_T[threadIdx.x].Te + Te_next + Te_opp);

						// LIMIT flux T to 2*ours.

						if (Ti_prevavg > 2.0*shared_T[threadIdx.x].Ti) Ti_prevavg = 2.0*shared_T[threadIdx.x].Ti;
						if (Ti_nextavg > 2.0*shared_T[threadIdx.x].Ti) Ti_nextavg = 2.0*shared_T[threadIdx.x].Ti;
						if (Te_prevavg > 2.0*shared_T[threadIdx.x].Te) Te_prevavg = 2.0*shared_T[threadIdx.x].Te;
						if (Te_nextavg > 2.0*shared_T[threadIdx.x].Te) Te_nextavg = 2.0*shared_T[threadIdx.x].Te;

						f64 Ti_avg = 0.5*(shared_T[threadIdx.x].Ti + Ti_opp);
						f64 Te_avg = 0.5*(shared_T[threadIdx.x].Te + Te_opp);
						if (Ti_avg > 2.0*shared_T[threadIdx.x].Ti) Ti_avg = 2.0*shared_T[threadIdx.x].Ti;
						if (Te_avg > 2.0*shared_T[threadIdx.x].Te) Te_avg = 2.0*shared_T[threadIdx.x].Te;

						NTrates NTaddn;
						memset(&NTaddn, 0, sizeof(NTrates));

						NTaddn.N = 0.25*n_prev*prev_relv + 0.25*n_next*next_relv
							+ 0.5*0.25*(n_prev + n_next)*(prev_relv + next_relv);
						NTaddn.NiTi = 0.25*n_prev*Ti_prevavg*prev_relv + 0.25*n_next*Ti_nextavg*next_relv
							+ 0.125*(n_prev + n_next)*Ti_avg*(prev_relv + next_relv);
						NTaddn.NeTe = 0.25*n_prev*Te_prevavg*prev_relv + 0.25*n_next*Te_nextavg*next_relv
							+ 0.125*(n_prev + n_next)*Te_avg*(prev_relv + next_relv);

						totalmassflux_out.n += NTaddn.N;
						totalheatflux_out.Ti += NTaddn.NiTi;
						totalheatflux_out.Te += NTaddn.NeTe;


						// Maybe there's a speedup we can use.
						// Now save to downwind cell:
						short who_am_I = p_who_am_I_to_my_neighbours[iVertex*MAXNEIGH + inext];

						memcpy(&(p_store_flux[iNeigh*MAXNEIGH + who_am_I]), &(NTaddn), sizeof(NTrates));

						// NOTE WE DID N*O*T ADD A MINUS.

						if (((iNeigh == VERTCHOSEN) || (iVertex == VERTCHOSEN)) && (TEST_ADV_HEAT_FLAG)) {
							printf("iVertex %d NTaddn.NiTi %1.9E n_prev %1.9E Ti_prevavg %1.9E prev_relv %1.9E \n"
								"n_next %1.9E Ti_nextavg %1.9E next_relv %1.9E Ti_avg %1.9E \n"
								"totalheatflux_out.Ti %1.9E i %d iNeigh %d who_am_I %d\n"
								"Ti_ours %1.8E Ti_opp %1.8E \n"
								"---------------------------------------------------\n",
								iVertex, NTaddn.NiTi, n_prev, Ti_prevavg, prev_relv, n_next, Ti_nextavg, next_relv,
								Ti_avg, totalheatflux_out.Ti, i, iNeigh, who_am_I,
								shared_T[threadIdx.x].Ti, Ti_opp);

						}
						if (((iNeigh == VERTCHOSEN) || (iVertex == VERTCHOSEN)) && (TEST_ADV_MASS_FLAG)) {
							printf("iVertex %d iNeigh %d massflux_out %1.9E NTaddn.N %1.9E  \n"
								"n_prev n_next %1.9E %1.9E prev_relv next_relv %1.9E %1.9E \n"
								"vxy_prev %1.8E %1.8E vxy_next %1.9E %1.9E edge_normal %1.8E %1.8E \n"
								"v_overall prev %1.8E %1.8E next %1.8E %1.8E \n&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
								iVertex, iNeigh, totalmassflux_out.n, NTaddn.N,
								n_prev, n_next, prev_relv, next_relv,
								vxy_prev.x, vxy_prev.y, vxy_next.x, vxy_next.y, edge_normal.x, edge_normal.y,
								v_overall_prev.x, v_overall_prev.y, v_overall_next.x, v_overall_next.y);
						};
					}
					else {
						// downwind cell: collect flux later.
					}

				};
			};

			endpt0 = endpt1;
			vxy_prev = vxy_next;
			v_overall_prev = v_overall_next;
			
			Ti_prev = Ti_opp;
			Ti_opp = Ti_next;
			Te_prev = Te_opp;
			Te_opp = Te_next;
			iNeigh = iNeighNext;
		};

		NTrates NTplus;

		NTplus.N = -totalmassflux_out.n;
		NTplus.Nn = 0.0; // -totalmassflux_out.n_n;
		NTplus.NeTe = -totalheatflux_out.Te;
		NTplus.NiTi = -totalheatflux_out.Ti;
		NTplus.NnTn = 0.0; // -totalheatflux_out.Tn;


		//
		//		if (TEST) printf("\n%d : NTplus.NiTi %1.10E NTplus.N %1.10E Tsrc.i %1.10E nsrc.n %1.10E\n"
		//			"NTplus.NiTi/NTplus.N (avg temp of those coming/going) %1.10E\n"
		//			"NTplus.NiTi/N (ROC Ti) %1.10E\n"
		//			"NTplus.NiTi/NiTi (elasticity of T?) %1.10E \n"
		//			"NTplus.N/N (elasticity of N) %1.10E \n\n",
		//			CHOSEN, NTplus.NiTi, NTplus.N,
		//			Tsrc.Ti, nsrc.n,
		//			NTplus.NiTi/NTplus.N,
		//			NTplus.NiTi/(AreaMajor*nsrc.n),
		//			NTplus.NiTi/(AreaMajor*nsrc.n*Tsrc.Ti),
		//			NTplus.N/(AreaMajor*nsrc.n)
		//			);

		memcpy(p_NTadditionrates + iVertex, &NTplus, sizeof(NTrates));
		// ROUTINE MUST BE CALLED FIRST - WE ZEROED OUT NEUTRAL DATA.

		// What we need now: 
		//	* Cope with non-domain vertex
		p_div_v[iVertex] = Integrated_div_v / AreaMajor;
		p_Integrated_div_v_overall[iVertex] = Integrated_div_v_overall;
	}
	else {
		p_div_v[iVertex] = 0.0;
		p_Integrated_div_v_overall[iVertex] = 0.0;
	};
}


__global__ void kernelAccumulateNeutralAdvectiveMassHeatRateNew(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBCtri_verts,
	long * __restrict__ p_izNeigh_vert,
	short * __restrict__ p_who_am_I_to_my_neighbours,

	nvals * __restrict__ p_n_src_major,
	T3 * __restrict__ p_T_src_major,   // use T vertex itself to infer what T to use.

	f64_vec3 * __restrict__ p_v_n_minor,
	f64_vec2 * __restrict__ p_v_overall_minor,
	ShardModel * __restrict__ p_n_shard_major,

	NTrates * __restrict__ p_NTadditionrates,
	f64 * __restrict__ p_div_v_n, 
	NTrates * __restrict__ p_store_flux
)
{
	// Use the upwind density from tris together with v_tri.
	// Seems to include a factor h

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];   // 4 -- assume we still use this.
														   // only reused what, 3 times?  2*2 per major thread
														   // do we 

	__shared__ f64_vec2 shared_vxy[threadsPerTileMinor]; // +2*2 per major thread
														 // __shared__ f64_vec2 shared_v_n[threadsPerTileMinor]; // +4

														 // could split routine; no good reason not to.
														 //__shared__ f64_vec2 v_overall[threadsPerTileMinor];
														 // choosing just to load it ad hoc

	__shared__ f64 shared_T[threadsPerTileMajor];          // +3  ... = 15 total. Can run 128 threads.

														  // Should we just pre-average this on tris to make life easier for ourselves? No, because we also need T_opp.

	__shared__ f64_12 shared_shards[threadsPerTileMajor];

	// probably stick with loading in tri positions if we can.
	// Probably can't manage to run this routine with 256 threads at a time. Can't fit 8 doubles / thread to shared.

	// 24 doubles/thread to get 256 threads so we still need to chuck some out!
	// 48K shared is default.
	// scrap v_n, do neutral in sequence.

	/////////////////////////////////////////////////////////////////////////////
	// Can't store rel v: we use div v of each v in what follows.

	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	{
		structural info[2];
		memcpy(info, p_info_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(structural) * 2);
		shared_pos[2 * threadIdx.x] = info[0].pos;
		shared_pos[2 * threadIdx.x + 1] = info[1].pos;

		f64_vec3 v_n[2];
		memcpy(&v_n, p_v_n_minor + (threadsPerTileMinor*blockIdx.x + 2 * threadIdx.x), sizeof(f64_vec3) * 2);
		shared_vxy[2 * threadIdx.x] = v_n[0].xypart();
		shared_vxy[2 * threadIdx.x + 1] = v_n[1].xypart();
		
		shared_T[threadIdx.x] = p_T_src_major[iVertex].Tn;

	}
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const EndMinor = threadsPerTileMinor + StartMinor;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	__syncthreads();

	// What happens for abutting ins?
	// T defined reasonably at insulator-crossing tri, A defined, v defined reasonably

	// Now that we have T from vertices, we'll need to define it on INS tri -- gulp. Just use avg of here and opp.

	structural info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

	if ((info.flag == DOMAIN_VERTEX)) {
		// see above -- no shift vertex outside certain radius

		memcpy(&(shared_shards[threadIdx.x].n), &(p_n_shard_major[iVertex].n), MAXNEIGH * sizeof(f64));

		// T3 Tsrc = p_T_src_major[iVertex]; // UNUSED!
		nvals nsrc = p_n_src_major[iVertex];
		// is this USED?

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		memcpy(izTri, p_izTri + iVertex * MAXNEIGH, sizeof(long) * MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_verts + iVertex*MAXNEIGH, sizeof(char)*MAXNEIGH);
		// Now we are assuming what? Neigh 0 is below tri 0, so 0 1 are on neigh 0
		// Check in debug. Looks true from comments.
		short tri_len = info.neigh_len;

		f64_vec2 edge_normal, endpt0, endpt1;
		f64_vec2 vxy_prev, vxy_next;
		f64_vec2 v_n_prev, v_n_next;
		f64 n_next, n_prev, nn_next, nn_prev;
		f64_vec2 v_overall_prev, v_overall_next;
		f64 T_next, T_prev;

		short inext, i = 0;

		// Initial scenario: use triangle 0 & triangle 1. These face at neighbour 1. prev neigh = 0.

		long iNeigh = p_izNeigh_vert[iVertex*MAXNEIGH + 0];
		if ((iNeigh >= StartMajor) && (iNeigh < EndMajor)) {
			T_prev = shared_T[iNeigh - StartMajor];
		} else {
			T_prev = p_T_src_major[iNeigh].Tn;			
		};
		if (T_prev == 0.0) T_prev = shared_T[threadIdx.x];


		long iTri = izTri[0];
		v_overall_prev = p_v_overall_minor[iTri];
		if ((iTri >= StartMinor) && (iTri < EndMinor)) {
			endpt0 = shared_pos[iTri - StartMinor];
			//nvals nvls = shared_n_upwind[iTri - StartMinor];
			//n_prev = nvls.n;
			//nn_prev = nvls.n_n;
			vxy_prev = shared_vxy[iTri - StartMinor];
			//v_n_prev = shared_v_n[iTri - StartMinor];
			//Te_prev = shared_T[iTri - StartMinor].Te;
			//Ti_prev = shared_T[iTri - StartMinor].Ti;
			//Tn_prev = shared_T[iTri - StartMinor].Tn;

		}
		else {
			// The volume of random bus accesses means that we would have been better off making a separate
			// neutral routine even though it looks efficient with the shared loading. nvm
			endpt0 = p_info_minor[iTri].pos;
			//nvals n_upwind = p_n_upwind_minor[iTri];
			//n_prev = n_upwind.n;
			//nn_prev = n_upwind.n_n;
			vxy_prev = p_v_n_minor[iTri].xypart();
			//v_n_prev = p_v_n_minor[iTri].xypart();
			//T3 Tuse = p_T_upwind_minor[iTri];
			//Te_prev = Tuse.Te;
			//Ti_prev = Tuse.Ti;
			//Tn_prev = Tuse.Tn;
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
			endpt0 = Clockwise_d*endpt0;
			vxy_prev = Clockwise_d*vxy_prev;
			//v_n_prev = Clockwise_d*v_n_prev;
			v_overall_prev = Clockwise_d*v_overall_prev;
		};
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
			endpt0 = Anticlockwise_d*endpt0;
			vxy_prev = Anticlockwise_d*vxy_prev;
			//v_n_prev = Anticlockwise_d*v_n_prev;
			v_overall_prev = Anticlockwise_d*v_overall_prev;
		};

		// We're going to need position of out vertex?

		f64 totalmassflux_out;
		memset(&totalmassflux_out, 0, sizeof(f64));
		f64 totalheatflux_out;
		memset(&totalheatflux_out, 0, sizeof(f64));

		f64 Integrated_div_v = 0.0;
		f64 AreaMajor = 0.0;
		f64 T_opp;

		iNeigh = p_izNeigh_vert[iVertex*MAXNEIGH + 1]; // neigh0 is between 0 and 1, neigh -1 is before tri 0.
		if ((iNeigh >= StartMajor) && (iNeigh < EndMajor)) {
			T_opp = shared_T[iNeigh - StartMajor];
		} else {
			T_opp = p_T_src_major[iNeigh].Tn;
		};
		if (T_opp == 0.0) T_opp = shared_T[threadIdx.x];
		
#pragma unroll MAXNEIGH
		for (i = 0; i < tri_len; i++)
		{
			inext = i + 1; if (inext == tri_len) inext = 0; // i,inext are the triangle indices
			// Let's assume inext is the index of iNeigh but we should spit out lists to check this.
			short inext2 = inext + 1; if (inext2 == tri_len) inext2 = 0;

			long iNeighNext = p_izNeigh_vert[iVertex*MAXNEIGH + inext2]; // neigh0 is between 0 and 1, neigh -1 is before tri 0.
			if ((iNeighNext >= StartMajor) && (iNeighNext < EndMajor)) {
				T_next = shared_T[iNeighNext - StartMajor];
			}
			else {
				T_next = p_T_src_major[iNeighNext].Tn; // it's actually use not src.
			};
			if (T_next == 0.0) T_next = shared_T[threadIdx.x];
			
			long iTri = izTri[inext];
			f64_vec2 v_overall_next = p_v_overall_minor[iTri];

			if ((iTri >= StartMinor) && (iTri < EndMinor)) {
				endpt1 = shared_pos[iTri - StartMinor];
				
				vxy_next = shared_vxy[iTri - StartMinor];
				//		Te_next = shared_T[iTri - StartMinor].Te;
				//		Ti_next = shared_T[iTri - StartMinor].Ti;
				//		Tn_next = shared_T[iTri - StartMinor].Tn;

				// We are going to need a separate attempt to get at T from vertices, to use in Simpson.
			} else {
				// The volume of random bus accesses means that we would have been better off making a separate
				// neutral routine even though it looks efficient with the shared loading. nvm
				endpt1 = p_info_minor[iTri].pos;
				vxy_next = p_v_n_minor[iTri].xypart();				
			};

			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
				endpt1 = Clockwise_d*endpt1;
				vxy_next = Clockwise_d*vxy_next;
				v_overall_next = Clockwise_d*v_overall_next;
			};
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
				endpt1 = Anticlockwise_d*endpt1;
				vxy_next = Anticlockwise_d*vxy_next;
				v_overall_next = Anticlockwise_d*v_overall_next;
			};

			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			AreaMajor += 0.5*edge_normal.x*(endpt0.x + endpt1.x);

			//	if (iVertex == CHOSEN) printf("GPU %d : AreaMajor %1.9E edge_nml.x %1.6E endpt0.x %1.6E endpt1.x %1.6E \n",
			//		iVertex,
			//		AreaMajor, edge_normal.x, endpt0.x, endpt1.x);

			n_prev = shared_shards[threadIdx.x].n[i];
			n_next = shared_shards[threadIdx.x].n[inext];

			// Totally smash up the runtime:
			char neighflag = p_info_minor[iNeigh + BEGINNING_OF_CENTRAL].flag;

			if ((neighflag == DOMAIN_VERTEX) ||
				((info.flag == DOMAIN_VERTEX) && (neighflag == OUTERMOST)))
			{
				if ((n_prev != 0.0) && (n_next != 0.0)) {

					Integrated_div_v += 0.5*(vxy_prev + vxy_next).dot(edge_normal);

					f64 prev_relv = (vxy_prev - v_overall_prev).dot(edge_normal);
					f64 next_relv = (vxy_next - v_overall_next).dot(edge_normal);

					// Insulator: If 1 or 2 of the vertices comes out at T=0 then what?
					// Fill in with our own value. Upwind.
					// But absolutely ensure we are not looking out of domain at vertex! And we are throwing away anything that flowed into OUTERMOST. Too bad about that, leave it.
					// Without loading info for the vertex we look at, we do not know if it's out of domain. So we have to rely on vr=0 at insulator.

					// upwind? :
					if (prev_relv + next_relv > 0.0) {

						// Note: you are not upwind for both neutrals & ions necessarily.

						// For now:
						f64 T_prevavg = THIRD*(shared_T[threadIdx.x] + T_prev + T_opp);
						f64 T_nextavg = THIRD*(shared_T[threadIdx.x] + T_next + T_opp);

						// LIMIT flux T to 2*ours.

						if (T_prevavg > 2.0*shared_T[threadIdx.x]) T_prevavg = 2.0*shared_T[threadIdx.x];
						if (T_nextavg > 2.0*shared_T[threadIdx.x]) T_nextavg = 2.0*shared_T[threadIdx.x];

						f64 T_avg = 0.5*(shared_T[threadIdx.x] + T_opp);
						if (T_avg > 2.0*shared_T[threadIdx.x]) T_avg = 2.0*shared_T[threadIdx.x];

						f64 NTaddnN, NTaddnNT;

						NTaddnN = 0.25*n_prev*prev_relv + 0.25*n_next*next_relv
							+ 0.5*0.25*(n_prev + n_next)*(prev_relv + next_relv);
						NTaddnNT = 0.25*n_prev*T_prevavg*prev_relv + 0.25*n_next*T_nextavg*next_relv
							+ 0.125*(n_prev + n_next)*T_avg*(prev_relv + next_relv);

						totalmassflux_out += NTaddnN;
						totalheatflux_out += NTaddnNT;

						// Maybe there's a speedup we can use.
						// Now save to downwind cell:
						short who_am_I = p_who_am_I_to_my_neighbours[iVertex*MAXNEIGH + inext];

						p_store_flux[iNeigh*MAXNEIGH + who_am_I].Nn = NTaddnN;
						p_store_flux[iNeigh*MAXNEIGH + who_am_I].NnTn = NTaddnNT;

						// NOTE WE DID N*O*T ADD A MINUS.

					//	if (((iNeigh == VERTCHOSEN) || (iVertex == VERTCHOSEN)) && (TEST_ADV_MASS_FLAG)) {
					//		printf("iVertex %d iNeigh %d massflux_out %1.9E NTaddn.Nn %1.9E  \n"
					//			"nn_prev nn_next %1.9E %1.9E prev_relv next_relv %1.9E %1.9E \n"
					//			"vxy_prev %1.8E %1.8E vxy_next %1.9E %1.9E edge_normal %1.8E %1.8E \n"
					//			"v_overall prev %1.8E %1.8E next %1.8E %1.8E who_am_I %d \n&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n",
					//			iVertex, iNeigh, totalmassflux_out, NTaddnN,
					//			n_prev, n_next, prev_relv, next_relv,
					//			vxy_prev.x, vxy_prev.y, vxy_next.x, vxy_next.y, edge_normal.x, edge_normal.y,
					//			v_overall_prev.x, v_overall_prev.y, v_overall_next.x, v_overall_next.y, who_am_I);
					//	};
					}
					else {
						// downwind cell: collect flux later.
					};
				};
			};

			endpt0 = endpt1;
			vxy_prev = vxy_next;
			v_overall_prev = v_overall_next;

			T_prev = T_opp;
			T_opp = T_next;
			iNeigh = iNeighNext;
		};

		p_NTadditionrates[iVertex].Nn = -totalmassflux_out;
		p_NTadditionrates[iVertex].NnTn = -totalheatflux_out;

		p_div_v_n[iVertex] = Integrated_div_v / AreaMajor;
	}
	else {
		p_div_v_n[iVertex] = 0.0;
	};
}


__global__ void kernelCreateLinearRelationship(
	f64 const h_use,
	structural * __restrict__ p_info,
	OhmsCoeffs* __restrict__ p_Ohms,
	v4 * __restrict__ p_v0,
	f64 * __restrict__ p_Lap_Az_use,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_denom_e,
	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_coeff_of_vez_upon_viz,
	f64 * __restrict__ p_beta_ie_z,
	AAdot * __restrict__ p_AAdot_intermediate,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_Azdot0,
	f64 * __restrict__ p_gamma
)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 const Lap_Az_used = p_Lap_Az_use[iMinor];
	structural const info = p_info[iMinor];

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX))
	{
		v4 v0 = p_v0[iMinor];
		// Cancel the part that was added in order to get at Ez_strength:

		f64 denom_e = p_denom_e[iMinor];
		f64 denom_i = p_denom_i[iMinor];

		if (((TESTTRI)) && (0)) printf("\nv0.vez before remove Lapcontrib %1.14E \n", v0.vez);

		v0.viz += 0.5*qoverM*h_use*h_use* c* Lap_Az_used / denom_i; // adaptation for this.
		f64 coeff_of_vez_upon_viz = p_coeff_of_vez_upon_viz[iMinor];

		f64 cancel_from_vez = -0.5*eoverm*h_use*h_use* c* Lap_Az_used / denom_e
			+ coeff_of_vez_upon_viz * 0.5*qoverM*h_use*h_use* c* Lap_Az_used / denom_i;

		v0.vez += cancel_from_vez;
		f64 beta_ie_z = p_beta_ie_z[iMinor];
		v0.viz += beta_ie_z * cancel_from_vez;

		if (((TESTTRI)) && (0)) printf("\n##############\nviz before remove LapAzcontrib %1.14E Lapcontrib %1.14E \n\n",
			v0.viz - 0.5*qoverM*h_use*h_use* c* Lap_Az_used / denom_i,
			-0.5*qoverM*h_use*h_use* c* Lap_Az_used / denom_i
		);

		// Inadequate because we need to take account of the effect of Lap Az on vez0 via viz0.

		// We see now that re-jigging things is absolutely not what we should have done.
		// It will make the most complicated overspilling routine, more complicated still.
		if (((TESTTRI)) && (0)) printf("own part of effect (we cancel): %1.14E \n"
			"via viz (we cancel): coeff %1.14E vizeffect %1.14E\n",
			0.5*eoverm*h_use*h_use* c* Lap_Az_used / denom_e,
			coeff_of_vez_upon_viz,
			-0.5*qoverM*h_use*h_use* c* Lap_Az_used / denom_i);

		if (((TESTTRI)) && (0)) printf("v0.vez after remove Lapcontrib %1.14E \n", v0.vez);
		OhmsCoeffs Ohms = p_Ohms[iMinor];

		f64 vez_1 = v0.vez + Ohms.sigma_e_zz * Ez_strength;
		f64 viz_1 = v0.viz + Ohms.sigma_i_zz * Ez_strength;

		if (((TESTTRI)) && (0)) printf("vez_1 with Ezcontrib %1.14E sigma_e_zz %1.14E Ez %1.14E vizeffect %1.14E \n", vez_1,
			Ohms.sigma_e_zz, Ez_strength, Ohms.sigma_i_zz * Ez_strength);

		// Cancelled Lap contrib from vez1 here.
		// Be sure we know that makes sense. Is that what we missed on CPU?

		nvals n_use = p_n_minor[iMinor];

		//	AAzdot_k.Azdot +=
		//	  h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
		//		0.5*FOURPI_OVER_C * q*n_use.n*(vie_k.viz - vie_k.vez)); // INTERMEDIATE
		//		p_AAdot_intermediate[iMinor] = AAzdot_k; // not k any more
#ifdef MIDPT_A	
		p_Azdot0[iMinor] = p_AAdot_intermediate[iMinor].Azdot
			- 0.5*h_use*c*c*Lap_Az_used // cancel out half what PopOhms did!
										// + h_use * ROCAzdot_antiadvect[iMinor]   // we did this as part of PopOhms.
										// + h_use *c*2.0*PI* q*n_use.n*(v_src.viz - v_src.vez) // we did this as part of PopOhms
			+ h_use *c*2.0*M_PI* q*n_use.n*(viz_1 - vez_1);

		// HALVED:
		f64 viz0_coeff_on_Lap_Az = -0.25*h_use*h_use*qoverM*c / denom_i;
		f64 vez0_coeff_on_Lap_Az = 0.25* h_use*h_use*eoverm*c / denom_e
			+ coeff_of_vez_upon_viz*viz0_coeff_on_Lap_Az;

#else 
		p_Azdot0[iMinor] = p_AAdot_intermediate[iMinor].Azdot
			- h_use*c*c*Lap_Az_used // cancel out what PopOhms did!
									// + h_use * ROCAzdot_antiadvect[iMinor]   // we did this as part of PopOhms.
									// + h_use *c*2.0*PI* q*n_use.n*(v_src.viz - v_src.vez) // we did this as part of PopOhms
			+ h_use *c*2.0*M_PI* q*n_use.n*(viz_1 - vez_1);

		f64 viz0_coeff_on_Lap_Az = -0.5*h_use*h_use*qoverM*c / denom_i;
		f64 vez0_coeff_on_Lap_Az = 0.5* h_use*h_use*eoverm*c / denom_e
			+ coeff_of_vez_upon_viz*viz0_coeff_on_Lap_Az;
#endif

		viz0_coeff_on_Lap_Az += beta_ie_z*vez0_coeff_on_Lap_Az;

		if (((TESTTRI)) && (0)) printf("vez0_coeff_on_Lap undivided %1.14E coeff_viz_on_vez %1.14E viz0_coeff %1.14E denom_e %1.14E\n",
			0.5* h_use*h_use*eoverm*c,
			coeff_of_vez_upon_viz,
			viz0_coeff_on_Lap_Az,
			denom_e
		);
#ifdef MIDPT_A
		p_gamma[iMinor] = h_use*c*c*(0.5 + 0.5*FOURPI_OVER_C * q*n_use.n*
			(viz0_coeff_on_Lap_Az - vez0_coeff_on_Lap_Az));
#else
		p_gamma[iMinor] = h_use*c*c*(1.0 + 0.5*FOURPI_OVER_C * q*n_use.n*
			(viz0_coeff_on_Lap_Az - vez0_coeff_on_Lap_Az));
#endif

		// This represents the effect on Azdot of LapAz. 
		// Did we get this wrong for CPU also?

		if (((TESTTRI)) && (0)) {
			printf("kernelCLR %d: Azdot_intermed %1.14E Lap_Az_used %1.14E Lapcontrib cancel %1.14E Azdot0 %1.14E\n",
				CHOSEN, p_AAdot_intermediate[iMinor].Azdot, Lap_Az_used,
				-h_use*c*c*Lap_Az_used,
				p_Azdot0[iMinor]);
			printf("Jcontrib1 %1.14E viz1 %1.14E vez1 %1.14E\n",
				h_use *c*2.0*M_PI* q*n_use.n*(viz_1 - vez_1),
				viz_1, vez_1);
			printf("gamma %1.14E components: n %1.14E viz0coeff %1.14E vez0coeff %1.14E",
				p_gamma[iMinor],
				n_use.n, viz0_coeff_on_Lap_Az, vez0_coeff_on_Lap_Az);

		}
	}
	else {
		// In PopOhms:
		// AAdot temp = p_AAdot_src[iMinor];
		// temp.Azdot += h_use * c*(c*p_LapAz[iMinor] 
		// NO: + 4.0*PI*Jz);
		// p_AAdot_intermediate[iMinor] = temp; // 

		// We need to do the same sort of thing here as in CalcVelocityAzdot :

		f64 Jz = 0.0;
		
		//if ((iMinor >= numStartZCurrentTriangles) && (iMinor < numEndZCurrentTriangles))
		if (info.flag == REVERSE_JZ_TRI)
		{
			// We should find a way to set these to exactly what we need for it to work,
			// at TriMesh::Initialise and then propagated through the Invoke function.

			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;
		}

#ifdef MIDPT_A
		p_Azdot0[iMinor] = p_AAdot_intermediate[iMinor].Azdot - h_use*0.5*c*c*Lap_Az_used
			+ h_use*c*FOUR_PI*Jz;
		p_gamma[iMinor] = h_use*0.5 * c*c;
#else
		p_Azdot0[iMinor] = p_AAdot_intermediate[iMinor].Azdot - h_use*c*c*Lap_Az_used
			+ h_use*c*FOUR_PI*Jz;
		p_gamma[iMinor] = h_use * c*c;
#endif	

		if ((info.flag == INNER_FRILL) || (info.flag == OUTER_FRILL))
		{
			p_Azdot0[iMinor] = 0.0; // difference found? But we did set = 0 on CPU.
			p_gamma[iMinor] = 0.0;
		}

		if (((TESTTRI)) && (0)) printf("kernelCLR %d: Azdot_intermed %1.14E Lap_Az_used %1.14E Azdot0 %1.14E\n",
			CHOSEN, p_AAdot_intermediate[iMinor].Azdot, Lap_Az_used, p_Azdot0[iMinor]);
		// Note that for frills these will simply not be used.
	};
}

__global__ void kernelCreateLinearRelationshipBwd(
	f64 const h_use,
	structural * __restrict__ p_info,
	OhmsCoeffs* __restrict__ p_Ohms,
	v4 * __restrict__ p_v0,
	f64 * __restrict__ p_Lap_Az_use,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_denom_e,
	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_coeff_of_vez_upon_viz,
	f64 * __restrict__ p_beta_ie_z,
	AAdot * __restrict__ p_AAdot_k,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_Azdot0,
	f64 * __restrict__ p_gamma,
	f64 * __restrict__ ROCAzdotduetoAdvection
)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 const Lap_Az_used = p_Lap_Az_use[iMinor];
	structural const info = p_info[iMinor];
	
	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
	{
		v4 v0 = p_v0[iMinor];

		// Cancel the part that was added in order to get at Ez_strength:
		
		f64 denom_e = p_denom_e[iMinor];
		f64 denom_i = p_denom_i[iMinor];
		
		v0.viz += qoverM*h_use*h_use* c* Lap_Az_used / denom_i; // adaptation for this.

		f64 coeff_of_vez_upon_viz = p_coeff_of_vez_upon_viz[iMinor];
		
		f64 cancel_from_vez = -eoverm*h_use*h_use* c* Lap_Az_used / denom_e
						+ coeff_of_vez_upon_viz * qoverM*h_use*h_use* c* Lap_Az_used / denom_i;

		v0.vez += cancel_from_vez;
		f64 beta_ie_z = p_beta_ie_z[iMinor];
		v0.viz += beta_ie_z * cancel_from_vez;
		
		// We see now that re-jigging things is absolutely not what we should have done.

		// It will make the most complicated overspilling routine, more complicated still.
		
		OhmsCoeffs Ohms = p_Ohms[iMinor];
		
		f64 vez_1 = v0.vez + Ohms.sigma_e_zz * Ez_strength;
		f64 viz_1 = v0.viz + Ohms.sigma_i_zz * Ez_strength;
		
		nvals n_use = p_n_minor[iMinor];		
		p_Azdot0[iMinor] = p_AAdot_k[iMinor].Azdot
						+ h_use * ROCAzdotduetoAdvection[iMinor] // our prediction contains this
						+ h_use *c*4.0*M_PI* q*n_use.n*(viz_1 - vez_1);
		
		// ROCAzdot_antiadvect --- we need this to be in there only

		// on cycles that we do advection
		
		// So do the addition in here.
		
		f64 viz0_coeff_on_Lap_Az = h_use*h_use*qoverM*c / denom_i;

		f64 vez0_coeff_on_Lap_Az = h_use*h_use*eoverm*c / denom_e
					+ coeff_of_vez_upon_viz*viz0_coeff_on_Lap_Az;
		
		viz0_coeff_on_Lap_Az += beta_ie_z*vez0_coeff_on_Lap_Az;

		p_gamma[iMinor] = h_use*c*c*(1.0 + FOURPI_OVER_C * q*n_use.n*
					(viz0_coeff_on_Lap_Az - vez0_coeff_on_Lap_Az));
		
	} else {

		// We need to do the same sort of thing here as in CalcVelocityAzdot :
		
		f64 Jz = 0.0;
		
		//if ((iMinor >= numStartZCurrentTriangles) && (iMinor < numEndZCurrentTriangles))

		if (info.flag == REVERSE_JZ_TRI)
		{
			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;
		}

		p_Azdot0[iMinor] = p_AAdot_k[iMinor].Azdot + h_use*c*FOUR_PI*Jz;

		p_gamma[iMinor] = h_use * c*c;

		
		if ((info.flag == INNER_FRILL) || (info.flag == OUTER_FRILL))

		{
			p_Azdot0[iMinor] = 0.0; // difference found? But we did set = 0 on CPU.
			p_gamma[iMinor] = 0.0;
		}
	};
}


__global__ void kernelCreateLinearRelationshipBwd_noadvect(
	f64 const h_use,
	structural * __restrict__ p_info,
	OhmsCoeffs* __restrict__ p_Ohms,
	v4 * __restrict__ p_v0,
	f64 * __restrict__ p_Lap_Az_use,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_denom_e, 
	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_coeff_of_vez_upon_viz, 
	f64 * __restrict__ p_beta_ie_z,
	AAdot * __restrict__ p_AAdot_k,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_Azdot0,
	f64 * __restrict__ p_gamma
)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 const Lap_Az_used = p_Lap_Az_use[iMinor];
	structural const info = p_info[iMinor];

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
	{
		v4 v0 = p_v0[iMinor];
		// Cancel the part that was added in order to get at Ez_strength:
		
		f64 denom_e = p_denom_e[iMinor];
		f64 denom_i = p_denom_i[iMinor];

		v0.viz += qoverM*h_use*h_use* c* Lap_Az_used/denom_i; // adaptation for this.
		f64 coeff_of_vez_upon_viz = p_coeff_of_vez_upon_viz[iMinor];
		
		f64 cancel_from_vez = -eoverm*h_use*h_use* c* Lap_Az_used / denom_e
			+ coeff_of_vez_upon_viz * qoverM*h_use*h_use* c* Lap_Az_used / denom_i;
		
		v0.vez += cancel_from_vez;
		f64 beta_ie_z = p_beta_ie_z[iMinor];
		v0.viz += beta_ie_z * cancel_from_vez;

		// We see now that re-jigging things is absolutely not what we should have done.
		// It will make the most complicated overspilling routine, more complicated still.
		
		OhmsCoeffs Ohms = p_Ohms[iMinor];

		f64 vez_1 = v0.vez + Ohms.sigma_e_zz * Ez_strength;
		f64 viz_1 = v0.viz + Ohms.sigma_i_zz * Ez_strength;

		nvals n_use = p_n_minor[iMinor];

		p_Azdot0[iMinor] = p_AAdot_k[iMinor].Azdot
				+ h_use *c*4.0*M_PI* q*n_use.n*(viz_1 - vez_1);

	//	if ((iMinor - BEGINNING_OF_CENTRAL == VERTCHOSEN) || (iMinor - BEGINNING_OF_CENTRAL == VERTCHOSEN2))
	//		printf("%d : AAdot_k.Azdot %1.10E n_use.n %1.9E viz1 %1.9E vez1 %1.9E\n",
	//			iMinor, p_AAdot_k[iMinor].Azdot, n_use.n, viz_1, vez_1);

		// ROCAzdot_antiadvect --- we need this to be in there only
		// on cycles that we do advection

		// So do the addition in here.
		
		// THIS WAS IN ERROR.

		f64 viz0_coeff_on_Lap_Az = -h_use*h_use*qoverM*c / denom_i;
		f64 vez0_coeff_on_Lap_Az = h_use*h_use*eoverm*c / denom_e
			+ coeff_of_vez_upon_viz*viz0_coeff_on_Lap_Az;

		viz0_coeff_on_Lap_Az += beta_ie_z*vez0_coeff_on_Lap_Az;

		p_gamma[iMinor] = h_use*c*c*(1.0 + FOURPI_OVER_C * q*n_use.n*
			(viz0_coeff_on_Lap_Az - vez0_coeff_on_Lap_Az));

	} else {
		// We need to do the same sort of thing here as in CalcVelocityAzdot :

		f64 Jz = 0.0;
		
		//if ((iMinor >= numStartZCurrentTriangles) && (iMinor < numEndZCurrentTriangles))
		if (info.flag == REVERSE_JZ_TRI)
		{
			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;			
		}

		p_Azdot0[iMinor] = p_AAdot_k[iMinor].Azdot 
			+ h_use*c*FOUR_PI*Jz;
		p_gamma[iMinor] = h_use * c*c;

		if ((info.flag == INNER_FRILL) || (info.flag == OUTER_FRILL))
		{
			p_Azdot0[iMinor] = 0.0; // difference found? But we did set = 0 on CPU.
			p_gamma[iMinor] = 0.0;			
		}
	};
}




/*
__global__ void kernelPopulateOhmsLaw(
	f64 h_use,

	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	f64_vec3 * __restrict__ p_B,
	f64 * __restrict__ p_LapAz,
	f64_vec2 * __restrict__ p_GradAz,
	f64_vec2 * __restrict__ p_GradTe,
	nvals * __restrict__ p_n_minor_use,

	nvals * __restrict__ p_one_over_n,

	T3 * __restrict__ p_T_minor_use,

	v4 * __restrict__ p_vie_src,
	f64_vec3 * __restrict__ p_v_n_src,
	AAdot * __restrict__ p_AAdot_src,
	f64 * __restrict__ p_AreaMinor,

	f64 * __restrict__ ROCAzdotduetoAdvection,
	// Now going to need to go through and see this set 0 or sensible every time.

	f64_vec3 * __restrict__ p_vn0_dest,
	v4 * __restrict__ p_v0_dest,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs_dest,
	AAdot * __restrict__ p_AAdot_intermediate,

	f64 * __restrict__ p_Iz0,
	f64 * __restrict__ p_sigma_zz,
	
	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_denom_e, 
	f64 * __restrict__ p_effect_of_viz0_on_vez0,
	f64 * __restrict__ p_beta_ie_z,

	bool const bSwitchSave,
	bool const bUse_dest_n_for_Iz,
	nvals * __restrict__ p_n_dest_minor) // for turning on save of these denom_ quantities
{
	// Don't forget we can use 16KB shared memory to save a bit of overspill:
	// (16*1024)/(512*8) = 4 doubles only for 512 threads. 128K total register space per SM we think.

	__shared__ f64 Iz[threadsPerTileMinor], sigma_zz[threadsPerTileMinor];
//	__shared__ f64 Iz_k[threadsPerTileMinor];

	__shared__ f64_vec2 omega[threadsPerTileMinor], grad_Az[threadsPerTileMinor],
		gradTe[threadsPerTileMinor];
	
	// Putting 8 reduces to 256 simultaneous threads. Experiment with 4 in shared.
	// f64 viz0_coeff_on_Lap_Az, vez0_coeff_on_Lap_Az; // THESE APPLY TO FEINT VERSION. ASSUME NOT FEINT FIRST.

	v4 v0;
	f64 denom, ROCAzdot_antiadvect, AreaMinor;
	f64_vec3 vn0;
	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX
	structural info = p_info_minor[iMinor];

	// Can see no reason not to put OUTERMOST here. No point creating a big gradient of vz to it.

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == DOMAIN_TRIANGLE)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 vie_k = p_vie_src[iMinor];
		f64_vec3 v_n_src = p_v_n_src[iMinor];
		nvals n_use = p_n_minor_use[iMinor];
		AreaMinor = p_AreaMinor[iMinor];
		// Are we better off with operator = or with memcpy?
		vn0 = v_n_src;

//		if ((TESTTRI)) printf("GPU %d vie_k %1.14E %1.14E\n", iMinor, vie_k.vxy.x, vie_k.vxy.y);
		{
			f64_vec3 MAR;
			memcpy(&MAR, p_MAR_neut + iMinor, sizeof(f64_vec3));
			// CHECK IT IS INTENDED TO AFFECT Nv

			// REVERTED THE EDIT TO USE 1/n -- THIS WILL NOT GIVE CORRECT M.A.R. EFFECT ON INTEGRAL nv
			// We need conservation laws around shock fronts.
			vn0.x += h_use * (MAR.x / (AreaMinor*n_use.n_n));								
				// p_one_over_n[iMinor].n_n/ (AreaMinor));
			vn0.y += h_use * (MAR.y/(AreaMinor*n_use.n_n));// MomAddRate is addition rate for Nv. Divide by N.

			memcpy(&MAR, p_MAR_ion + iMinor, sizeof(f64_vec3));
			v0.vxy = vie_k.vxy + h_use * (m_i*MAR.xypart()/ (n_use.n*(m_i + m_e)*AreaMinor));
			v0.viz = vie_k.viz + h_use * MAR.z / (n_use.n*AreaMinor);

			if (((TESTTRI))) printf("\nGPU %d vxyk %1.10E %1.10E aMAR_i.y %1.10E MAR.y %1.10E 1/n %1.10E Area %1.10E\n", iMinor, 
				v0.vxy.x, v0.vxy.y,
				h_use * (m_i*MAR.y / (n_use.n*(m_i + m_e)*AreaMinor)),
				MAR.y,
				p_one_over_n[iMinor].n,
				AreaMinor);
			
			memcpy(&MAR, p_MAR_elec + iMinor, sizeof(f64_vec3));
			v0.vxy += h_use * (m_e*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
			v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);   // UM WHY WAS THIS NEGATIVE
													 // + !!!!
			if (v0.vez != v0.vez) printf("NANVEZ %d v_k %1.9E MAR.z %1.9E \n", iMinor, vie_k.vez, MAR.z);

			if (((TESTTRI))) printf("\nGPU %d a:MAR_e %1.10E %1.10E MAR.y %1.10E 1/n %1.10E Area %1.10E\n", iMinor,
				h_use * (m_e*MAR.x/ (n_use.n*(m_i + m_e)*AreaMinor)),
				h_use * (m_e*MAR.y/ (n_use.n*(m_i + m_e)*AreaMinor)),
				MAR.y,
				p_one_over_n[iMinor].n, AreaMinor);

	//		if (((TESTTRI))) 
		//		printf("GPU %d WITH MAR v0.vxy %1.14E %1.14E\n", CHOSEN, v0.vxy.x, v0.vxy.y);
				//	printf("GPU %d data_k %1.10E %1.10E MAR %1.10E %1.10E\n", CHOSEN, vie_k.vxy.x, vie_k.vxy.y,
					//	MAR.x, MAR.y);
//				printf("GPU %d n %1.12E AreaMinor %1.12E \n", CHOSEN, n_use.n, AreaMinor);
	//		}
		}

		OhmsCoeffs ohm;
		f64 beta_ie_z, LapAz;
		f64 cross_section_times_thermal_en, cross_section_times_thermal_in,
			nu_eiBar, nu_eHeart;
		T3 T = p_T_minor_use[iMinor];
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal,
				lnLambda, s_in_MT, s_en_MT, s_en_visc;
			sqrt_Te = sqrt(T.Te);
			ionneut_thermal = sqrt(T.Ti / m_ion + T.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_use.n, T.Te);
			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T.Ti*one_over_kB, &s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T.Te*one_over_kB, &s_en_MT, &s_en_visc);

			//nu_ne_MT = s_en_MT * electron_thermal * n_use.n; // have to multiply by n_e for nu_ne_MT
			//nu_ni_MT = s_in_MT * ionneut_thermal * n_use.n;
			//nu_in_MT = s_in_MT * ionneut_thermal * n_use.n_n;
			//nu_en_MT = s_en_MT * electron_thermal * n_use.n_n;

			cross_section_times_thermal_en = s_en_MT * electron_thermal;
			cross_section_times_thermal_in = s_in_MT * ionneut_thermal;
			
			nu_eiBar = nu_eiBarconst * kB_to_3halves*n_use.n*lnLambda / (T.Te*sqrt_Te);
			nu_eHeart = 1.87*nu_eiBar + n_use.n_n*s_en_visc*electron_thermal;
			if (nu_eiBar != nu_eiBar) printf("&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
				"iMinor %d n_use.n %1.9E lnLambda %1.9E Te %1.9E sqrt %1.9E \n",
				iMinor, n_use.n, lnLambda, T.Te, sqrt_Te);

			// ARTIFICIAL CHANGE TO STOP IONS SMEARING AWAY OFF OF NEUTRAL BACKGROUND:
			if (n_use.n_n > ARTIFICIAL_RELATIVE_THRESH *n_use.n) {
				cross_section_times_thermal_en *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
				cross_section_times_thermal_in *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			}

		}

		vn0.x += -0.5*h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n)*(v_n_src.x - vie_k.vxy.x)
			- 0.5*h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n)*(v_n_src.x - vie_k.vxy.x);
		vn0.y += -0.5*h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n)*(v_n_src.y - vie_k.vxy.y)
			- 0.5*h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n)*(v_n_src.y - vie_k.vxy.y);
		vn0.z += -0.5*h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n)*(v_n_src.z - vie_k.vez)
			- 0.5*h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n)*(v_n_src.z - vie_k.viz);
		denom = 1.0 + h_use * 0.5*M_e_over_en* (cross_section_times_thermal_en*n_use.n)
			+ 0.5*h_use*M_i_over_in* (cross_section_times_thermal_in*n_use.n);

		vn0 /= denom; // It is now the REDUCED value

		if (((TESTTRI))) 
			printf("GPU %d vn0 %1.9E %1.9E %1.9E denom %1.14E \n", CHOSEN, vn0.x, vn0.y, vn0.z, denom);
	

		ohm.beta_ne = 0.5*h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n) / denom;
		ohm.beta_ni = 0.5*h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n) / denom;

		// Now we do vexy:

		grad_Az[threadIdx.x] = p_GradAz[iMinor];
		gradTe[threadIdx.x] = p_GradTe[iMinor];
		LapAz = p_LapAz[iMinor];
		f64 ROCAzdot_antiadvect = ROCAzdotduetoAdvection[iMinor];
		
		if (((TESTTRI))) printf("GPU %d: LapAz %1.14E\n", CHOSEN, LapAz);

		// %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		// Here is where we should be using v_use:
		// We do midpoint instead? Why not? Thus allowing us not to load v_use.
		// %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

		v0.vxy +=
			-h_use * (q / (2.0*c*(m_i + m_e)))*(vie_k.vez - vie_k.viz)*grad_Az[threadIdx.x]

			- (h_use / (2.0*(m_i + m_e)))*(m_n*M_i_over_in*(cross_section_times_thermal_in*n_use.n_n)
				+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*
				(vie_k.vxy - v_n_src.xypart() - vn0.xypart());


		if (((TESTTRI))) printf("GPU %d vzgradAz contrib_k %1.10E %1.10E vez_k viz_k %1.9E %1.9E gradAz %1.9E %1.9E\n", iMinor, 
			-h_use * (q / (2.0*c*(m_i + m_e)))*(vie_k.vez - vie_k.viz)*grad_Az[threadIdx.x].x,
			-h_use * (q / (2.0*c*(m_i + m_e)))*(vie_k.vez - vie_k.viz)*grad_Az[threadIdx.x].y, vie_k.vez, vie_k.viz,
			grad_Az[threadIdx.x].x, grad_Az[threadIdx.x].y);


		denom = 1.0 + (h_use / (2.0*(m_i + m_e)))*(
			m_n* M_i_over_in* (cross_section_times_thermal_in*n_use.n_n)
			+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*(1.0 - ohm.beta_ne - ohm.beta_ni);
		v0.vxy /= denom;
//
		if (((TESTTRI))) 
			printf("GPU %d v0.vxy %1.14E %1.14E denom %1.14E \n"
				"nu_in_MT %1.14E nu_en_MT %1.14E beta_ne %1.14E \n", 
				CHOSEN, v0.vxy.x, v0.vxy.y, denom,
				cross_section_times_thermal_in*n_use.n_n, cross_section_times_thermal_en*n_use.n_n, ohm.beta_ne);
			
		ohm.beta_xy_z = (h_use * q / (2.0*c*(m_i + m_e)*denom)) * grad_Az[threadIdx.x];
		/////////////////////////////////////////////////////////////////////////////// midpoint
//		if (((TESTTRI))) printf("ohm.beta_xy_z %1.14E \n", ohm.beta_xy_z);

		omega[threadIdx.x] = qovermc*p_B[iMinor].xypart();

		f64 nu_ei_effective = nu_eiBar * (1.0 - 0.9*nu_eiBar*(nu_eHeart*nu_eHeart + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT) /
			(nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].x*omega[threadIdx.x].x + omega[threadIdx.x].y*omega[threadIdx.x].y + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT)));

	//	if (nu_ei_effective != nu_ei_effective) printf("nu_ei NaN: omega %1.8E %1.8E nu_eHeart %1.8E nu_eiBar %1.8E\n",
	//		omega[threadIdx.x].x, omega[threadIdx.x].y, nu_eHeart, nu_eiBar);

		AAdot AAzdot_k = p_AAdot_src[iMinor];

		//if ((iPass == 0) || (bFeint == false))
		{
	//		if (((TESTTRI)) && (0)) printf("viz0: %1.14E\n", v0.viz);
			if (((TESTTRI))) printf("GPU %d: LapAz %1.14E\n", CHOSEN, LapAz); // nonzero
			v0.viz +=
				-0.5*h_use*qoverMc*(2.0*AAzdot_k.Azdot
					+ h_use * ROCAzdot_antiadvect + h_use * c*c*(LapAz
						+ FOURPI_OVER_C*0.5 * q*n_use.n*(vie_k.viz - vie_k.vez)))
				- 0.5*h_use*qoverMc*(vie_k.vxy + v0.vxy).dot(grad_Az[threadIdx.x]);

			if (((TESTTRI))) {
				printf("viz0 I: %1.14E contribs:\n", v0.viz);
				printf("   Azdotk %1.14E \n   ROC %1.14E\n   JviaAzdot %1.14E\n   lorenzmag %1.14E\n",
					-0.5*h_use*qoverMc*(2.0*AAzdot_k.Azdot),
					-0.5*h_use*qoverMc*h_use * ROCAzdot_antiadvect,
					-0.5*h_use*qoverMc*h_use * c*c*(FOURPI_OVER_C*0.5 * q*n_use.n*(vie_k.viz - vie_k.vez)),
					-0.5*h_use*qoverMc*(vie_k.vxy + v0.vxy).dot(grad_Az[threadIdx.x])
				);
				printf("due to LapAz: %1.14E = %1.6E %1.6E %1.6E %1.6E\n",
					-0.5*h_use*qoverMc*h_use *c*c*LapAz,
					h_use*h_use*0.5,
					qoverMc,
					c*c,
					LapAz); // == 0
			};

		}
		//else {
		//	viz0 = data_k.viz
		//				- h_use * MomAddRate.ion.z / (data_use.n*AreaMinor)
		//				- 0.5*h_use*qoverMc*(2.0*data_k.Azdot
		//				+ h_use * ROCAzdot_antiadvect + h_use * c*c*(TWOPIoverc * q*data_use.n*(data_k.viz - data_k.vez)))
		//				- 0.5*h_use*qoverMc*(data_k.vxy + vxy0).dot(grad_Az[threadIdx.x]);
		//	};

		//
		// Still omega_ce . Check formulas.
		// 

		v0.viz +=
			1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x])));

	//	if (((TESTTRI))) printf("viz0 with thermal force %1.14E \n", v0.viz);

		v0.viz += -h_use * 0.5*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(vie_k.viz - v_n_src.z - vn0.z) // THIS DOESN'T LOOK RIGHT
			+ h_use * 0.5*(moverM)*nu_ei_effective*(vie_k.vez - vie_k.viz);

		if (((TESTTRI))) printf("viz0 contrib i-n %1.14E contrib e-i %1.14E\nviz0 %1.14E\n",
			-h_use * 0.5*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(vie_k.viz - v_n_src.z - vn0.z),
			h_use * 0.5*(moverM)*nu_ei_effective*(vie_k.vez - vie_k.viz), v0.viz
			);

		denom = 1.0 + h_use * h_use*M_PI*qoverM*q*n_use.n + h_use * 0.5*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) +
			h_use * 0.5*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(1.0 - ohm.beta_ni) + h_use * 0.5*moverM*nu_ei_effective;

		if (bSwitchSave) p_denom_i[iMinor] = denom;
		//				viz0_coeff_on_Lap_Az = -0.5*h_use*qoverMc*h_use*c*c / denom;

		v0.viz /= denom;

		if (((TESTTRI))) printf("viz0 divided %1.14E denom %1.14E \n", v0.viz, denom);

		ohm.sigma_i_zz = h_use * qoverM / denom;
		beta_ie_z = (h_use*h_use*M_PI*qoverM*q*n_use.n
			+ 0.5*h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))
			+ h_use * 0.5*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *ohm.beta_ne
			+ h_use * 0.5*moverM*nu_ei_effective) / denom;

		if (((TESTTRI2))) printf("vez0 %1.14E \n", v0.vez);

		v0.vez +=
			h_use * 0.5*qovermc*(2.0*AAzdot_k.Azdot
				+ h_use * ROCAzdot_antiadvect
				+ h_use * c*c*(LapAz
					+ 0.5*FOURPI_Q_OVER_C*n_use.n*(vie_k.viz + v0.viz - vie_k.vez))) // ?????????????????
			+ 0.5*h_use*qovermc*(vie_k.vxy + v0.vxy + v0.viz * ohm.beta_xy_z).dot(grad_Az[threadIdx.x]);


		if (((TESTTRI2))) 
			printf(" %d v0.vez %1.14E Azdotctb %1.14E antiadvect %1.14E LapAzctb %1.14E \n"
				"%d JviaAzdot %1.14E lorenzmag %1.14E \n",
				iMinor, v0.vez, h_use * 0.5*qovermc*2.0*AAzdot_k.Azdot,
				h_use * 0.5*qovermc*h_use * ROCAzdot_antiadvect,
				h_use * 0.5*qovermc*h_use * c*c*LapAz,
				iMinor,
				h_use * 0.5*qovermc*h_use * c*c* 0.5*FOURPI_Q_OVER_C*n_use.n*(vie_k.viz + v0.viz - vie_k.vez),
				0.5*h_use*qovermc*(vie_k.vxy + v0.vxy + v0.viz * ohm.beta_xy_z).dot(grad_Az[threadIdx.x])
				);		

		// implies:
		f64 effect_of_viz0_on_vez0 = 			
			h_use * 0.5*qovermc*h_use * c*c*0.5*FOURPI_Q_OVER_C*n_use.n			
			+ 0.5*h_use*qovermc*( ohm.beta_xy_z.dot(grad_Az[threadIdx.x]));
		
		v0.vez -=
			1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x])+ qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT));

		if (((TESTTRI2)))
			printf("%d v0.vez TF contrib : %1.14E nu_eiBar %1.14E nu_eHeart %1.14E \n"
				"%d omega %1.14E %1.14E %1.14E\n",iMinor,

				-1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
				(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
					(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x]) + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT)),
				
				nu_eiBar, nu_eHeart, iMinor,
				omega[threadIdx.x].x, omega[threadIdx.x].y, qovermc*BZ_CONSTANT);
			
		// could store this from above and put opposite -- dividing by m_e instead of m_i

		v0.vez += -0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(vie_k.vez - v_n_src.z - vn0.z - ohm.beta_ni * v0.viz)
			- 0.5*h_use*nu_ei_effective*(vie_k.vez - vie_k.viz - v0.viz);
		// implies:
		effect_of_viz0_on_vez0 += 
			0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni + 0.5*h_use*nu_ei_effective;
		


		if (
			//(iMinor == 11761 + BEGINNING_OF_CENTRAL) ||
			//(iMinor == 11616 + BEGINNING_OF_CENTRAL) ||
			//(iMinor == 11762 + BEGINNING_OF_CENTRAL) ||
			((TESTTRI2)) )
		{
			printf("%d cross_section_times_thermal_en %1.10E n_use.n_n %1.10E vezk %1.10E vez0 %1.10E Mnoverne %1.10E nu_ei_effective %1.10E \n",
				iMinor, cross_section_times_thermal_en, n_use.n_n,
				vie_k.vez, v0.vez,
				M_n_over_ne, nu_ei_effective);
		}
		
		if (((TESTTRI2))) 
			printf("v0.vez contribs e-n e-i: %1.14E %1.14E v0.viz %1.14E\n", 
				-0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(vie_k.vez - v_n_src.z - vn0.z - ohm.beta_ni * v0.viz),
				- 0.5*h_use*nu_ei_effective*(vie_k.vez - vie_k.viz - v0.viz),
				v0.viz);

		denom = 1.0 + (h_use*h_use*M_PI*q*eoverm*n_use.n
			+ 0.5*h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)))*(1.0 - beta_ie_z)
			+ 0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne - ohm.beta_ni * beta_ie_z)
			+ 0.5*h_use*nu_ei_effective*(1.0 - beta_ie_z);

		//		vez0_coeff_on_Lap_Az = h_use * h_use*0.5*qovermc* c*c / denom; 

		ohm.sigma_e_zz = 			
			(-h_use * eoverm
			+ h_use * h_use*M_PI*q*eoverm*n_use.n*ohm.sigma_i_zz
			+ h_use * 0.5*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*ohm.sigma_i_zz
			+ 0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni*ohm.sigma_i_zz
			+ 0.5*h_use*nu_ei_effective*ohm.sigma_i_zz)
			/ denom;
		
	//	if (((TESTTRI)1) || ((TESTTRI)2))
//printf("GPU %d vez0 before divide %1.14E \n", iMinor, v0.vez);
//
		v0.vez /= denom;
		effect_of_viz0_on_vez0 /= denom; // of course 

		//if (v0.vez != v0.vez) {
		//	printf("iMinor %d v0.vez %1.10E ohm.sigma_e %1.10E denom %1.10E \n"
		//		"%1.10E %1.10E %1.10E %1.10E n %1.10E Te %1.10E\n"	,
		//		iMinor, v0.vez, ohm.sigma_e_zz, denom,
		//		h_use*h_use*M_PI*q*eoverm*n_use.n,//*(1.0 - beta_ie_z) // this was ok
		//		0.5*h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*(1.0 - beta_ie_z), // this was not ok
		//		0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne - ohm.beta_ni * beta_ie_z),
		//		0.5*h_use*nu_ei_effective,//*(1.0 - beta_ie_z) // this was not ok -- even though n,T come out ok
		//		n_use.n, T.Te);			
		//}


		if ( ((TESTTRI2))) 
			printf("GPU %d v0.vez %1.14E denom %1.14E \n"
				"ohm.sigma_e_zz %1.14E n_use %1.10E nn %1.10E Te %1.10E\n"
				"%d %1.12E %1.12E %1.12E %1.12E %1.12E \n"
				"%d denom %1.14E : %1.12E %1.12E %1.12E %1.12E\n",
				iMinor, v0.vez, denom,
				ohm.sigma_e_zz,
				n_use.n,n_use.n_n, T.Te, iMinor, -h_use * eoverm,
				h_use * h_use*M_PI*q*eoverm*n_use.n*ohm.sigma_i_zz,
				h_use * 0.5*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*ohm.sigma_i_zz,
				0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni*ohm.sigma_i_zz,
				0.5*h_use*nu_ei_effective*ohm.sigma_i_zz,
				iMinor, denom,
				(h_use*h_use*M_PI*q*eoverm*n_use.n)*(1.0 - beta_ie_z),
				(0.5*h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)))*(1.0 - beta_ie_z),
				0.5*h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne - ohm.beta_ni * beta_ie_z),
				0.5*h_use*nu_ei_effective*(1.0 - beta_ie_z)
			);	

		if (bSwitchSave) {
			p_denom_e[iMinor] = denom;
			p_effect_of_viz0_on_vez0[iMinor] = effect_of_viz0_on_vez0;
			p_beta_ie_z[iMinor] = beta_ie_z; // see that doing it this way was not best.
		} else {
			// #########################################################################################################
			// DEBUG: pass graphing parameters through these.
			// #########################################################################################################
			p_denom_i[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective;
			p_denom_e[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n /
				(M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective);
		};
		
		// Now update viz(Ez):
		v0.viz += beta_ie_z * v0.vez;
		ohm.sigma_i_zz += beta_ie_z * ohm.sigma_e_zz;

		// sigma_e_zz and sigma_i_zz are change in vz for a change in Ez
		{
			f64 EzShape = GetEzShape(info.pos.modulus());
			ohm.sigma_i_zz *= EzShape;
			ohm.sigma_e_zz *= EzShape;
		}

		// Think maybe we should get rid of most of this routine out of the subcycle.
		// Rate of acceleration over timestep due to resistance, pressure, thermal force etc could be stored.
		// Saving off some eqn data isn't so bad when we probably overflow registers and L1 here anyway.
		// All we need is to know that we update sigma
		// We can do addition of 
		// ==============================================================================================

		p_v0_dest[iMinor] = v0;
		p_OhmsCoeffs_dest[iMinor] = ohm;
		p_vn0_dest[iMinor] = vn0;

		if (bUse_dest_n_for_Iz) {
			f64 ndest = p_n_dest_minor[iMinor].n;
			Iz[threadIdx.x] = q*AreaMinor*ndest*(v0.viz - v0.vez);
			sigma_zz[threadIdx.x] = q*AreaMinor*ndest*(ohm.sigma_i_zz - ohm.sigma_e_zz);

			if (((TESTTRI2))) {
				printf( "ndest %1.12E sigma_zz/Area %1.12E AreaMinor %1.12E\n\n",
					ndest, q*ndest*(ohm.sigma_i_zz - ohm.sigma_e_zz), AreaMinor);
			}

		} else {
			// On intermediate substeps, the interpolated n that applies halfway through the substep is a reasonable choice...
			Iz[threadIdx.x] = q*AreaMinor*n_use.n*(v0.viz - v0.vez);
			sigma_zz[threadIdx.x] = q*AreaMinor*n_use.n*(ohm.sigma_i_zz - ohm.sigma_e_zz);
			// I'm sure we can do better on this. But we also might prefer to excise a lot of this calc from the subcycle.
			if (((TESTTRI2))) {
				printf("n_use.n %1.12E sigma_zz/Area %1.12E AreaMinor %1.12E\n\n",
					n_use.n, q*n_use.n*(ohm.sigma_i_zz - ohm.sigma_e_zz), AreaMinor);
			}

		}
		
		
		// Totally need to be skipping the load of an extra n.
		// ^^ old remark.
		// But it's too messy never loading it. t_half means changing all the
		// Iz formula to involve v_k. Don't want that.


	//	if (blockIdx.x == 340) printf("%d: %1.14E %1.14E \n",
		//	iMinor, q*n_use.n*(ohm.sigma_i_zz - ohm.sigma_e_zz), sigma_zz[threadIdx.x]);

		// On iPass == 0, we need to do the accumulate.
		//	p_Azdot_intermediate[iMinor] = Azdot_k
		//		+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
		//			0.5*FOURPI_OVER_C * q*n_use.n*(data_k.viz - data_k.vez)); // INTERMEDIATE

		//if ((0) && ((TESTTRI))) printf("******************* AAzdot_k.Azdot %1.14E \n", AAzdot_k.Azdot);

		AAzdot_k.Azdot +=
			 h_use * ROCAzdot_antiadvect + h_use * c*c*(LapAz +
				0.5*FOURPI_OVER_C * q*n_use.n*(vie_k.viz - vie_k.vez)); // INTERMEDIATE

		p_AAdot_intermediate[iMinor] = AAzdot_k; // not k any more

		//Iz_k[threadIdx.x] = q*n_use.n*(vie_k.viz - vie_k.vez)*AreaMinor;
		
		//if ((0) && ((TESTTRI))) {
		//	printf("\n!!! kernelPopOhms GPU %d: \n******* Azdot_intermediate %1.14E vie_k %1.14E %1.14E\n"
		//		"antiadvect %1.10E Lapcontrib %1.13E Jcontrib_k %1.14E\n\n",
		//		CHOSEN, p_AAdot_intermediate[iMinor].Azdot,
		//		vie_k.viz, vie_k.vez,
		//		h_use * ROCAzdot_antiadvect,
		//		h_use * c*c*LapAz,
		//		h_use * c*c*0.5*FOURPI_OVER_C * q*n_use.n*(vie_k.viz - vie_k.vez)
		//		);
		//}

		//data_1.Azdot = data_k.Azdot
		//	+ h_use * ROCAzdot_antiadvect + h_use * c*c*(LapAz +
		//		0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz + data_1.viz
		//			- data_k.vez - data_1.vez));

	} else {
		// Non-domain triangle or vertex
		// ==============================
		// Need to decide whether crossing_ins triangle will experience same accel routine as the rest?
		// I think yes so go and add it above??
		// We said v_r = 0 necessarily to avoid sending mass into ins.
		// So how is that achieved there? What about energy loss?
		// Need to determine a good way. Given what v_r in tri represents. We construe it to be AT the ins edge so 
		// ...
		Iz[threadIdx.x] = 0.0;
		sigma_zz[threadIdx.x] = 0.0;

		if ((iMinor < BEGINNING_OF_CENTRAL) && ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)))
		{
			p_AAdot_intermediate[iMinor].Azdot = 0.0;
			// Set Az equal to neighbour in every case, after Accelerate routine.
		} else {
			// Let's make it go right through the middle of a triangle row for simplicity.

			//f64 Jz = 0.0;
			//if ((iMinor >= numStartZCurrentTriangles) && (iMinor <  numEndZCurrentTriangles))
			//{
			//	// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			//	// ASSUME we are fed Iz_prescribed.
			//	//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			//	AreaMinor = p_AreaMinor[iMinor];
			//	Jz = negative_Iz_per_triangle / AreaMinor; // Iz would come from multiplying back by area and adding.
			//};

			AAdot temp = p_AAdot_src[iMinor];
			temp.Azdot += h_use * c*(c*p_LapAz[iMinor]);// +4.0*M_PI*Jz);
			// + h_use * ROCAzdot_antiadvect // == 0
			p_AAdot_intermediate[iMinor] = temp; // 

		};
	};

	__syncthreads();

	// .Collect Jz = Jz0 + sigma_zz Ez_strength on each minor cell
	// .Estimate Ez
	// sigma_zz should include EzShape for this minor cell

	// The mission if iPass == 0 was passed is to save off Iz0, SigmaIzz.
	// First pass set Ez_strength = 0.0.


	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + k];
			Iz[threadIdx.x] += Iz[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + s - 1];
			Iz[threadIdx.x] += Iz[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sigma_zz[blockIdx.x] = sigma_zz[0];
		p_Iz0[blockIdx.x] = Iz[0];
	}
	// Wish to make the Jz contribs to Azdot on each side of the ins exactly equal in L1, 
	// meant making this long routine even longer with collecting Iz_k.
}
*/

__global__ void kernelCollectOhmsGraphs(
	structural * __restrict__ p_info_major,
	f64_vec3 * __restrict__ p_MAR_ion_pressure_major,
	f64_vec3 * __restrict__ p_MAR_ion_visc_major,
	f64_vec3 * __restrict__ p_MAR_elec_pressure_major,  // need to distinguish viscous from pressure part.
	f64_vec3 * __restrict__ p_MAR_elec_visc_major,
	f64_vec3 * __restrict__ p_MAR_elec_ionization_major,
	f64_vec3 * __restrict__ p_B_major,

	v4 * __restrict__ p_vie_k, // ALL MAJOR
	v4 * __restrict__ p_vie_kplus,

	f64_vec2 * __restrict__ p_GradTe_major,
	nvals * __restrict__ p_n_major_use,
	T3 * __restrict__ p_T_major_use,

	AAdot * __restrict__ p_AAdot_kplus,
	f64 * __restrict__ p_AreaMinor, // EXCEPT THIS ONE

	f64 * __restrict__ p_Ohmsgraph_0, // elastic effective frictional coefficient zz
	f64 * __restrict__ p_Ohmsgraph_1, // ionization effective frictional coefficient zz
	f64 * __restrict__ p_Ohmsgraph_2, // 2 is combined y pressure accel rate
	f64 * __restrict__ p_Ohmsgraph_3,// 3 is q/(M+m) Ez -- do we have
	f64 * __restrict__ p_Ohmsgraph_4, // 4 is thermal force accel

	f64 * __restrict__ p_Ohmsgraph_5, // T_zy
	f64 * __restrict__ p_Ohmsgraph_6, // T_zz

	f64 * __restrict__ p_Ohmsgraph_7, // T acting on pressure
	f64 * __restrict__ p_Ohmsgraph_8, // T acting on electromotive
	f64 * __restrict__ p_Ohmsgraph_9, // T acting on thermal force
	f64 * __restrict__ p_Ohmsgraph_10, // prediction vez-viz

	f64 * __restrict__ p_Ohmsgraph_11, // difference of prediction from vez_k
	f64 * __restrict__ p_Ohmsgraph_12, // progress towards eqm: need vez_k+1
	f64 * __restrict__ p_Ohmsgraph_13, // viscous acceleration of electrons and ions (z)
	f64 * __restrict__ p_Ohmsgraph_14, // Prediction of Jz
	f64 * __restrict__ p_Ohmsgraph_15, // sigma zy
	f64 * __restrict__ p_Ohmsgraph_16, // sigma zz
	f64 * __restrict__ p_Ohmsgraph_17, // sigma zz times electromotive 
	f64 * __restrict__ p_Ohmsgraph_18 // Difference of prediction from Jz predicted.
)
{
	// Don't forget we can use 16KB shared memory to save a bit of overspill:
	// (16*1024)/(512*8) = 4 doubles only for 512 threads. 128K total register space per SM we think.

	__shared__ f64_vec2 gradTe[threadsPerTileMinor];

	f64_vec3 omega_ce;
	// Putting 8 reduces to 256 simultaneous threads. Experiment with 4 in shared.
	// f64 viz0_coeff_on_Lap_Az, vez0_coeff_on_Lap_Az; // THESE APPLY TO FEINT VERSION. ASSUME NOT FEINT FIRST.

	v4 v0;
	f64 denom, AreaMinor;
	long const iVertex = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX
	structural info = p_info_major[iVertex];

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
	{
		v4 vie_k = p_vie_k[iVertex];
	//	f64_vec3 v_n_src = p_v_n_src[iMinor];
		nvals n_use = p_n_major_use[iVertex];
		AreaMinor = p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL];
		
		f64_vec3 MAR_elec, MAR_ion;
		memcpy(&MAR_elec, p_MAR_elec_ionization_major + iVertex, sizeof(f64_vec3));
		p_Ohmsgraph_1[iVertex] = (MAR_elec.z / (n_use.n*AreaMinor*vie_k.vez));
		// // ionization effective frictional coefficient zz

		memcpy(&MAR_ion, p_MAR_ion_pressure_major + iVertex, sizeof(f64_vec3));
		memcpy(&MAR_elec, p_MAR_elec_pressure_major + iVertex, sizeof(f64_vec3));
		p_Ohmsgraph_2[iVertex] = (m_i*MAR_ion.y + m_e*MAR_elec.y)/((m_i + m_e)*(n_use.n*AreaMinor));

		memcpy(&MAR_ion, p_MAR_ion_visc_major + iVertex, sizeof(f64_vec3));
		memcpy(&MAR_elec, p_MAR_elec_visc_major + iVertex, sizeof(f64_vec3));
		p_Ohmsgraph_13[iVertex] = (m_i*MAR_ion.y + m_e*MAR_elec.y) / ((m_i + m_e)*(n_use.n*AreaMinor));

		// v0.vxy += h_use * (m_e*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
		//v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);			
		
		f64 cross_section_times_thermal_en, cross_section_times_thermal_in,
			nu_eiBar, nu_eHeart;
		T3 T = p_T_major_use[iVertex];
		f64 sqrt_Te, ionneut_thermal, electron_thermal,
			lnLambda, s_in_MT, s_en_MT, s_en_visc;

		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			sqrt_Te = sqrt(T.Te);
			ionneut_thermal = sqrt(T.Ti / m_ion + T.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_use.n, T.Te);
			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T.Ti*one_over_kB, &s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T.Te*one_over_kB, &s_en_MT, &s_en_visc);

			cross_section_times_thermal_en = s_en_MT * electron_thermal;
			cross_section_times_thermal_in = s_in_MT * ionneut_thermal;

			nu_eiBar = nu_eiBarconst * kB_to_3halves*n_use.n*lnLambda / (T.Te*sqrt_Te);
			nu_eHeart = 1.87*nu_eiBar + n_use.n_n*s_en_visc*electron_thermal;

			// ARTIFICIAL CHANGE TO STOP IONS SMEARING AWAY OFF OF NEUTRAL BACKGROUND:
			if (n_use.n_n > ARTIFICIAL_RELATIVE_THRESH *n_use.n) {
				cross_section_times_thermal_en *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
				cross_section_times_thermal_in *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
			}
		}
		
		gradTe[threadIdx.x] = p_GradTe_major[iVertex];
		omega_ce = qovermc*p_B_major[iVertex];
		omega_ce.z = BZ_CONSTANT*qovermc;

		f64 nu_ei_effective =
			nu_eiBar * (1.0 - 0.9*nu_eiBar*
			(nu_eHeart*nu_eHeart + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT) /
				(nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))));

		p_Ohmsgraph_0[iVertex] = nu_ei_effective + M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n);

		AAdot AAzdot_kplus = p_AAdot_kplus[iVertex];

		p_Ohmsgraph_3[iVertex] = -(q * (m_i + m_e)/(m_e*m_i))*(Ez_strength*GetEzShape(info.pos.modulus()) - AAzdot_kplus.Azdot / c);
		
		//v0.viz +=
		//	1.5*h_use*nu_eiBar*(
		//	(omega_ce.x*qovermc*BZ_CONSTANT - nu_eHeart * omega_ce.y)*gradTe[threadIdx.x].x +
		//		(omega_ce.y*qovermc*BZ_CONSTANT + nu_eHeart * omega_ce.x)*gradTe[threadIdx.x].y) /
		//		(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)));

		//v0.viz += h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *vn0.z;

	//	denom = 1.0 + h_use * h_use*4.0*M_PI*qoverM*q*n_use.n
	//		+ h_use * qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) +
	//		h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(1.0 - ohm.beta_ni)
	//		+ h_use *moverM*nu_ei_effective;
	//	v0.viz /= denom;
		
		// implies:
		//f64 effect_of_viz0_on_vez0 =
		//	h_use * qovermc*h_use * c*c* FOURPI_Q_OVER_C*n_use.n
		//	+ h_use*qovermc*(ohm.beta_xy_z.dot(grad_Az[threadIdx.x]));

		// remember it's ve-vi :
		p_Ohmsgraph_4[iVertex] = -1.5*nu_eiBar*((
			(omega_ce.x*qovermc*BZ_CONSTANT - nu_eHeart * omega_ce.y)*gradTe[threadIdx.x].x +
			(omega_ce.y*qovermc*BZ_CONSTANT + nu_eHeart * omega_ce.x)*gradTe[threadIdx.x].y) /
			(nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))))*(1.0 / m_i + 1.0 / m_e);						

		// Now it's time to work out T_zy and T_zz:

		f64 Tzy, Tzz;

		f64 a_ = -0.9*nu_eiBar*nu_eiBar / (nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)));
		f64 b_ = 1.0 - a_*nu_eHeart;
		f64 c_ = M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) + nu_eiBar + nu_eHeart*nu_eHeart*a_;

		Tzy = ((a_*b_*omega_ce.dot(omega_ce) + b_*c_)*omega_ce.x + (b_*b_ - a_*c_)*omega_ce.y*omega_ce.z) /
			((a_*omega_ce.dot(omega_ce) + c_)*(b_*b_*omega_ce.dot(omega_ce) + c_*c_));

		Tzz = (c_*c_ + a_*c_*omega_ce.dot(omega_ce) + (b_*b_-a_*c_)*omega_ce.z*omega_ce.z) /
			((a_*omega_ce.dot(omega_ce) + c_)*(b_*b_*omega_ce.dot(omega_ce) + c_*c_));
		
		// Notice: if omega_ce = 0 then we get Tzz = c*c/c*c*c = 1/c.
		// a = -0.9*nu_eiBar^2/(nu_eHeart^3)
		// Tzz = 1/(nu_en + nu_ei + nu_ei^2/(1.87nu_ei+nu_en)^3)
		// Not sure that is what it's supposed to be.... it's close. Remember we had e-i.

		p_Ohmsgraph_5[iVertex] = Tzy;
		p_Ohmsgraph_6[iVertex] = Tzz;

		p_Ohmsgraph_7[iVertex] = Tzy*p_Ohmsgraph_2[iVertex];
		p_Ohmsgraph_8[iVertex] = Tzz*p_Ohmsgraph_3[iVertex];
		p_Ohmsgraph_9[iVertex] = Tzz*p_Ohmsgraph_4[iVertex];
		p_Ohmsgraph_10[iVertex] = p_Ohmsgraph_7[iVertex] + p_Ohmsgraph_8[iVertex] + p_Ohmsgraph_9[iVertex];

		if (iVertex == VERTCHOSEN) printf("\n\nOhmsgraphs info %d : omega %1.8E %1.8E %1.8E abc %1.8E %1.8E %1.8E\n"
			"nu_eiBar nu_eHeart nu_en %1.10E %1.10E %1.10E nu 1 and 2 %1.9E %1.9E ; \naccels 2 3 4 %1.9E %1.9E %1.9E\n"
			"Tzy Tzz %1.9E %1.9E  prediction %1.9E \n",
			iVertex, omega_ce.x, omega_ce.y, omega_ce.z,
			a_, b_, c_,
			nu_eiBar, nu_eHeart, n_use.n_n*s_en_visc*electron_thermal,
			p_Ohmsgraph_0[iVertex],p_Ohmsgraph_1[iVertex], p_Ohmsgraph_2[iVertex], p_Ohmsgraph_3[iVertex], p_Ohmsgraph_4[iVertex],
			Tzy, Tzz, p_Ohmsgraph_10[iVertex]			
			);

		v4 vie_kplus = p_vie_kplus[iVertex];

		p_Ohmsgraph_11[iVertex] = p_Ohmsgraph_10[iVertex] - vie_k.vez + vie_k.viz;
		p_Ohmsgraph_12[iVertex] = vie_kplus.vez - vie_kplus.viz - vie_k.vez + vie_k.viz;
		
		p_Ohmsgraph_14[iVertex] = -q*n_use.n*p_Ohmsgraph_10[iVertex];
		p_Ohmsgraph_15[iVertex] = eoverm*n_use.n*Tzy;
		p_Ohmsgraph_16[iVertex] = eoverm*n_use.n*Tzz;
		p_Ohmsgraph_17[iVertex] = -q*n_use.n*Tzz*p_Ohmsgraph_3[iVertex];
		p_Ohmsgraph_18[iVertex] = p_Ohmsgraph_14[iVertex] - q*n_use.n*(vie_kplus.viz - vie_kplus.vez);
				
	} else {
		// Non-domain triangle or vertex
		// ==============================	
	}
}

__global__ void MeasureAccelz(
	structural * __restrict__ p_info,
	v4 * __restrict__ p_vie_initial,
	v4 * __restrict__ p_vie_final,
	f64_vec3 * __restrict__ p_v_nk,
	f64_vec3 * __restrict__ p_v_nkplus1,

	f64 const h_use, // substep
	f64_vec2 * __restrict__ pGradAz,
	f64_vec2 * __restrict__ pGradTe,
	AAdot * __restrict__ p_AAdot,
	AAdot * __restrict__ p_AAdot_k,
	f64 * __restrict__ pLapAz,

	nvals * __restrict__ p_n_central,
	T3 * __restrict__ p_T_central,
	f64_vec3 * __restrict__ p_B,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	f64_vec3 * __restrict__ p_MAR_neut,
	f64 * __restrict__ p_AreaMinor,

	f64 * __restrict__ p_arelz,
	f64 * __restrict__ p_MAR_ion_effect,
	f64 * __restrict__ p_MAR_elec_effect,
	f64 * __restrict__ p_Ezext_electromotive,
	f64 * __restrict__ p_inductive_electromotive,
	f64 * __restrict__ p_vxB,
	f64 * __restrict__ p_thermal_force_effect,
	f64 * __restrict__ p_friction_neutrals,
	f64 * __restrict__ p_friction_ei,
	f64 * __restrict__ p_sum_of_effects,
	f64 * __restrict__ p_difference
) {
	long iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	structural info = p_info[iVertex];

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == DOMAIN_TRIANGLE)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		
		v4 vie_i = p_vie_initial[iVertex];
		v4 vie_f = p_vie_final[iVertex];
		f64 accel;
		p_arelz[iVertex] = (vie_f.vez - vie_f.viz - vie_i.vez + vie_i.viz) / h_use;

		f64_vec2 Grad_Az = pGradAz[iVertex];
		f64_vec2 gradTe = pGradTe[iVertex];
		f64 Azdot = p_AAdot[iVertex].Azdot;
		f64 dAzdt_k = p_AAdot_k[iVertex].Azdot;
		f64 AreaMinor = p_AreaMinor[iVertex];
		nvals n_use = p_n_central[iVertex];

		if (iVertex == VERTCHOSEN) printf("iVertex = %d BOC = %d sum = %d \n",
			iVertex, BEGINNING_OF_CENTRAL, iVertex + BEGINNING_OF_CENTRAL);

		f64_vec3 MAR, MAR_ion, MAR_elec;

		memcpy(&MAR_ion, p_MAR_ion + iVertex, sizeof(f64_vec3));		
		p_MAR_ion_effect[iVertex] = -MAR_ion.z / (n_use.n*AreaMinor); // note minus

		memcpy(&MAR_elec, p_MAR_elec + iVertex, sizeof(f64_vec3));
		p_MAR_elec_effect[iVertex] = MAR_elec.z / (n_use.n*AreaMinor); 

		p_Ezext_electromotive[iVertex] = -(eoverm + qoverM) * GetEzShape(info.pos.modulus()) * Ez_strength;

		p_inductive_electromotive[iVertex] = (eoverm + qoverM) *Azdot / c;

		p_vxB[iVertex] = (qovermc+qoverMc)*Grad_Az.dot(vie_f.vxy);

		f64_vec3 omega_ce; 
		omega_ce.x = p_B[iVertex].x*qovermc;
		omega_ce.y = p_B[iVertex].y*qovermc;
		omega_ce.z = BZ_CONSTANT*qovermc;
		f64 cross_section_times_thermal_en, cross_section_times_thermal_in,
			nu_eiBar, nu_eHeart;
		T3 T = p_T_central[iVertex];
		
		f64 sqrt_Te, ionneut_thermal, electron_thermal,
				lnLambda, s_in_MT, s_en_MT, s_en_visc;
		sqrt_Te = sqrt(T.Te);
		ionneut_thermal = sqrt(T.Ti / m_ion + T.Tn / m_n); // hopefully not sqrt(0)
		electron_thermal = sqrt_Te * over_sqrt_m_e;
		lnLambda = Get_lnLambda_d(n_use.n, T.Te);
		{
			f64 s_in_visc_dummy;
			Estimate_Ion_Neutral_Cross_sections_d(T.Ti*one_over_kB, &s_in_MT, &s_in_visc_dummy);
		}
		Estimate_Ion_Neutral_Cross_sections_d(T.Te*one_over_kB, &s_en_MT, &s_en_visc);
		//nu_ne_MT = s_en_MT * electron_thermal * n_use.n; // have to multiply by n_e for nu_ne_MT
		//nu_ni_MT = s_in_MT * ionneut_thermal * n_use.n;
		//nu_in_MT = s_in_MT * ionneut_thermal * n_use.n_n;
		//nu_en_MT = s_en_MT * electron_thermal * n_use.n_n;

		s_en_MT *= ArtificialUpliftFactor_MT(n_use.n, n_use.n_n);
		s_in_MT *= ArtificialUpliftFactor_MT(n_use.n, n_use.n_n); // returns factor 1.0 if n+nn > 1.0e14.

		cross_section_times_thermal_en = s_en_MT * electron_thermal;
		cross_section_times_thermal_in = s_in_MT * ionneut_thermal;
		
		nu_eiBar = nu_eiBarconst * kB_to_3halves*max(n_use.n, MINIMUM_NU_EI_DENSITY)*lnLambda / (T.Te*sqrt_Te);
		nu_eHeart = 1.87*nu_eiBar + n_use.n_n*s_en_visc*electron_thermal;
		f64 nu_ei_effective =
			nu_eiBar * (1.0 - 0.9*nu_eiBar*(nu_eHeart*nu_eHeart + omega_ce.z*omega_ce.z) /
			                (nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce) )) );


			// ARTIFICIAL CHANGE TO STOP IONS SMEARING AWAY OFF OF NEUTRAL BACKGROUND:
		if (n_use.n_n > ARTIFICIAL_RELATIVE_THRESH *n_use.n) {
			cross_section_times_thermal_en *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
			cross_section_times_thermal_in *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
		}
		
		p_thermal_force_effect[iVertex] =
			// viz part:
			-(1.5*nu_eiBar*(
			(omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
				(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
				(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)))
				)
			// vez part:
			- 1.5*nu_eiBar*(
			(omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
			(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
				(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)));
		;

		f64_vec3 v_nk = p_v_nk[iVertex];
		f64_vec3 v_nkplus1 = p_v_nkplus1[iVertex];

		// Is this the right sign?
		p_friction_neutrals[iVertex] =
			M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n)*
			(p_v_nkplus1[iVertex].z - vie_f.vez)
			- M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n)*
			(p_v_nkplus1[iVertex].z - vie_f.viz);

		p_friction_ei[iVertex] = -(1.0 + moverM)*nu_ei_effective*(vie_f.vez-vie_f.viz);

		p_sum_of_effects[iVertex] = 
			p_MAR_ion_effect[iVertex] + p_MAR_elec_effect[iVertex] +
			p_Ezext_electromotive[iVertex] + p_inductive_electromotive[iVertex] +
			p_vxB[iVertex] + p_thermal_force_effect[iVertex] +
			p_friction_neutrals[iVertex] + p_friction_ei[iVertex]			
			; 
		// should equal acceleration that obtained. Is it different??
		p_difference[iVertex] = p_arelz[iVertex] - p_sum_of_effects[iVertex];
		
		if (TEST_VS_MATRIX2) {
			printf("vie_f.vez %1.10E vie_i.vez %1.10E vie_f.viz %1.8E vie_i.viz %1.8E \narelz %1.13E hsub %1.9E \n  sum %1.13E diff %1.8E \n\n",
				vie_f.vez, vie_i.vez, vie_f.viz, vie_i.viz, p_arelz[iVertex], h_use, p_sum_of_effects[iVertex], p_difference[iVertex]);
			printf("effects: %1.8E %1.8E Ez %1.8E %1.8E vxB %1.8E thermalforce %1.8E fric %1.8E %1.8E\n\n######################\n\n",
				p_MAR_ion_effect[iVertex], p_MAR_elec_effect[iVertex],
				p_Ezext_electromotive[iVertex], p_inductive_electromotive[iVertex],
				p_vxB[iVertex], p_thermal_force_effect[iVertex],
				p_friction_neutrals[iVertex], p_friction_ei[iVertex]);
		

			//// Now consider an intermediate formula:
			//f64 beta_ie_z = (h_use*h_use*4.0*M_PI*qoverM*q*n_use.n
			//	+ h_use*qoverMc*(Grad_Az.dot(ohm.beta_xy_z))
			//	+ h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *ohm.beta_ne
			//	+ h_use * moverM*nu_ei_effective) / denom;

			//f64 denom = 1.0 + h_use*h_use*q*eoverm*FOUR_PI*n_use.n
			//	+ M_n_over_ne*h_use*(cross_section_times_thermal_en*n_use.n_n)
			//	+ h_use*nu_ei_effective				
			//	+ h_use*qovermc*(Grad_Az.dot(ohm.beta_xy_z)))*(1.0 - beta_ie_z)				
			//	;

			//f64 vez_test_2 = vie_i.vez
			//	+ h_use*p_MAR_elec_effect[iVertex]
			//	+ h_use*(-(eoverm) * GetEzShape(info.pos.modulus()) * Ez_strength)//p_Ezext_electromotive[iVertex]				
			//	+ h_use*qovermc*(dAzdt_k + h_use*c*c*(pLapAz[iVertex] + FOURPI_OVER_C*q*n_use.n*vie_f.viz))								
			//	+ h_use*((qovermc + qoverMc)*Grad_Az.dot(vie_f.vxy)) //p_vxB[iVertex]
			//	+ h_use*(-1.5*nu_eiBar*(
			//	(omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
			//		(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
			//		(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))))//p_thermal_force_effect[iVertex]
			//	- M_n_over_ne*h_use*(cross_section_times_thermal_en*n_use.n_n)*(-v_nkplus1.z)
			//	- h_use*nu_ei_effective*(-vie_f.viz);

			//vez_test_2 /= denom;

			//if (iVertex == VERTCHOSEN) {
			//	printf("MAR elec component %1.12E \n"
			//		"Ez ext component %1.12E \n",
			//		"dAz/dt component %1.12E vie_f.viz %1.12E\n",
			//		"v x B component %1.12E \n",
			//		"thermal force component %1.12E \n",
			//		"- M_n_over_ne*h_use*nu_en*(-v_nkplus1.z) %1.12E \n"
			//		"- h_use*nu_ei_effective*(-vie_f.viz) %1.12E \n"
			//		"denom %1.12E \n------------------\nresult vez = %1.12E",
			//		h_use*p_MAR_elec_effect[iVertex],
			//		h_use*(-(eoverm)* GetEzShape(info.pos.modulus()) * Ez_strength),
			//		h_use*qovermc*(dAzdt_k + h_use*c*c*(pLapAz[iVertex] + FOURPI_OVER_C*q*n_use.n*vie_f.viz)),
			//		vie_f.viz,
			//		h_use*((qovermc + qoverMc)*Grad_Az.dot(vie_f.vxy)), //p_vxB[iVertex]
			//		h_use*(-1.5*nu_eiBar*(
			//		(omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
			//			(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
			//			(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))))//p_thermal_force_effect[iVertex]
			//		,
			//		-M_n_over_ne*h_use*(cross_section_times_thermal_en*n_use.n_n)*(-v_nkplus1.z),
			//		-h_use*nu_ei_effective*(-vie_f.viz),
			//		denom, vez_test_2
			//	);
			//}

			// Recreate vie_f from components:

			f64 vdifftest_1 = vie_i.vez - vie_i.viz + h_use*(p_MAR_ion_effect[iVertex] + p_MAR_elec_effect[iVertex] +
				p_Ezext_electromotive[iVertex] + p_inductive_electromotive[iVertex] +
				p_vxB[iVertex] + p_thermal_force_effect[iVertex] +
				p_friction_neutrals[iVertex] + p_friction_ei[iVertex]);
			
			// Produce vez 1 : make it simpler.

			f64 vez_1 = vie_i.vez + h_use*p_MAR_ion_effect[iVertex] 

				+h_use*p_MAR_elec_effect[iVertex]

				+ h_use*(-(eoverm)* GetEzShape(info.pos.modulus()) * Ez_strength)//p_Ezext_electromotive[iVertex]

				+ h_use*qovermc*(dAzdt_k + h_use*c*c*(pLapAz[iVertex] + FOURPI_OVER_C*q*n_use.n*vie_f.viz))

				+ h_use*((qovermc + qoverMc)*Grad_Az.dot(vie_f.vxy)) //p_vxB[iVertex]

				+ h_use*(-1.5*nu_eiBar*(
				(omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
					(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
					(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))))//p_thermal_force_effect[iVertex]

				- M_n_over_ne*h_use*(cross_section_times_thermal_en*n_use.n_n)*(vie_f.vez-v_nkplus1.z)

				- h_use*nu_ei_effective*(vie_f.vez-vie_f.viz);


		//	f64 vdiff2 = vez_test_2 - vie_f.viz;

			printf("vie_i.vez vie_f.vez diff | vdifftest_1 "//veztest_2 vdiff2
				"\n%1.14E %1.14E %1.14E %1.14E vez1 %1.14E\n",
					vie_i.vez, vie_f.vez, vie_f.vez-vie_f.viz, vdifftest_1, //vez_test_2, vdiff2, 
				vez_1);

			printf("Azdot_k+1 %1.14E calc'd %1.14E dA/dt_k %1.10E LapAz %1.10E 4pi/c Jz %1.10E n %1.14E vie_f.viz %1.14E vie_f.vez %1.14E\n",
				Azdot,
				dAzdt_k + h_use*c*c*(pLapAz[iVertex] + FOURPI_OVER_C*q*n_use.n*(vie_f.viz - vie_f.vez)),
				dAzdt_k, pLapAz[iVertex], FOURPI_OVER_C*q*n_use.n*(vie_f.viz - vie_f.vez),
				n_use.n, vie_f.viz, vie_f.vez);


			// Result : difference 2 is closer to the program difference. Diff 1 is quite different.
			// Explain diff between diff 2 and diff 1.





			// Magic up matrix eqn:

			if (TEST_VS_MATRIX2) {
				


				memcpy(&MAR, p_MAR_neut + iVertex, sizeof(f64_vec3));
				// 1. Need to work out vn coefficients !!!
				f64 nu_ne_MT = cross_section_times_thermal_en*n_use.n;
				f64 nu_ni_MT = cross_section_times_thermal_in*n_use.n;
				f64 nu_in_MT = cross_section_times_thermal_in*n_use.n_n;
				f64 nu_en_MT = cross_section_times_thermal_en*n_use.n_n;

				f64 denom = 1.0 + h_use*M_e_over_en*nu_ne_MT +
					h_use*M_i_over_in*nu_ni_MT;
				f64_vec3 vn0 = (v_nk + h_use*MAR / (AreaMinor*n_use.n_n))
					/ denom;
				f64	beta_ne = h_use*M_e_over_en*nu_ne_MT / denom;
				f64	beta_ni = h_use*M_i_over_in*nu_ni_MT / denom;

				printf("v_nk.xy %1.14E %1.14E MAR.xy %1.14E %1.14E Nn %1.14E Area %1.14E denom %1.14E\n", v_nk.x,
					v_nk.y, MAR.x, MAR.y, (AreaMinor*n_use.n_n), AreaMinor, denom);
				printf("vn0 %1.14E %1.14E %1.14E beta_ni %1.14E beta_ne %1.14E \n",
					vn0.x, vn0.y, vn0.z, beta_ni, beta_ne);



				// vx, vy :
				// from bwd eqn :

				// given Lap Az and EzStrength, (Azdot -- do both ways) :
				// the Azdot we got given used the vie_f that got calculated, so no we have to go back to Lap Az.


				// Do without Ez terms. Put into separate sigma_izz, sigma_ezz


				// 2. vx equation?

				f64 temp = (h_use / (m_i + m_e))*
					(m_n*m_i*nu_in_MT / (m_i + m_n) + m_e*m_n*nu_en_MT / (m_e + m_n));
			
				f64 M_i_over_ie = m_i / (m_i + m_e);
				f64 M_e_over_ie = m_e / (m_i + m_e);
				f64 M_n_over_in = m_n / (m_i + m_n);
				f64 M_n_over_en = m_n / (m_e + m_n);

				f64_vec2 vxy0 = vie_i.vxy
					+ h_use*M_i_over_ie*MAR_ion.xypart()/(AreaMinor*n_use.n)
					+ h_use*M_e_over_ie*MAR_elec.xypart()/(AreaMinor*n_use.n)
					+ temp*vn0.xypart(); // added

				printf("vxy0 components:\n"
					"vie_i.vxy %1.14E %1.14E  MAR_ion_contrib %1.14E %1.14E \n"
					"MAR_elec_contrib %1.14E %1.14E  temp %1.14E vn0contrib %1.14E %1.14E\n\n",
					vie_i.vxy.x, vie_i.vxy.y,
					h_use*M_i_over_ie*MAR_ion.x / (AreaMinor*n_use.n),
					h_use*M_i_over_ie*MAR_ion.y / (AreaMinor*n_use.n),
					h_use*M_e_over_ie*MAR_elec.x / (AreaMinor*n_use.n),
					h_use*M_e_over_ie*MAR_elec.y / (AreaMinor*n_use.n),
					temp,
					temp*vn0.x, temp*vn0.y
				);

				f64 vx_viz = (h_use*q / (c*(m_i + m_e)))*Grad_Az.x;				
				f64 vx_vez = (-h_use*q / (c*(m_i + m_e)))*Grad_Az.x;

				f64 vy_viz = (h_use*q / (c*(m_i + m_e)))*Grad_Az.y;
				f64 vy_vez = (-h_use*q / (c*(m_i + m_e)))*Grad_Az.y;

				f64 vxy_vxy = -temp*(1.0- beta_ne - beta_ni);

				vxy_vxy -= 1.0; // move LHS over to RHS so we've got 0 = .
 
				printf("    ...     1   vx   vy   viz   vez \n");
				printf("    vx     %1.14E %1.14E %1.14E %1.14E %1.14E \n"
					   "    vy     %1.14E %1.14E %1.14E %1.14E %1.14E \n",
					vxy0.x, vxy_vxy, 0.0, vx_viz, vx_vez,
					vxy0.y, 0.0, vxy_vxy, vy_viz, vy_vez
					);

				// VERIFY AGAIN THAT THIS IS GIVING SAME COEFFICIENTS.

				// Work systematically: reduce vxy equation and sub in.

				denom = -vxy_vxy; // move to LHS .. 
				OhmsCoeffs ohm;
				v4 v0;


				v0.vxy = vxy0 / denom;
				ohm.beta_xy_z.x = vx_viz/denom;
				ohm.beta_xy_z.y = vy_viz/denom;

				printf("=-----------------\nv0.vxy %1.14E %1.14E beta_xy_z %1.14E %1.14E \n---------------\n",
					v0.vxy.x, v0.vxy.y, ohm.beta_xy_z.x, ohm.beta_xy_z.y);



				f64 EzExt = Ez_strength*GetEzShape(info.pos.modulus());

				// Worry afterwards about what sigma Ez does.
				// Do this in stages.

				f64 viz0 = vie_i.viz + h_use*MAR_ion.z / (AreaMinor*n_use.n)
					+ h_use*qoverM*EzExt
					- h_use*qoverMc*(dAzdt_k + h_use*c*c*(pLapAz[iVertex]))
					+ h_use*1.5*nu_eiBar*
					((omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
						(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
						(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)))				
					+ h_use*M_n_over_in*nu_in_MT*vn0.z;


				printf("viz0 components: vie_i.viz %1.14E \n"
					"from MAR_ion.z : %1.14E  |  from EzExt %1.14E \n"
					"from Azdot_k+hc^2LapAz %1.14E \n"
					"thermalforceterm %1.14E \n"
					"vn0.z effect %1.14E \n"
					"total = viz0 : %1.14E \n",
					vie_i.viz, h_use*MAR_ion.z / (AreaMinor*n_use.n),
					h_use*qoverM*EzExt,
					-h_use*qoverMc*(dAzdt_k + h_use*c*c*(pLapAz[iVertex])),
					h_use*1.5*nu_eiBar*
					((omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
					(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
						(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))),
					h_use*M_n_over_in*nu_in_MT*vn0.z, viz0
				);

				f64 viz_vx = -h_use*qoverMc*Grad_Az.x;
				f64 viz_vy = -h_use*qoverMc*Grad_Az.y;

				f64 viz_viz = -h_use*qoverM*h_use*4.0*M_PI*q*n_use.n

					-h_use*M_n_over_in*nu_in_MT*(1.0-beta_ni)
					- h_use*moverM*nu_ei_effective
					
					;
				f64 viz_vez = h_use*qoverM*h_use*4.0*M_PI*q*n_use.n
					+ h_use*M_n_over_in*nu_in_MT*(beta_ne)
					+ h_use*moverM*nu_ei_effective;

				// Think about how it will be solved.

				// Eqn: vxvx vx + vxvy vy + .. = -vx0.
				// And here vxvx should include -1 vx

				viz_viz -= 1.0;

				printf("    viz    %1.14E %1.14E %1.14E %1.14E %1.14E \n",
					viz0, viz_vx, viz_vy, viz_viz, viz_vez);



				f64 vez0 = vie_i.vez + h_use*MAR_elec.z / (AreaMinor*n_use.n)
					- h_use*eoverm*EzExt
					+ h_use*qovermc*(dAzdt_k + h_use*c*c*pLapAz[iVertex])
					- h_use*1.5*nu_eiBar*
					((omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
					(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
						(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce)))

					+ h_use*M_n_over_en*nu_en_MT*vn0.z;

				printf("\nvez0 : vez_k %1.14E \n"
					"MAReffect %1.14E \n"
					"EzExteffect %1.14E \n"
					"Azdot_effect %1.14E \n"
					"thermalforceeffect %1.14E \n"
					"vn0.z_effect %1.14E \n"
					"vez0 %1.14E\n",
					vie_i.vez,
					h_use*MAR_elec.z / (AreaMinor*n_use.n),
					-h_use*eoverm*EzExt,
					h_use*qovermc*(dAzdt_k + h_use*c*c*pLapAz[iVertex]),
					-h_use*1.5*nu_eiBar*
					((omega_ce.x*omega_ce.z - nu_eHeart * omega_ce.y)*gradTe.x +
					(omega_ce.y*omega_ce.z + nu_eHeart * omega_ce.x)*gradTe.y) /
						(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega_ce.dot(omega_ce))),
					h_use*M_n_over_en*nu_en_MT*vn0.z,
					vez0
				);


				f64 vez_vx = h_use*qovermc*Grad_Az.x;
				f64 vez_vy = h_use*qovermc*Grad_Az.y;
				f64 vez_viz = h_use*eoverm*h_use*4.0*M_PI*q*n_use.n 
					+ h_use*M_n_over_en*nu_en_MT*beta_ni
					+ h_use*nu_ei_effective;
				f64 vez_vez = -h_use*eoverm*h_use*4.0*M_PI*q*n_use.n
					- h_use*M_n_over_en*nu_en_MT*(1.0-beta_ne)
					- h_use*nu_ei_effective;
				
				printf("\n vez_vx %1.14E vez_vez %1.14E \n"
					"vezvez components: hhq4piqn/m %1.14E hnu_en %1.14E hnu_ei %1.14E\n\n",
					vez_vx, vez_vez, -h_use*eoverm*h_use*4.0*M_PI*q*n_use.n,
					- h_use*M_n_over_en*nu_en_MT*(1.0 - beta_ne),
					- h_use*nu_ei_effective
					);

				// FIGURE IT OUT : WHAT DOES THIS GIVE AND DOES IT SATISFY BWD EQN
				// It practically is the bwd eqn
				// If the result from our PopOhms is different, should be possible to detect exactly why.
				// We do not take Azdot_k+1 as given but this also should be possible to compute
				// given the v_k+1 from this.
				// This is bound to work.
				
				vez_vez -= 1.0;

				printf("    vez    %1.14E %1.14E %1.14E %1.14E %1.14E \n",
					vez0, vez_vx, vez_vy, vez_viz, vez_vez);
				printf(" ----------------------------------------------------\n");
					

				// Work systematically: reduce vxy equation and sub in.

				denom = -vxy_vxy; // move to LHS .. 

				vxy0.x /= denom;
				vxy0.y /= denom;
				vx_viz /= denom;
				vx_vez /= denom;
				vy_viz /= denom;
				vy_vez /= denom;

				// Substitute in:
				viz0 += viz_vx*vxy0.x + viz_vy*vxy0.y;
				viz_viz += viz_vx*vx_viz + viz_vy*vy_viz;
				viz_vez += viz_vx*vx_vez + viz_vy*vy_vez;

				vez0 += vez_vx*vxy0.x + vez_vy*vxy0.y;
				vez_viz += vez_vx*vx_viz + vez_vy*vy_viz;
				vez_vez += vez_vx*vx_vez + vez_vy*vy_vez;
				
				printf("    viz0 vizviz vizvez    %1.14E %1.14E %1.14E \n",
					viz0, viz_viz, viz_vez);
				printf("    vez0 vezviz vezvez    %1.14E %1.14E %1.14E \n",
					vez0, vez_viz, vez_vez);
				printf(" ----------------------------------------------------\n");
				printf("REDUCE viz:\n");

				viz0 /= viz_viz;
				printf("viz0 %1.14E vizvez %1.14E \n", viz0, viz_vez);

				printf(" ----------------------------------------------------\n");



			};
		};
	};
}

__global__ void kernelPopulateBackwardOhmsLaw_noadvect(
	f64 h_use,
	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	f64_vec3 * __restrict__ p_B,
	f64 * __restrict__ p_LapAz,
	f64_vec2 * __restrict__ p_GradAz,
	f64_vec2 * __restrict__ p_GradTe,
	nvals * __restrict__ p_n_minor_use,	
	T3 * __restrict__ p_T_minor_use,
	v4 * __restrict__ p_vie_src,
	f64_vec3 * __restrict__ p_v_n_src,
	AAdot * __restrict__ p_AAdot_src,
	f64 * __restrict__ p_AreaMinor,

	f64_vec3 * __restrict__ p_vn0_dest,
	v4 * __restrict__ p_v0_dest,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs_dest,
	//AAdot * __restrict__ p_AAdot_intermediate,

	f64 * __restrict__ p_Iz0,
	f64 * __restrict__ p_sigma_zz,

	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_denom_e,
	f64 * __restrict__ p_effect_of_viz0_on_vez0,
	f64 * __restrict__ p_beta_ie_z,

	bool const bSwitchSave) 
{
	// Don't forget we can use 16KB shared memory to save a bit of overspill:
	// (16*1024)/(512*8) = 4 doubles only for 512 threads. 128K total register space per SM we think.

	__shared__ f64 Iz[threadsPerTileMinor], sigma_zz[threadsPerTileMinor];
	//	__shared__ f64 Iz_k[threadsPerTileMinor];

	__shared__ f64_vec2 omega[threadsPerTileMinor], grad_Az[threadsPerTileMinor],
		gradTe[threadsPerTileMinor];

	// Putting 8 reduces to 256 simultaneous threads. Experiment with 4 in shared.
	// f64 viz0_coeff_on_Lap_Az, vez0_coeff_on_Lap_Az; // THESE APPLY TO FEINT VERSION. ASSUME NOT FEINT FIRST.

	v4 v0;
	f64 denom, AreaMinor;
	f64_vec3 vn0;
	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX
	structural info = p_info_minor[iMinor];

	// Can see no reason not to put OUTERMOST here. No point creating a big gradient of vz to it.

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == DOMAIN_TRIANGLE)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 vie_k = p_vie_src[iMinor];
		f64_vec3 v_n_src = p_v_n_src[iMinor];
		nvals n_use = p_n_minor_use[iMinor];
		AreaMinor = p_AreaMinor[iMinor];
		// Are we better off with operator = or with memcpy?
		vn0 = v_n_src;

		//		if ((TESTTRI)) printf("GPU %d vie_k %1.14E %1.14E\n", iMinor, vie_k.vxy.x, vie_k.vxy.y);
		{
			f64_vec3 MAR;
			memcpy(&MAR, p_MAR_neut + iMinor, sizeof(f64_vec3));
			// CHECK IT IS INTENDED TO AFFECT Nv

			if (TEST_VS_MATRIX) {
				printf("%d VS_MAT: v_nk.y %1.14E MAR.y %1.14E Nn %1.14E Area %1.14E\n",
					iMinor, vn0.y, MAR.y, (AreaMinor*n_use.n_n), AreaMinor);

				printf("%d VS_MAT: v_nk.x %1.14E MAR.x %1.14E Nn %1.14E \n",
					iMinor, vn0.x, MAR.x, (AreaMinor*n_use.n_n));
			};

			// REVERTED THE EDIT TO USE 1/n -- THIS WILL NOT GIVE CORRECT M.A.R. EFFECT ON INTEGRAL nv
			// We need conservation laws around shock fronts.
			vn0.x += h_use * (MAR.x / (AreaMinor*n_use.n_n));			// p_one_over_n[iMinor].n_n/ (AreaMinor));
			vn0.y += h_use * (MAR.y / (AreaMinor*n_use.n_n));// MomAddRate is addition rate for Nv. Divide by N.
			vn0.z += h_use * (MAR.z / (AreaMinor*n_use.n_n));
			
			memcpy(&MAR, p_MAR_ion + iMinor, sizeof(f64_vec3));
			v0.vxy = vie_k.vxy + h_use * (m_i*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
			v0.viz = vie_k.viz + h_use * MAR.z / (n_use.n*AreaMinor);

			if (TEST_VS_MATRIX) {
				printf("%d VS_MAT viz_k %1.14E viz0 with MAR %1.14E \n",
					iMinor, vie_k.viz, v0.viz);
			}

			if (TESTACCEL_X) printf("%d vx_k %1.9E with MARi %1.9E n %1.8E N %1.8E\n", iMinor, vie_k.vxy.x, v0.vxy.x,
				n_use.n, n_use.n*AreaMinor);
			if (TESTACCEL2) printf("%d vy_k %1.9E with MARi %1.9E MAR.y %1.9E\n", iMinor-BEGINNING_OF_CENTRAL, vie_k.vxy.y, v0.vxy.y,
				MAR.y);

			memcpy(&MAR, p_MAR_elec + iMinor, sizeof(f64_vec3));
			v0.vxy += h_use * (m_e*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
			v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);   

			if (TESTVEZ) printf("%d vez_k %1.9E MAR.z %1.9E N %1.9E \n",
				iMinor, vie_k.vez, MAR.z, (n_use.n*AreaMinor));

			if (TESTACCEL_X) printf("%d v0x with MARi+e %1.9E\n", iMinor - BEGINNING_OF_CENTRAL, v0.vxy.x);
			if (TESTACCEL2) printf("%d v0y with MARi+e %1.9E MAR.y \n", iMinor - BEGINNING_OF_CENTRAL, v0.vxy.y, MAR.y);

			if (v0.vez != v0.vez) printf("NANVEZ %d v_k %1.9E MAR.z %1.9E \n", iMinor, vie_k.vez, MAR.z);

			if (TESTVEZ) printf("\nGPU %d MAR: changexy %1.10E %1.10E vezchange %1.10E Area %1.10E v0.vez %1.9E vie_k.vez %1.9E\n", iMinor,
				h_use * (m_e*MAR.x / (n_use.n*(m_i + m_e)*AreaMinor)),
				h_use * (m_e*MAR.y / (n_use.n*(m_i + m_e)*AreaMinor)),
				h_use * MAR.z / (n_use.n*AreaMinor),
				AreaMinor, v0.vez, vie_k.vez);
			
		}

		OhmsCoeffs ohm;
		f64 beta_ie_z, LapAz;
		f64 cross_section_times_thermal_en, cross_section_times_thermal_in,
			nu_eiBar, nu_eHeart;
		T3 T = p_T_minor_use[iMinor];
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal,
				lnLambda, s_in_MT, s_en_MT, s_en_visc;
			sqrt_Te = sqrt(T.Te);
			ionneut_thermal = sqrt(T.Ti / m_ion + T.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_use.n, T.Te);
			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T.Ti*one_over_kB, &s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T.Te*one_over_kB, &s_en_MT, &s_en_visc);

			//nu_ne_MT = s_en_MT * electron_thermal * n_use.n; // have to multiply by n_e for nu_ne_MT
			//nu_ni_MT = s_in_MT * ionneut_thermal * n_use.n;
			//nu_in_MT = s_in_MT * ionneut_thermal * n_use.n_n;
			//nu_en_MT = s_en_MT * electron_thermal * n_use.n_n;

			cross_section_times_thermal_en = s_en_MT * electron_thermal;
			cross_section_times_thermal_in = s_in_MT * ionneut_thermal;

			nu_eiBar = nu_eiBarconst * kB_to_3halves*max(MINIMUM_NU_EI_DENSITY,n_use.n)*lnLambda / (T.Te*sqrt_Te);
						
			nu_eHeart = 1.87*nu_eiBar + n_use.n_n*s_en_visc*electron_thermal; 
			
			// Confusing, why does this say that? We used visc en in nu_eHeart, explanation?

			if (nu_eiBar != nu_eiBar) printf("&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
				"iMinor %d n_use.n %1.9E lnLambda %1.9E Te %1.9E sqrt %1.9E \n",
				iMinor, n_use.n, lnLambda, T.Te, sqrt_Te);

			// ARTIFICIAL CHANGE TO STOP IONS SMEARING AWAY OFF OF NEUTRAL BACKGROUND:
			if (n_use.n_n > ARTIFICIAL_RELATIVE_THRESH *n_use.n) {
				cross_section_times_thermal_en *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH*n_use.n);
				cross_section_times_thermal_in *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH*n_use.n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			};


			// ARTIFICIAL CHANGE TO STOP HAVING TO WORRY ABOUT SILLY VALUES IN AREAS THAT DON'T MATTER MUCH :
			cross_section_times_thermal_en *= ArtificialUpliftFactor_MT(n_use.n, n_use.n_n);
			cross_section_times_thermal_in *= ArtificialUpliftFactor_MT(n_use.n, n_use.n_n); // returns factor 1.0 if n+nn > 1.0e14.
			
			if (TESTVEZ) printf("Uplift factor %1.9E n %1.8E %1.8E\n", ArtificialUpliftFactor(n_use.n, n_use.n_n), n_use.n, n_use.n_n);

			// DEBUG:
			if (0)//iMinor == CHOSEN) 
			{
				printf("%d xs_therm_in en %1.8E %1.8E nn %1.8E n %1.8E s_in en %1.8E %1.8E i-n therm %1.8E Uplift %1.8E\n",
					iMinor,
					cross_section_times_thermal_in, cross_section_times_thermal_en,
					n_use.n_n, n_use.n, s_en_MT, s_in_MT, ionneut_thermal,
					ArtificialUpliftFactor_MT(n_use.n, n_use.n_n));

				//				Did not trigger? What gives?
			};
		};

		denom = 1.0 + h_use*M_e_over_en*(cross_section_times_thermal_en*n_use.n)
					+ h_use*M_i_over_in*(cross_section_times_thermal_in*n_use.n);

		if (TESTVNX) printf("%d v_n.x before divide %1.10E \n", iMinor, vn0.x);
		if (TESTVNY) printf("%d v_n.y before divide %1.10E \n", iMinor, vn0.y);
		vn0 /= denom; // It is now the REDUCED value
		if (TESTVNX) printf("%d v_n.x after divide %1.10E \n", iMinor, vn0.x);
		if (TESTVNY) printf("%d v_n.y after divide %1.10E \n", iMinor, vn0.y);

		//if (iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL) {
		//	printf("vn_k %1.9E %1.9E %1.9E vn0 %1.9E %1.9E %1.9E denom %1.9E\n",
		//		v_n_src.x, v_n_src.y, v_n_src.z, vn0.x, vn0.y, vn0.z, denom);
		//};

		ohm.beta_ne = h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n) / denom;
		ohm.beta_ni = h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n) / denom;

		if (TEST_VS_MATRIX) printf("VS_MAT: vn0 %1.14E %1.14E %1.14E beta_ni %1.14E beta_ne %1.14E denom %1.14E\n",
			vn0.x, vn0.y, vn0.z, ohm.beta_ni, ohm.beta_ne, denom);

		// Now we do vexy:

		grad_Az[threadIdx.x] = p_GradAz[iMinor];
		gradTe[threadIdx.x] = p_GradTe[iMinor];
		LapAz = p_LapAz[iMinor];
		
		// debug:
		if (LapAz != LapAz) printf("----------\n%d LapAz NaN\n---------\n", iMinor);
				
		if (((TESTTRI))) printf("GPU %d: LapAz %1.14E\n", CHOSEN, LapAz);

		v0.vxy +=
			  (h_use / ((m_i + m_e)))*(m_n*M_i_over_in*(cross_section_times_thermal_in*n_use.n_n)
				+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*
				( vn0.xypart()); // this reflects v_n and the next reflects minus itself
				
		denom = 1.0 + (h_use / (m_i + m_e))*(
			m_n* M_i_over_in* (cross_section_times_thermal_in*n_use.n_n)
			+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*
			         (1.0 - ohm.beta_ne - ohm.beta_ni);
		
		if (TEST_VS_MATRIX) printf("VS_MAT: vxy0 before divide %1.14E %1.14E denom %1.14E\n",
			v0.vxy.x, v0.vxy.y, denom);
		
		v0.vxy /= denom;

		if (TESTACCEL_X) printf("%d v0x with neut soak %1.9E\n", iMinor, v0.vxy.x);
		if (TESTACCEL2) printf("%d v0y with neut soak %1.9E\n", iMinor - BEGINNING_OF_CENTRAL, v0.vxy.y);

		ohm.beta_xy_z = (h_use * q / (c*(m_i + m_e)*denom)) * grad_Az[threadIdx.x]; // coeff on viz-vez
		
		if (TEST_VS_MATRIX) printf("VS_MAT: vxy0 %1.14E %1.14E beta_xy_z %1.14E %1.14E \n\n",
			v0.vxy.x, v0.vxy.y, ohm.beta_xy_z.x, ohm.beta_xy_z.y);

		// ================================================================================================

		omega[threadIdx.x] = qovermc*p_B[iMinor].xypart();

		f64 nu_ei_effective = nu_eiBar * (1.0 - 0.9*nu_eiBar*(nu_eHeart*nu_eHeart + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT) /
			(nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].x*omega[threadIdx.x].x + omega[threadIdx.x].y*omega[threadIdx.x].y + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT)));

		AAdot AAzdot_k = p_AAdot_src[iMinor];

		v0.viz +=
				-h_use*qoverMc*(AAzdot_k.Azdot + h_use * c*c*LapAz)
				- h_use*qoverMc*(v0.vxy).dot(grad_Az[threadIdx.x]);// v x B

		if (TEST_VS_MATRIX) {
			printf("%d VS_MAT viz0 %1.14E Azdot+ccLapAz term %1.14E vxy.gradAz term %1.14E \n",
				iMinor, v0.viz,
				-h_use*qoverMc*(AAzdot_k.Azdot + h_use * c*c*LapAz),
				-h_use*qoverMc*(v0.vxy).dot(grad_Az[threadIdx.x])
				);
		}
		// Still omega_ce . Check formulas.
		
		v0.viz +=
			1.5*h_use*nu_eiBar*(
			(omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x])));

		v0.viz += h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *vn0.z;
		if (TEST_VS_MATRIX) {
			printf("%d VS_MAT viz0 %1.14E thermalforceterm %1.14E vn0.z for friction %1.14E \n",
				iMinor, v0.viz,
				1.5*h_use*nu_eiBar*(
				(omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
					(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
					(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x]))),
				h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *vn0.z
			);			
		}
		denom = 1.0 + h_use * h_use*4.0*M_PI*qoverM*q*n_use.n 
			+ h_use * qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) +
			h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(1.0 - ohm.beta_ni)
			+ h_use *moverM*nu_ei_effective;

		if (bSwitchSave) p_denom_i[iMinor] = denom;
		//				viz0_coeff_on_Lap_Az = -0.5*h_use*qoverMc*h_use*c*c / denom;

		v0.viz /= denom;


		if (TEST_VS_MATRIX)
			printf("Denom %1.14E = 1+components:\n"
				"h_use*h_use*4.0*M_PI*qoverM*q*n_use.n %1.14E \n"
				"h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) %1.14E \n"
				"h_use*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n)*(1.0-ohm.beta_ni) %1.14E \n"
				"h_use *moverM*nu_ei_effective %1.14E \n"
				"------------------------------------- new value of viz0 %1.14E \n"
				,
				denom,
				h_use * h_use*4.0*M_PI*qoverM*q*n_use.n,
				h_use * qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)),
				h_use*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n)*(1.0 - ohm.beta_ni),
				h_use *moverM*nu_ei_effective,
				v0.viz
			);
		//if (((TESTTRI))) printf("viz0 divided %1.14E denom %1.14E \n", v0.viz, denom);

		ohm.sigma_i_zz = h_use * qoverM / denom;
		beta_ie_z = (h_use*h_use*4.0*M_PI*qoverM*q*n_use.n
			+ h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))
			+ h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *ohm.beta_ne
			+ h_use * moverM*nu_ei_effective) / denom;

		if (TEST_VS_MATRIX)
			printf("ohm.sigma_i_zz %1.14E = hq/M / denom \n"
				"beta_ie_z %1.14E components before divide by denom:\n"
				"h_use*h_use*4.0*M_PI*qoverM*q*n_use.n %1.14E \n"
				"h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) %1.14E \n"
				"h_use*M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n)*ohm.beta_ne %1.14E \n"
				"h_use*moverM*nu_ei_effective %1.14E \n"
				"-----------------------------------------------\n",
				ohm.sigma_i_zz,
				beta_ie_z,
				h_use*h_use*4.0*M_PI*qoverM*q*n_use.n,
				h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)),
				h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *ohm.beta_ne,
				h_use * moverM*nu_ei_effective);
		
		if (TESTOHMS) printf("%d v0.vez %1.12E before Azdot LapAz and JxB\n", iMinor, v0.vez);
		


		// ====================================================================
		// vez:





		v0.vez +=
			h_use *qovermc*(AAzdot_k.Azdot
				+ h_use * c*c*(LapAz + FOURPI_Q_OVER_C*n_use.n*v0.viz))
			+ h_use*qovermc*(v0.vxy + ohm.beta_xy_z*v0.viz ).dot(grad_Az[threadIdx.x]); // v x B

		if (TESTVEZ) printf("%d AzdotLapAzcomponent(v0.viz) %1.12E v0.viz %1.12E \n"
			"v x B term (v0) %1.12E \n--------------------------\n"
			, iMinor,
			h_use *qovermc*(AAzdot_k.Azdot
				+ h_use * c*c*(LapAz + FOURPI_Q_OVER_C*n_use.n*v0.viz)), v0.viz,
			h_use*qovermc*(v0.vxy + ohm.beta_xy_z*v0.viz).dot(grad_Az[threadIdx.x])
			);

		if (TESTVEZ) printf("%d vh_use *qovermc*(AAzdot_k.Azdot) %1.14E \nhhqc_overm(LapAz) %1.14E LapAz %1.14E \n"
			"hh4piqqoverm n viz %1.14E  hq/mc v0.vxy.gradAz %1.14E hq/mc beta_xyz viz.gradAz %1.14E \n"
			"v0.vxy %1.12E %1.12E grad Az %1.12E %1.12E \n", 
			iMinor, h_use *qovermc*(AAzdot_k.Azdot), 
			h_use *qovermc*(h_use * c*c*(LapAz )),
			LapAz,
			h_use *qovermc*(h_use * c*c*(FOURPI_Q_OVER_C*n_use.n*v0.viz)),
			h_use*qovermc*v0.vxy.dot(grad_Az[threadIdx.x]),
			h_use*qovermc*(ohm.beta_xy_z*v0.viz).dot(grad_Az[threadIdx.x]),
			v0.vxy.x, v0.vxy.y, grad_Az[threadIdx.x].x, grad_Az[threadIdx.x].y
			);

		// implies:
		f64 effect_of_viz0_on_vez0 =
			h_use * qovermc*h_use * c*c* FOURPI_Q_OVER_C*n_use.n
			+ h_use*qovermc*(ohm.beta_xy_z.dot(grad_Az[threadIdx.x])); // from the instruction above

		if (TESTOHMS) printf("%d v0.vez %1.14E before thermal force\n", iMinor, v0.vez);

		v0.vez -=
			1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x]) + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT));

		if (TESTVEZ) printf("%d thermal force %1.14E \n", iMinor, -1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
			(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x]) + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT)));

		// could store this from above and put opposite -- dividing by m_e instead of m_i
		// overdue..?

		if (TESTVEZ) printf("%d v0.vez %1.12E MARKER1 \n", iMinor, v0.vez);
		 
		effect_of_viz0_on_vez0 +=
				h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni + h_use*nu_ei_effective;
		
		// Apparently we thought to save this INSTEAD of putting it into vez0
		// So the question is-- - have we deliberately excluded the effect from vez0 IN THE CASE that we are setting up a linear relationship ?
		
			// NEUE:

		v0.vez += (h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni + h_use*nu_ei_effective)*v0.viz
			+ h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n)*vn0.z;
		

		denom = 1.0 + (h_use*h_use*4.0*M_PI*q*eoverm*n_use.n
			+ h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)))*(1.0 - beta_ie_z)
			+ h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne - ohm.beta_ni * beta_ie_z)
			+ h_use*nu_ei_effective*(1.0 - beta_ie_z);

		if (TEST_VS_MATRIX)
			printf("\nPOPOHMS denom_e %1.14E components: \nhh4piqqn/m*(1.0-beta_ie_z) %1.14E grad_Az_dot_beta_xy_z %1.14E \n"
				"nu_en_without_ni_ie(1-beta_ne) %1.14E nu_en_ni_ie %1.14E\n"
				"hnu_ei_eff %1.14E times_minus_beta_ie_z %1.14E\n\n",
				denom,
				h_use*h_use*4.0*M_PI*q*eoverm*n_use.n*(1.0 - beta_ie_z),
				h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*(1.0 - beta_ie_z),
			h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne),
			h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(-ohm.beta_ni*beta_ie_z),
			h_use*nu_ei_effective,
			h_use*nu_ei_effective*(- beta_ie_z)
			);

		if (TESTVEZ) printf("%d v0.vez %1.12E nu_ei_effective %1.12E v0.viz %1.12E \n"
			"beta_ie_z %1.12E  nu_en %1.12E denom %1.12E\n", iMinor, v0.vez, nu_ei_effective, v0.viz, beta_ie_z,
			M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n), denom);



		//		vez0_coeff_on_Lap_Az = h_use * h_use*0.5*qovermc* c*c / denom; 

		ohm.sigma_e_zz =
			(-h_use * eoverm
				+ h_use * h_use*4.0*M_PI*q*eoverm*n_use.n*ohm.sigma_i_zz
				+ h_use *qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*ohm.sigma_i_zz
				+ h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni*ohm.sigma_i_zz
				+ h_use*nu_ei_effective*ohm.sigma_i_zz)
			/ denom;

		if (TESTVEZ) printf("%d grad_Az %1.9E %1.9E \n", iMinor, grad_Az[threadIdx.x].x, grad_Az[threadIdx.x].y);

		v0.vez /= denom;
		effect_of_viz0_on_vez0 /= denom; // of course 

		if (TESTVEZ) printf("%d v0.vez %1.12E after divide\n", iMinor, v0.vez);


		if (bSwitchSave) {
			p_denom_e[iMinor] = denom;
			p_effect_of_viz0_on_vez0[iMinor] = effect_of_viz0_on_vez0;
			p_beta_ie_z[iMinor] = beta_ie_z; // see that doing it this way was not best.
		} else {
			// #########################################################################################################
			// DEBUG: pass graphing parameters through these.
			// #########################################################################################################
			p_denom_i[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective;
			p_denom_e[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n /
				(M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective);
		};

		// Now update viz(Ez):
		v0.viz += beta_ie_z * v0.vez;
		ohm.sigma_i_zz += beta_ie_z * ohm.sigma_e_zz;

		// sigma_e_zz and sigma_i_zz are change in vz for a change in Ez
		{
			f64 EzShape = GetEzShape(info.pos.modulus());
			ohm.sigma_i_zz *= EzShape;
			ohm.sigma_e_zz *= EzShape;
		}

		if (TESTVEZ) printf("%d final v0.vez %1.12E sigma %1.12E \n", iMinor, v0.vez, ohm.sigma_e_zz);

		// Think maybe we should get rid of most of this routine out of the subcycle.
		// Rate of acceleration over timestep due to resistance, pressure, thermal force etc could be stored.
		// Saving off some eqn data isn't so bad when we probably overflow registers and L1 here anyway.
		// All we need is to know that we update sigma
		// We can do addition of 
		// ==============================================================================================

		p_v0_dest[iMinor] = v0;
		p_OhmsCoeffs_dest[iMinor] = ohm;
		p_vn0_dest[iMinor] = vn0;

		Iz[threadIdx.x] = q*AreaMinor*n_use.n*(v0.viz - v0.vez);
		sigma_zz[threadIdx.x] = q*AreaMinor*n_use.n*(ohm.sigma_i_zz - ohm.sigma_e_zz);
		
		// BRING THIS BACK AND CHECK IT ALL OUT:
	//	if (Iz[threadIdx.x] > 0.0) printf("%d : Iz %1.8E n_use %1.8E v0.viz %1.8E v0.vez %1.8E\n",
	//		iMinor, Iz[threadIdx.x], n_use.n, v0.viz, v0.vez);

	}
	else {
		// Non-domain triangle or vertex
		// ==============================
		// Need to decide whether crossing_ins triangle will experience same accel routine as the rest?
		// I think yes so go and add it above??
		// We said v_r = 0 necessarily to avoid sending mass into ins.
		// So how is that achieved there? What about energy loss?
		// Need to determine a good way. Given what v_r in tri represents. We construe it to be AT the ins edge so 
		// ...
		Iz[threadIdx.x] = 0.0;
		sigma_zz[threadIdx.x] = 0.0;

		memset(&(p_v0_dest[iMinor]), 0, sizeof(v4)); // no velocity!
		memset(&(p_vn0_dest[iMinor]), 0, sizeof(f64_vec3));
		memset(&(p_OhmsCoeffs_dest[iMinor]), 0, sizeof(OhmsCoeffs));


	//	if ((iMinor < BEGINNING_OF_CENTRAL) && ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)))
//		{
	//		p_AAdot_intermediate[iMinor].Azdot = 0.0;
			// Set Az equal to neighbour in every case, after Accelerate routine.
	//	}
//		else {
			// Let's make it go right through the middle of a triangle row for simplicity.

			//f64 Jz = 0.0;
			//if ((iMinor >= numStartZCurrentTriangles) && (iMinor <  numEndZCurrentTriangles))
			//{
			//	// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			//	// ASSUME we are fed Iz_prescribed.
			//	//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			//	AreaMinor = p_AreaMinor[iMinor];
			//	Jz = negative_Iz_per_triangle / AreaMinor; // Iz would come from multiplying back by area and adding.
			//};

	//		AAdot temp = p_AAdot_src[iMinor];
	//		temp.Azdot += h_use * c*(c*p_LapAz[iMinor]);// +4.0*M_PI*Jz);
														// + h_use * ROCAzdot_antiadvect // == 0
	//		p_AAdot_intermediate[iMinor] = temp; // 

	//	};
	};

	__syncthreads();

	// .Collect Jz = Jz0 + sigma_zz Ez_strength on each minor cell
	// .Estimate Ez
	// sigma_zz should include EzShape for this minor cell

	// The mission if iPass == 0 was passed is to save off Iz0, SigmaIzz.
	// First pass set Ez_strength = 0.0.


	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + k];
			Iz[threadIdx.x] += Iz[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + s - 1];
			Iz[threadIdx.x] += Iz[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sigma_zz[blockIdx.x] = sigma_zz[0];
		p_Iz0[blockIdx.x] = Iz[0];
	}
	// Wish to make the Jz contribs to Azdot on each side of the ins exactly equal in L1, 
	// meant making this long routine even longer with collecting Iz_k.
}


__global__ void kernelPopulateResiduals(
	f64 * __restrict__ pLapAz,
	nvals * __restrict__ p_n_minor,
	v4 * __restrict__ p_vie,
	f64 * __restrict__ p_residual
) {
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 FourPiovercJz = FOURPI_Q_OVER_C*p_n_minor[iMinor].n*(p_vie[iMinor].viz - p_vie[iMinor].vez);
	p_residual[iMinor] = -pLapAz[iMinor] - FourPiovercJz;
}

__global__ void kernelAccelerate_v_from_advection
(
	f64 const h_use,
	structural * __restrict__ p_info_minor,
	nvals * __restrict__ p_n_k,    // multiply by old mass ..
	f64 * __restrict__ p_AreaMinor_k,
	nvals * __restrict__ p_n_plus, // divide by new mass ..
	f64 * __restrict__ p_AreaMinor_plus,

	v4 * __restrict__ p_vie_k,
	f64_vec3 * __restrict__ p_v_n_k,

	f64_vec3 * __restrict__ p_MAR_neut, // these contain the mom flux due to advection.
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,

	// outputs:
	v4 * __restrict__ p_vie_dest,
	f64_vec3 * __restrict__ p_v_n_dest)
{
	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; // INDEX OF VERTEX
	structural info = p_info_minor[iMinor];

	if ((info.flag == DOMAIN_VERTEX) || (info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS))
	{
		v4 vie_k = p_vie_k[iMinor];
		f64_vec3 v_n_k = p_v_n_k[iMinor];
		nvals n_k = p_n_k[iMinor];
		f64 AreaMinor_k = p_AreaMinor_k[iMinor];
		nvals n_dest = p_n_plus[iMinor];
		f64 AreaMinor_plus = p_AreaMinor_plus[iMinor];

		f64 Nk = n_k.n*AreaMinor_k;
		f64 Nnk = n_k.n_n*AreaMinor_k;
		f64 Nplus = n_dest.n*AreaMinor_plus;
		f64 Nnplus = n_dest.n_n*AreaMinor_plus;

		f64_vec3 MAR;
		memcpy(&MAR, p_MAR_neut + iMinor, sizeof(f64_vec3));
		
		f64_vec3 v_n;
		v_n = (v_n_k*Nnk + h_use * MAR) / Nnplus;
		
		// . We will really need to overview and see if this formula is correct -- did we already account for change in n?
		// . Need to check if we would be double-counting to include * oldAreaMinor / newAreaMinor --- I think we counted it.
			
		// The problem is that n's change is the change in N inferred in minor
		// so we are preferring to assume that we use the change in N that
		// would follow from looking at v on the boundaries of the minor.

		// That is what we already rounded up in MAR.

		if (TESTVNY2) printf("\n\n%d : v_n_k.y %1.10E v_n.x %1.12E h %1.8E MAR.y %1.10E N %1.10E \n",
			iMinor, v_n_k.y, v_n.y, h_use, MAR.y, Nnplus);
		if (TESTVNX) printf("\n\n%d : v_n_k.x %1.10E v_n.x %1.12E h %1.8E MAR.x %1.10E N %1.10E \n",
			iMinor, v_n_k.x, v_n.x, h_use, MAR.x, Nnplus);
		
		v4 vie;
		memcpy(&MAR, p_MAR_ion + iMinor, sizeof(f64_vec3));
		//vie.vxy = (vie_k.vxy * n_k.n + h_use * (m_i*MAR.xypart() / ((m_i + m_e)*AreaMinor))) / n_dest.n;
		//vie.viz = (vie_k.viz * n_k.n + h_use * MAR.z / (AreaMinor)) / n_dest.n;
		
		// We would PREFER the N_k / N_k+1 version, however 


		vie.vxy = (vie_k.vxy*Nk + h_use * m_i*MAR.xypart() / (m_i + m_e))/Nplus;
		vie.viz = (vie_k.viz*Nk + h_use * MAR.z)/Nplus;

		if (TEST_ACCEL_Y) printf("iMinor %d vie_k.vxy.y %1.8E Nk %1.9E Nplus %1.9E nk nplus %1.9E %1.9E \n"
			"AreaMinor k plus %1.9E %1.9E intermediate vxy %1.9E MAR_ion %1.9E h_use %1.10E \n",
			iMinor, vie_k.vxy.y, Nk, Nplus, n_k.n, n_dest.n, AreaMinor_k, AreaMinor_plus, vie.vxy.y, MAR.y, h_use);

		memcpy(&MAR, p_MAR_elec + iMinor, sizeof(f64_vec3));

		vie.vxy += h_use * (m_e*MAR.xypart() / ((m_i + m_e)*Nplus));
		vie.vez = (vie_k.vez*Nk + h_use * MAR.z) / Nplus;
		

		// 22/11/20 FIX FOR NOW: If wind is blowing outwards, do not increase velocity in CROSSING_INS just due to density decreasing.
		// Density loss is loss of momentum at same rate so leave velocity unchanged.
		// (We have prevented momentum traffic to vertex minors!)

		if ((info.flag == CROSSING_INS) && (vie.vxy.dot(info.pos) > 0.0)) vie = vie_k;
		if ((info.flag == CROSSING_INS) && (v_n.dot(info.pos) > 0.0)) v_n = v_n_k;

		// Bit messed up, but we've got to try something.



		if (TEST_ACCEL_Y) printf("MAR_e %1.9E vxy.y %1.9E \n", MAR.y, vie.vxy.y);
		if (TEST_ACCEL_EZ) printf("\n%d vie.vez %1.10E vie_k.vez %1.10E Nk %1.9E Nplus %1.9E oldNvez %1.8E \n"
			"Nratio %1.8E h*MAR.z %1.8E Areaminor k %1.9E plus %1.9E ; \n",
			iMinor, vie.vez, vie_k.vez, Nk, Nplus, Nk*vie_k.vez, Nplus / Nk, h_use*MAR.z,
			AreaMinor_k, AreaMinor_plus);


		memcpy(&(p_vie_dest[iMinor]), &vie, sizeof(v4));
		p_v_n_dest[iMinor] = v_n;
	} else {
		if (info.flag == OUTERMOST) {
			memcpy(&(p_vie_dest[iMinor]), &(p_vie_k[iMinor]), sizeof(v4));
			memcpy(&(p_v_n_dest[iMinor]), &(p_v_n_k[iMinor]), sizeof(f64_vec3));
		} else {
			memset(&(p_vie_dest[iMinor]), 0, sizeof(v4));
			memset(&(p_v_n_dest[iMinor]), 0, sizeof(f64_vec3));
		};
	}
}


__global__ void kernelPopulateBackwardOhmsLaw(
	f64 h_use,
	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	f64_vec3 * __restrict__ p_B,
	f64 * __restrict__ p_LapAz,
	f64_vec2 * __restrict__ p_GradAz,
	f64_vec2 * __restrict__ p_GradTe,
	nvals * __restrict__ p_n_minor_use,
	T3 * __restrict__ p_T_minor_use,
	v4 * __restrict__ p_vie_src,
	f64_vec3 * __restrict__ p_v_n_src,
	AAdot * __restrict__ p_AAdot_src,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ ROCAzdotduetoAdvection,
	// Now going to need to go through and see this set 0 or sensible every time.

	f64_vec3 * __restrict__ p_vn0_dest,
	v4 * __restrict__ p_v0_dest,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs_dest,
	//AAdot * __restrict__ p_AAdot_intermediate,

	f64 * __restrict__ p_Iz0,
	f64 * __restrict__ p_sigma_zz,

	f64 * __restrict__ p_denom_i,
	f64 * __restrict__ p_denom_e,
	f64 * __restrict__ p_effect_of_viz0_on_vez0,
	f64 * __restrict__ p_beta_ie_z,

	bool const bSwitchSave)
{
	// Don't forget we can use 16KB shared memory to save a bit of overspill:
	// (16*1024)/(512*8) = 4 doubles only for 512 threads. 128K total register space per SM we think.

	__shared__ f64 Iz[threadsPerTileMinor], sigma_zz[threadsPerTileMinor];
	//	__shared__ f64 Iz_k[threadsPerTileMinor];

	__shared__ f64_vec2 omega[threadsPerTileMinor], grad_Az[threadsPerTileMinor],
		gradTe[threadsPerTileMinor];

	// Putting 8 reduces to 256 simultaneous threads. Experiment with 4 in shared.
	// f64 viz0_coeff_on_Lap_Az, vez0_coeff_on_Lap_Az; // THESE APPLY TO FEINT VERSION. ASSUME NOT FEINT FIRST.

	v4 v0;
	f64 denom, ROCAzdot_antiadvect, AreaMinor;
	f64_vec3 vn0;
	long const iMinor = threadIdx.x + blockIdx.x * blockDim.x; 
	structural info = p_info_minor[iMinor];

	// Can see no reason not to put OUTERMOST here. No point creating a big gradient of vz to it.
	
	if ((info.flag == DOMAIN_VERTEX) || (info.flag == DOMAIN_TRIANGLE)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 vie_k = p_vie_src[iMinor];
		f64_vec3 v_n_src = p_v_n_src[iMinor];
		nvals n_use = p_n_minor_use[iMinor];
		AreaMinor = p_AreaMinor[iMinor];
		// Are we better off with operator = or with memcpy?
		vn0 = v_n_src;

		//		if ((TESTTRI)) printf("GPU %d vie_k %1.14E %1.14E\n", iMinor, vie_k.vxy.x, vie_k.vxy.y);
		{
			f64_vec3 MAR;
			memcpy(&MAR, p_MAR_neut + iMinor, sizeof(f64_vec3));
			// CHECK IT IS INTENDED TO AFFECT Nv

			// REVERTED THE EDIT TO USE 1/n -- THIS WILL NOT GIVE CORRECT M.A.R. EFFECT ON INTEGRAL nv
			// We need conservation laws around shock fronts.
			vn0.x += h_use * (MAR.x / (AreaMinor*n_use.n_n));
			// p_one_over_n[iMinor].n_n/ (AreaMinor));
			vn0.y += h_use * (MAR.y / (AreaMinor*n_use.n_n));// MomAddRate is addition rate for Nv. Divide by N.

			memcpy(&MAR, p_MAR_ion + iMinor, sizeof(f64_vec3));
			v0.vxy = vie_k.vxy + h_use * (m_i*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
			v0.viz = vie_k.viz + h_use * MAR.z / (n_use.n*AreaMinor);

			memcpy(&MAR, p_MAR_elec + iMinor, sizeof(f64_vec3));
			v0.vxy += h_use * (m_e*MAR.xypart() / (n_use.n*(m_i + m_e)*AreaMinor));
			v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);

			if (v0.vez != v0.vez) printf("NANVEZ %d v_k %1.9E MAR.z %1.9E \n", iMinor, vie_k.vez, MAR.z);

			if (((TESTTRI))) printf("\nGPU %d a:MAR_e %1.10E %1.10E z %1.10E MAR.y %1.10E Area %1.10E\n", iMinor,
				h_use * (m_e*MAR.x / (n_use.n*(m_i + m_e)*AreaMinor)),
				h_use * (m_e*MAR.y / (n_use.n*(m_i + m_e)*AreaMinor)),
				MAR.z,
				MAR.y,
				AreaMinor);
		}
		OhmsCoeffs ohm;
		f64 beta_ie_z, LapAz;
		f64 cross_section_times_thermal_en, cross_section_times_thermal_in,
			nu_eiBar, nu_eHeart;
		T3 T = p_T_minor_use[iMinor];
		{
			// Dimensioning inside a brace allows the following vars to go out of scope at the end of the brace.
			f64 sqrt_Te, ionneut_thermal, electron_thermal,
				lnLambda, s_in_MT, s_en_MT, s_en_visc;
			sqrt_Te = sqrt(T.Te);
			ionneut_thermal = sqrt(T.Ti / m_ion + T.Tn / m_n); // hopefully not sqrt(0)
			electron_thermal = sqrt_Te * over_sqrt_m_e;
			lnLambda = Get_lnLambda_d(n_use.n, T.Te);
			{
				f64 s_in_visc_dummy;
				Estimate_Ion_Neutral_Cross_sections_d(T.Ti*one_over_kB, &s_in_MT, &s_in_visc_dummy);
			}
			Estimate_Ion_Neutral_Cross_sections_d(T.Te*one_over_kB, &s_en_MT, &s_en_visc);

			//nu_ne_MT = s_en_MT * electron_thermal * n_use.n; // have to multiply by n_e for nu_ne_MT
			//nu_ni_MT = s_in_MT * ionneut_thermal * n_use.n;
			//nu_in_MT = s_in_MT * ionneut_thermal * n_use.n_n;
			//nu_en_MT = s_en_MT * electron_thermal * n_use.n_n;

			cross_section_times_thermal_en = s_en_MT * electron_thermal;
			cross_section_times_thermal_in = s_in_MT * ionneut_thermal;

			nu_eiBar = nu_eiBarconst * kB_to_3halves*n_use.n*lnLambda / (T.Te*sqrt_Te);
			nu_eHeart = 1.87*nu_eiBar + n_use.n_n*s_en_visc*electron_thermal;
			if (nu_eiBar != nu_eiBar) printf("&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&\n"
				"iMinor %d n_use.n %1.9E lnLambda %1.9E Te %1.9E sqrt %1.9E \n",
				iMinor, n_use.n, lnLambda, T.Te, sqrt_Te);

			// ARTIFICIAL CHANGE TO STOP IONS SMEARING AWAY OFF OF NEUTRAL BACKGROUND:
			if (n_use.n_n > ARTIFICIAL_RELATIVE_THRESH *n_use.n) {
				cross_section_times_thermal_en *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
				cross_section_times_thermal_in *= n_use.n_n / (ARTIFICIAL_RELATIVE_THRESH *n_use.n);
				// So at 1e18 vs 1e8 it's 10 times stronger
				// At 1e18 vs 1e6 it's 1000 times stronger
				// nu starts at about 1e11 at the place it failed at 35ns. So 10000 times stronger gives us 1e15.
			};


		}

		denom = 1.0 + h_use * M_e_over_en* (cross_section_times_thermal_en*n_use.n)
			+ h_use*M_i_over_in* (cross_section_times_thermal_in*n_use.n);

		vn0 /= denom; // It is now the REDUCED value

		ohm.beta_ne = h_use*(M_e_over_en)*(cross_section_times_thermal_en*n_use.n) / denom;
		ohm.beta_ni = h_use*(M_i_over_in)*(cross_section_times_thermal_in*n_use.n) / denom;

		// Now we do vexy:

		grad_Az[threadIdx.x] = p_GradAz[iMinor];
		gradTe[threadIdx.x] = p_GradTe[iMinor];
		LapAz = p_LapAz[iMinor];

		// SOON GET RID OF THIS CRAP:
		f64 ROCAzdot_antiadvect = ROCAzdotduetoAdvection[iMinor];

		if (((TESTTRI))) printf("GPU %d: LapAz %1.14E\n", CHOSEN, LapAz);

		v0.vxy +=
			(h_use / ((m_i + m_e)))*(m_n*M_i_over_in*(cross_section_times_thermal_in*n_use.n_n)
				+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*
				(vn0.xypart());

		denom = 1.0 + (h_use / (m_i + m_e))*(
			m_n* M_i_over_in* (cross_section_times_thermal_in*n_use.n_n)
			+ m_n * M_e_over_en*(cross_section_times_thermal_en*n_use.n_n))*(1.0 - ohm.beta_ne - ohm.beta_ni);
		v0.vxy /= denom;

		ohm.beta_xy_z = (h_use * q / (c*(m_i + m_e)*denom)) * grad_Az[threadIdx.x]; // coeff on viz-vez

		omega[threadIdx.x] = qovermc*p_B[iMinor].xypart();

		f64 nu_ei_effective = nu_eiBar * (1.0 - 0.9*nu_eiBar*(nu_eHeart*nu_eHeart + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT) /
			(nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].x*omega[threadIdx.x].x + omega[threadIdx.x].y*omega[threadIdx.x].y + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT)));

		//	if (nu_ei_effective != nu_ei_effective) printf("nu_ei NaN: omega %1.8E %1.8E nu_eHeart %1.8E nu_eiBar %1.8E\n",
		//		omega[threadIdx.x].x, omega[threadIdx.x].y, nu_eHeart, nu_eiBar);

		AAdot AAzdot_k = p_AAdot_src[iMinor];

		v0.viz +=
			-h_use*qoverMc*(AAzdot_k.Azdot
				+ h_use * ROCAzdot_antiadvect + h_use * c*c*LapAz)
			- h_use*qoverMc*(v0.vxy).dot(grad_Az[threadIdx.x]);

		// Still omega_ce . Check formulas.

		v0.viz +=
			1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_i*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x])));

		v0.viz += h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *vn0.z;

		denom = 1.0 + h_use * h_use*4.0*M_PI*qoverM*q*n_use.n
			+ h_use * qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)) +
			h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *(1.0 - ohm.beta_ni)
			+ h_use *moverM*nu_ei_effective;

		if (bSwitchSave) p_denom_i[iMinor] = denom;
		//				viz0_coeff_on_Lap_Az = -0.5*h_use*qoverMc*h_use*c*c / denom;

		v0.viz /= denom;

		if (((TESTTRI))) printf("viz0 divided %1.14E denom %1.14E \n", v0.viz, denom);


		ohm.sigma_i_zz = h_use * qoverM / denom;
		beta_ie_z = (h_use*h_use*4.0*M_PI*qoverM*q*n_use.n
			+ h_use*qoverMc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))
			+ h_use * M_n_over_ni*(cross_section_times_thermal_in*n_use.n_n) *ohm.beta_ne
			+ h_use * moverM*nu_ei_effective) / denom;

		v0.vez +=
			h_use *qovermc*(AAzdot_k.Azdot
				+ h_use * ROCAzdot_antiadvect
				+ h_use * c*c*(LapAz + FOURPI_Q_OVER_C*n_use.n*v0.viz))
			+ h_use*qovermc*(v0.vxy + ohm.beta_xy_z*v0.viz).dot(grad_Az[threadIdx.x]);

		// implies:
		f64 effect_of_viz0_on_vez0 =
			h_use * qovermc*h_use * c*c* FOURPI_Q_OVER_C*n_use.n
			+ h_use*qovermc*(ohm.beta_xy_z.dot(grad_Az[threadIdx.x]));

		v0.vez -=
			1.5*h_use*nu_eiBar*((omega[threadIdx.x].x*qovermc*BZ_CONSTANT - nu_eHeart * omega[threadIdx.x].y)*gradTe[threadIdx.x].x +
			(omega[threadIdx.x].y*qovermc*BZ_CONSTANT + nu_eHeart * omega[threadIdx.x].x)*gradTe[threadIdx.x].y) /
				(m_e*nu_eHeart*(nu_eHeart*nu_eHeart + omega[threadIdx.x].dot(omega[threadIdx.x]) + qovermc*BZ_CONSTANT*qovermc*BZ_CONSTANT));

		// could store this from above and put opposite -- dividing by m_e instead of m_i
		// overdue..?

		v0.vez += h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(vn0.z + ohm.beta_ni * v0.viz)
			+ h_use*nu_ei_effective*v0.viz;

		// implies:
		effect_of_viz0_on_vez0 +=
			h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni + h_use*nu_ei_effective;

		denom = 1.0 + (h_use*h_use*4.0*M_PI*q*eoverm*n_use.n
			+ h_use*qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z)))*(1.0 - beta_ie_z)
			+ h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *(1.0 - ohm.beta_ne - ohm.beta_ni * beta_ie_z)
			+ h_use*nu_ei_effective*(1.0 - beta_ie_z);

		//		vez0_coeff_on_Lap_Az = h_use * h_use*0.5*qovermc* c*c / denom; 

		ohm.sigma_e_zz =
			(-h_use * eoverm
				+ h_use * h_use*4.0*M_PI*q*eoverm*n_use.n*ohm.sigma_i_zz
				+ h_use *qovermc*(grad_Az[threadIdx.x].dot(ohm.beta_xy_z))*ohm.sigma_i_zz
				+ h_use*M_n_over_ne*(cross_section_times_thermal_en*n_use.n_n) *ohm.beta_ni*ohm.sigma_i_zz
				+ h_use*nu_ei_effective*ohm.sigma_i_zz)
			/ denom;

		v0.vez /= denom;
		effect_of_viz0_on_vez0 /= denom; // of course 

		if (bSwitchSave) {
			p_denom_e[iMinor] = denom;
			p_effect_of_viz0_on_vez0[iMinor] = effect_of_viz0_on_vez0;
			p_beta_ie_z[iMinor] = beta_ie_z; // see that doing it this way was not best.
		}
		else {
			// #########################################################################################################
			// DEBUG: pass graphing parameters through these.
			// #########################################################################################################
			p_denom_i[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective;
			p_denom_e[iMinor] = M_n_over_ne*cross_section_times_thermal_en*n_use.n_n /
				(M_n_over_ne*cross_section_times_thermal_en*n_use.n_n + nu_ei_effective);
		};

		// Now update viz(Ez):
		v0.viz += beta_ie_z * v0.vez;
		ohm.sigma_i_zz += beta_ie_z * ohm.sigma_e_zz;

		// sigma_e_zz and sigma_i_zz are change in vz for a change in Ez
		{
			f64 EzShape = GetEzShape(info.pos.modulus());
			ohm.sigma_i_zz *= EzShape;
			ohm.sigma_e_zz *= EzShape;
		}

		// Think maybe we should get rid of most of this routine out of the subcycle.
		// Rate of acceleration over timestep due to resistance, pressure, thermal force etc could be stored.
		// Saving off some eqn data isn't so bad when we probably overflow registers and L1 here anyway.
		// All we need is to know that we update sigma
		// We can do addition of 
		// ==============================================================================================

		p_v0_dest[iMinor] = v0;
		p_OhmsCoeffs_dest[iMinor] = ohm;
		p_vn0_dest[iMinor] = vn0;

		Iz[threadIdx.x] = q*AreaMinor*n_use.n*(v0.viz - v0.vez);
		sigma_zz[threadIdx.x] = q*AreaMinor*n_use.n*(ohm.sigma_i_zz - ohm.sigma_e_zz);

	}
	else {
		// Non-domain triangle or vertex
		// ==============================
		// Need to decide whether crossing_ins triangle will experience same accel routine as the rest?
		// I think yes so go and add it above??
		// We said v_r = 0 necessarily to avoid sending mass into ins.
		// So how is that achieved there? What about energy loss?
		// Need to determine a good way. Given what v_r in tri represents. We construe it to be AT the ins edge so 
		// ...
		Iz[threadIdx.x] = 0.0;
		sigma_zz[threadIdx.x] = 0.0;

		//	if ((iMinor < BEGINNING_OF_CENTRAL) && ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)))
		//		{
		//		p_AAdot_intermediate[iMinor].Azdot = 0.0;
		// Set Az equal to neighbour in every case, after Accelerate routine.
		//	}
		//		else {
		// Let's make it go right through the middle of a triangle row for simplicity.

		//f64 Jz = 0.0;
		//if ((iMinor >= numStartZCurrentTriangles) && (iMinor <  numEndZCurrentTriangles))
		//{
		//	// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
		//	// ASSUME we are fed Iz_prescribed.
		//	//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

		//	AreaMinor = p_AreaMinor[iMinor];
		//	Jz = negative_Iz_per_triangle / AreaMinor; // Iz would come from multiplying back by area and adding.
		//};

		//		AAdot temp = p_AAdot_src[iMinor];
		//		temp.Azdot += h_use * c*(c*p_LapAz[iMinor]);// +4.0*M_PI*Jz);
		// + h_use * ROCAzdot_antiadvect // == 0
		//		p_AAdot_intermediate[iMinor] = temp; // 

		//	};
	};

	__syncthreads();

	// .Collect Jz = Jz0 + sigma_zz Ez_strength on each minor cell
	// .Estimate Ez
	// sigma_zz should include EzShape for this minor cell

	// The mission if iPass == 0 was passed is to save off Iz0, SigmaIzz.
	// First pass set Ez_strength = 0.0.


	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + k];
			Iz[threadIdx.x] += Iz[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sigma_zz[threadIdx.x] += sigma_zz[threadIdx.x + s - 1];
			Iz[threadIdx.x] += Iz[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sigma_zz[blockIdx.x] = sigma_zz[0];
		p_Iz0[blockIdx.x] = Iz[0];
	}
	// Wish to make the Jz contribs to Azdot on each side of the ins exactly equal in L1, 
	// meant making this long routine even longer with collecting Iz_k.
}

/*__global__ void Estimate_Effect_on_Integral_Azdot_from_Jz_and_LapAz(
	f64 hstep,
	structural * __restrict__ p_info,
	nvals * __restrict__ p_nvals_k,
	nvals * __restrict__ p_nvals_use,
	v4 * __restrict__ p_vie_k,
	v4 * __restrict__ p_vie_kplus1,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_LapAz,

	AAdot * __restrict__ p_Azdot,

	f64 * __restrict__ p_tile1, // +ve Jz
	f64 * __restrict__ p_tile2, // -ve Jz
	f64 * __restrict__ p_tile3, // LapAz
	f64 * __restrict__ p_tile4, // integrate Azdot diff
	f64 * __restrict__ p_tile5,
	f64 * __restrict__ p_tile6
)
{
	__shared__ f64 sum1[threadsPerTileMinor];
	__shared__ f64 sum2[threadsPerTileMinor];
	__shared__ f64 sum3[threadsPerTileMinor];
	__shared__ f64 sum4[threadsPerTileMinor];
	__shared__ f64 sum5[threadsPerTileMinor];
	__shared__ f64 sum6[threadsPerTileMinor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	structural info = p_info[iMinor];
	nvals n_k = p_nvals_k[iMinor];
	nvals n_use = p_nvals_use[iMinor];
	v4 v_k = p_vie_k[iMinor];
	v4 v_kplus1 = p_vie_kplus1[iMinor];
	f64 AreaMinor = p_AreaMinor[iMinor];
	f64 LapAz = p_LapAz[iMinor];

	sum1[threadIdx.x] = 0.0; 
	sum2[threadIdx.x] = 0.0;
	sum3[threadIdx.x] = 0.0;
	sum4[threadIdx.x] = 0.0;
	sum5[threadIdx.x] = 0.0;
	sum6[threadIdx.x] = 0.0;
	
	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
	{
		sum1[threadIdx.x] = 
			  hstep*c*c*0.5*FOURPI_OVER_C * q*n_k.n*(v_k.viz - v_k.vez)*AreaMinor
			+ hstep*c*0.5*FOUR_PI*q*n_use.n*(v_kplus1.viz - v_kplus1.vez)*AreaMinor;
		// Was n used consistently?
	} else {
		
										 //if ((iMinor >= numStartZCurrentTriangles) && (iMinor < numEndZCurrentTriangles))
		if (info.flag == REVERSE_JZ_TRI)
			sum2[threadIdx.x] = hstep*c*4.0*M_PI*negative_Iz_per_triangle;
	}

	// make sure we copy from the code:
	sum3[threadIdx.x] = hstep*c*c*LapAz*AreaMinor;
	sum4[threadIdx.x] = fabs(hstep*c*c*LapAz*AreaMinor);

	sum5[threadIdx.x] = p_Azdot[iMinor].Azdot * AreaMinor;
	sum6[threadIdx.x] = fabs(p_Azdot[iMinor].Azdot * AreaMinor);

	// -----------------------------------------------------------------------------

	__syncthreads();

	// .Collect Jz = Jz0 + sigma_zz Ez_strength on each minor cell
	// .Estimate Ez
	// sigma_zz should include EzShape for this minor cell

	// The mission if iPass == 0 was passed is to save off Iz0, SigmaIzz.
	// First pass set Ez_strength = 0.0.
	
	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sum1[threadIdx.x] += sum1[threadIdx.x + k];
			sum2[threadIdx.x] += sum2[threadIdx.x + k];
			sum3[threadIdx.x] += sum3[threadIdx.x + k];
			sum4[threadIdx.x] += sum4[threadIdx.x + k];
			sum5[threadIdx.x] += sum5[threadIdx.x + k];
			sum6[threadIdx.x] += sum6[threadIdx.x + k];
			
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sum1[threadIdx.x] += sum1[threadIdx.x + s - 1];
			sum2[threadIdx.x] += sum2[threadIdx.x + s - 1];
			sum3[threadIdx.x] += sum3[threadIdx.x + s - 1];
			sum4[threadIdx.x] += sum4[threadIdx.x + s - 1];
			sum5[threadIdx.x] += sum5[threadIdx.x + s - 1];
			sum6[threadIdx.x] += sum6[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_tile1[blockIdx.x] = sum1[0];
		p_tile2[blockIdx.x] = sum2[0];
		p_tile3[blockIdx.x] = sum3[0];
		p_tile4[blockIdx.x] = sum4[0];
		p_tile5[blockIdx.x] = sum5[0];
		p_tile6[blockIdx.x] = sum6[0];
	}
}*/


__global__ void kernelCalculateVelocityAndAzdot(
	f64 h_use,
	structural * p_info_minor,
	f64_vec3 * __restrict__ p_vn0,
	v4 * __restrict__ p_v0,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs,
	AAdot * __restrict__ p_AAzdot_src,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_LapAz, // would it be better just to be loading the Azdot0 relation?
	f64 * __restrict__ p_ROCAzdotantiadvect,

	AAdot * __restrict__ p_AAzdot_out,
	v4 * __restrict__ p_vie_out,
	f64_vec3 * __restrict__ p_vn_out)
{
	long iMinor = blockIdx.x*blockDim.x + threadIdx.x;

	structural info = p_info_minor[iMinor];
	AAdot temp = p_AAzdot_src[iMinor];
	temp.Azdot += h_use*(c*c*p_LapAz[iMinor] + p_ROCAzdotantiadvect[iMinor]);
	// We did not add LapAz into Azdot already in PopBackwardOhms.

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 v;
		nvals n_use = p_n_minor[iMinor];
		OhmsCoeffs ohm = p_OhmsCoeffs[iMinor];
		v4 v0 = p_v0[iMinor];
		f64_vec3 v_n = p_vn0[iMinor];							 // 3 sep

		v.vez = v0.vez + ohm.sigma_e_zz * Ez_strength;  // 2
		v.viz = v0.viz + ohm.sigma_i_zz * Ez_strength;  // 2

		v.vxy = v0.vxy + ohm.beta_xy_z * (v.viz - v.vez);   // 4
		v_n.x += (ohm.beta_ne + ohm.beta_ni)*v.vxy.x;    // 2
		v_n.y += (ohm.beta_ne + ohm.beta_ni)*v.vxy.y;
		v_n.z += ohm.beta_ne * v.vez + ohm.beta_ni * v.viz;

		if (info.flag == CROSSING_INS) {
			f64_vec2 rhat = info.pos / info.pos.modulus();
			v_n -= Make3((v_n.dotxy(rhat))*rhat, 0.0);
			v.vxy -= v.vxy.dot(rhat)*rhat;
		}

		memcpy(&(p_vie_out[iMinor]), &v, sizeof(v4)); // operator = vs memcpy
		p_vn_out[iMinor] = v_n;

		if (info.flag == OUTERMOST) {
			temp.Azdot = 0.0;
			temp.Az = 0.0;
		}
		else {
			// BACKWARD:
			temp.Azdot += h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez); // logical for C_INS too
		}

		if ((TESTTRI2)) printf(
			"CVAA iMinor %d v0.vez %1.9E sigma_e_zz %1.9E Ez %1.9E v.vez %1.9E\n",
			iMinor, v0.vez, ohm.sigma_e_zz, Ez_strength, v.vez);


	}
	else {

		memset(&(p_vie_out[iMinor]), 0, sizeof(v4));
		memset(&(p_vn_out[iMinor]), 0, sizeof(f64_vec3));

		f64 Jz = 0.0;

		if (info.flag == REVERSE_JZ_TRI)
		{

			// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			// ASSUME we are fed Iz_prescribed.
			//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;

			//		printf("temp.Azdot %1.10E ", temp.Azdot);
			temp.Azdot += h_use*c*FOUR_PI*Jz; // Iz would come from multiplying back by area and adding.

											  //		printf("%d Iz %1.14E Area %1.14E Jz %1.14E Azdot %1.14E \n",
											  //		iMinor,
											  //	negative_Iz_per_triangle, AreaMinor, Jz, temp.Azdot);
		};
	}
	// + h_use * ROCAzdot_antiadvect // == 0
	p_AAzdot_out[iMinor] = temp;

	// Would rather make this a separate routine beforehand.


	//data_1.Azdot = data_k.Azdot
	//	+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//		0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz + data_1.viz
	//			- data_k.vez - data_1.vez));

	//data_1.Azdot = data_k.Azdot
	//		+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//			0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz - data_k.vez));
	// intermediate
}

__global__ void kernelCalculateVelocityAndAzdot_noadvect(
	f64 h_use,
	structural * p_info_minor,
	f64_vec3 * __restrict__ p_vn0,
	v4 * __restrict__ p_v0,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs,
	AAdot * __restrict__ p_AAzdot_src,
	nvals * __restrict__ p_n_minor, 
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_LapAz, // would it be better just to be loading the Azdot0 relation?
	
	AAdot * __restrict__ p_AAzdot_out,
	v4 * __restrict__ p_vie_out,
	f64_vec3 * __restrict__ p_vn_out ) 
{
	long iMinor = blockIdx.x*blockDim.x + threadIdx.x;

	structural info = p_info_minor[iMinor];
	AAdot temp = p_AAzdot_src[iMinor];
	temp.Azdot += h_use*(c*c*p_LapAz[iMinor]);
	
	// We did not add LapAz into Azdot already in PopBackwardOhms.

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 v;
		nvals n_use = p_n_minor[iMinor];
		OhmsCoeffs ohm = p_OhmsCoeffs[iMinor];
		v4 v0 = p_v0[iMinor];
		f64_vec3 v_n = p_vn0[iMinor];							 // 3 sep
		
		// debug:
		long iVertex = iMinor - BEGINNING_OF_CENTRAL;
		if (TESTACCEL) printf("iVertex %d v0.xy %1.9E %1.9E\n",
			iVertex, v0.vxy.x, v0.vxy.y);

		v.vez = v0.vez + ohm.sigma_e_zz * Ez_strength;  // 2
		v.viz = v0.viz + ohm.sigma_i_zz * Ez_strength;  // 2

		v.vxy = v0.vxy + ohm.beta_xy_z * (v.viz - v.vez);   // 4

		if (TESTACCEL) printf("iVertex %d ohm.beta_xz yz %1.9E %1.9E viz %1.9E vez %1.9E effect xy %1.9E %1.9E\n",
			iVertex,
			ohm.beta_xy_z.x, ohm.beta_xy_z.y,
			v.viz,
			v.vez,
			ohm.beta_xy_z.x * (v.viz - v.vez),
			ohm.beta_xy_z.y * (v.viz - v.vez));

		//if (TESTACCEL) printf("iVertex %d ohm.beta_yz %1.9E viz %1.9E vez %1.9E effect %1.9E\n",
		//	iVertex,
		//	ohm.beta_xy_z.y,
		//	v.viz,
		//	v.vez,
		//	ohm.beta_xy_z.y * (v.viz - v.vez));

		v_n.x += (ohm.beta_ne + ohm.beta_ni)*v.vxy.x;    // 2
		v_n.y += (ohm.beta_ne + ohm.beta_ni)*v.vxy.y;
		v_n.z += ohm.beta_ne * v.vez + ohm.beta_ni * v.viz;
		
		if (TESTVNY) printf("%d v_n.y %1.9E since ohm %1.9E v.vxy.y %1.9E \n", iMinor, v_n.y,
			(ohm.beta_ne + ohm.beta_ni), v.vxy.y);

		if (TESTVNX) printf("%d v_n.y %1.9E since ohm %1.9E v.vxy.y %1.9E \n", iMinor, v_n.x,			
			(ohm.beta_ne + ohm.beta_ni), v.vxy.x);

//		if (info.flag == CROSSING_INS) {
//			f64_vec2 rhat = info.pos / info.pos.modulus();
//			v_n -= Make3((v_n.dotxy(rhat))*rhat, 0.0);
//			v.vxy -= v.vxy.dot(rhat)*rhat;
//
//			if (TESTACCEL) printf("v.vxy after negate r component : %1.9E %1.9E\n", v.vxy.x, v.vxy.y);
//		}

		memcpy(&(p_vie_out[iMinor]), &v, sizeof(v4)); // operator = vs memcpy
		p_vn_out[iMinor] = v_n;

	//	if (info.flag == OUTERMOST) {
	//		temp.Azdot = 0.0;
	//		temp.Az = 0.0; // really!
	//	} else {
			// BACKWARD:
		temp.Azdot += h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez); // logical for Crossing_INS too
	//	}
		
		if (TESTACCEL) printf("CVAA:iVertex %d v_out.xy %1.9E %1.9E\n", iVertex, v.vxy.x, v.vxy.y);
		if (TESTVEZ) printf("%d CVAA vez %1.11E v0 %1.11E Ez_strength %1.14E sigma %1.14E \n"
			"Azdot %1.9E components: k %1.9E h_use*(c*c*p_LapAz) %1.9E hc4piJ %1.9E\n"
			"n viz vez %1.14E %1.14E %1.14E\n"
			, iMinor, v.vez, v0.vez,
			Ez_strength, ohm.sigma_e_zz, temp.Azdot, p_AAzdot_src[iMinor].Azdot, h_use*(c*c*p_LapAz[iMinor]),
			h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez),
			n_use.n, v.viz, v.vez
			);
		
	} else {

		memset(&(p_vie_out[iMinor]), 0, sizeof(v4)); 
		memset(&(p_vn_out[iMinor]), 0, sizeof(f64_vec3));

		f64 Jz = 0.0;

		if (info.flag == REVERSE_JZ_TRI)
		{
			
			// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			// ASSUME we are fed Iz_prescribed.
			//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;

	//		printf("temp.Azdot %1.10E ", temp.Azdot);
			temp.Azdot += h_use*c*FOUR_PI*Jz; // Iz would come from multiplying back by area and adding.
			
	//		printf("%d Iz %1.14E Area %1.14E Jz %1.14E Azdot %1.14E \n",
		//		iMinor,
			//	negative_Iz_per_triangle, AreaMinor, Jz, temp.Azdot);
		};
	}
	// + h_use * ROCAzdot_antiadvect // == 0
	p_AAzdot_out[iMinor] = temp;

		 // Would rather make this a separate routine beforehand.
	

	//data_1.Azdot = data_k.Azdot
	//	+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//		0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz + data_1.viz
	//			- data_k.vez - data_1.vez));

	//data_1.Azdot = data_k.Azdot
	//		+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//			0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz - data_k.vez));
	// intermediate
}

/*
__global__ void kernelCalculateVelocityAndAzdot_noadvect__debugintegrate(
	f64 h_use,
	structural * p_info_minor,
	f64_vec3 * __restrict__ p_vn0,
	v4 * __restrict__ p_v0,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs,
	AAdot * __restrict__ p_AAzdot_src,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_LapAz, // would it be better just to be loading the Azdot0 relation?
	AAdot * __restrict__ p_AAzdot_out,
	v4 * __restrict__ p_vie_out,
	f64_vec3 * __restrict__ p_vn_out,

	f64 * __restrict__ p_integ_Jz1,
	f64 * __restrict__ p_integ_Jz2,
	f64 * __restrict__ p_integ_LapAz
	)
{
	long iMinor = blockIdx.x*blockDim.x + threadIdx.x;

	__shared__ f64 sum_Jzdomain[threadsPerTileMinor];
	__shared__ f64 sum_Jzreverse[threadsPerTileMinor];
	__shared__ f64 sum_LapAz[threadsPerTileMinor];
	
	structural info = p_info_minor[iMinor];
	AAdot temp = p_AAzdot_src[iMinor];
	
	sum_Jzdomain[threadIdx.x] = 0.0;
	sum_Jzreverse[threadIdx.x] = 0.0;

	temp.Azdot += h_use*(c*c*p_LapAz[iMinor]);
	sum_LapAz[threadIdx.x] = c*c*p_LapAz[iMinor] * p_AreaMinor[iMinor];

	// We did not add LapAz into Azdot already in PopBackwardOhms.

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 v;
		nvals n_use = p_n_minor[iMinor];
		OhmsCoeffs ohm = p_OhmsCoeffs[iMinor];
		v4 v0 = p_v0[iMinor];
		f64_vec3 v_n = p_vn0[iMinor];							 // 3 sep
			
		long iVertex = iMinor - BEGINNING_OF_CENTRAL;
		if (TESTACCEL) printf("iVertex %d v0.y %1.9E\n", iVertex, v0.vxy.y);

		v.vez = v0.vez + ohm.sigma_e_zz * Ez_strength;  // 2
		v.viz = v0.viz + ohm.sigma_i_zz * Ez_strength;  // 2
		v.vxy = v0.vxy + ohm.beta_xy_z * (v.viz - v.vez);   // 4

		if (TESTACCEL) printf("iVertex %d ohm.beta_yz %1.9E viz %1.9E vez %1.9E effect %1.9E\n",
			iVertex,
			ohm.beta_xy_z.y,
			v.viz,
			v.vez,
			ohm.beta_xy_z * (v.viz - v.vez));

		v_n.x += (ohm.beta_ne + ohm.beta_ni)*v.vxy.x;    // 2
		v_n.y += (ohm.beta_ne + ohm.beta_ni)*v.vxy.y;
		v_n.z += ohm.beta_ne * v.vez + ohm.beta_ni * v.viz;

		if (TESTVNY) printf("%d v_n.y %1.9E since ohm %1.9E v.vxy.y %1.9E \n", iMinor, v_n.y,
			(ohm.beta_ne + ohm.beta_ni), v.vxy.y);
//
//		if (info.flag == CROSSING_INS) {
//			f64_vec2 rhat = info.pos / info.pos.modulus();
//			v_n -= Make3((v_n.dotxy(rhat))*rhat, 0.0);
//			v.vxy -= v.vxy.dot(rhat)*rhat;
//		}

		memcpy(&(p_vie_out[iMinor]), &v, sizeof(v4)); // operator = vs memcpy
		p_vn_out[iMinor] = v_n;

		temp.Azdot += h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez); // logical for Crossing_INS too

		if (TESTACCEL) printf("CVAA:iVertex %d v_out.y %1.9E\n", iVertex, v.vxy.y);
		if (TESTVEZ) printf("%d CVAA vez %1.9E v0 %1.9E Ez %1.9E sigma %1.9E\n", iMinor, v.vez, v0.vez,
			Ez_strength, ohm.sigma_e_zz);

		sum_Jzdomain[threadIdx.x] = c*FOUR_PI*q*n_use.n*(v.viz - v.vez)*p_AreaMinor[iMinor];

	} else {

		memset(&(p_vie_out[iMinor]), 0, sizeof(v4));
		memset(&(p_vn_out[iMinor]), 0, sizeof(f64_vec3));

		f64 Jz = 0.0;
		if (info.flag == REVERSE_JZ_TRI)
		{
			// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			// ASSUME we are fed Iz_prescribed.
			//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;

			temp.Azdot += h_use*c*FOUR_PI*Jz; // Iz would come from multiplying back by area and adding.
			sum_Jzreverse[threadIdx.x] = negative_Iz_per_triangle*c*FOUR_PI;

		};
	}
	p_AAzdot_out[iMinor] = temp;


	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sum_Jzdomain[threadIdx.x] += sum_Jzdomain[threadIdx.x + k];
			sum_LapAz[threadIdx.x] += sum_LapAz[threadIdx.x + k];
			sum_Jzreverse[threadIdx.x] += sum_Jzreverse[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sum_Jzdomain[threadIdx.x] += sum_Jzdomain[threadIdx.x + s - 1];
			sum_LapAz[threadIdx.x] += sum_LapAz[threadIdx.x + s - 1];
			sum_Jzreverse[threadIdx.x] += sum_Jzreverse[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_integ_Jz1[blockIdx.x] = sum_Jzdomain[0];
		p_integ_Jz2[blockIdx.x] = sum_Jzreverse[0];
		p_integ_LapAz[blockIdx.x] = sum_LapAz[0];
	}
}*/

/*
__global__ void kernelCalculateVelocityAndAzdot_noadvect_SPIT(
	f64 h_use,
	structural * p_info_minor,
	LONG3 * p_tricornerindex,
	f64_vec3 * __restrict__ p_vn0,
	v4 * __restrict__ p_v0,
	OhmsCoeffs * __restrict__ p_OhmsCoeffs,
	AAdot * __restrict__ p_AAzdot_src,
	nvals * __restrict__ p_n_minor,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_LapAz, // would it be better just to be loading the Azdot0 relation?

	AAdot * __restrict__ p_AAzdot_out,
	v4 * __restrict__ p_vie_out,
	f64_vec3 * __restrict__ p_vn_out)
{
	long iMinor = blockIdx.x*blockDim.x + threadIdx.x;

	bool bReport = false;

	if (iMinor < BEGINNING_OF_CENTRAL) {
		LONG3 tci = p_tricornerindex[iMinor];
		if ((tci.i1 == VERTCHOSEN) || (tci.i2 == VERTCHOSEN) || (tci.i3 == VERTCHOSEN))
			bReport = true;
	}
	else {
		if (iMinor - BEGINNING_OF_CENTRAL == VERTCHOSEN) bReport = true;
	}
	structural info = p_info_minor[iMinor];
	AAdot temp = p_AAzdot_src[iMinor];
	
	f64 store_Azdot = temp.Azdot;

	temp.Azdot += h_use*(c*c*p_LapAz[iMinor]);
	// We did not add LapAz into Azdot already in PopBackwardOhms.

	f64 store_hccLap = h_use*c*c*p_LapAz[iMinor];

	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == DOMAIN_VERTEX)
		|| (info.flag == CROSSING_INS) || (info.flag == OUTERMOST))
	{
		v4 v;
		nvals n_use = p_n_minor[iMinor];
		OhmsCoeffs ohm = p_OhmsCoeffs[iMinor];
		v4 v0 = p_v0[iMinor];
		f64_vec3 v_n = p_vn0[iMinor];							 // 3 sep


		long iVertex = iMinor - BEGINNING_OF_CENTRAL;
		
		v.vez = v0.vez + ohm.sigma_e_zz * Ez_strength;  // 2
		v.viz = v0.viz + ohm.sigma_i_zz * Ez_strength;  // 2

		v.vxy = v0.vxy + ohm.beta_xy_z * (v.viz - v.vez);   // 4

		v_n.x += (ohm.beta_ne + ohm.beta_ni)*v.vxy.x;    // 2
		v_n.y += (ohm.beta_ne + ohm.beta_ni)*v.vxy.y;
		v_n.z += ohm.beta_ne * v.vez + ohm.beta_ni * v.viz;

		if (info.flag == CROSSING_INS) {
			f64_vec2 rhat = info.pos / info.pos.modulus();
			v_n -= Make3((v_n.dotxy(rhat))*rhat, 0.0);
			v.vxy -= v.vxy.dot(rhat)*rhat;
		}

		memcpy(&(p_vie_out[iMinor]), &v, sizeof(v4)); // operator = vs memcpy
		p_vn_out[iMinor] = v_n;

		if (info.flag == OUTERMOST) {
			temp.Azdot = 0.0;
			temp.Az = 0.0; // really!
		}
		else {
			// BACKWARD:
			temp.Azdot += h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez); // logical for Crossing_INS too
		}

		if (bReport) printf("%d Azdot_old %1.10E new %1.10E Az %1.10E hccLapAz %1.10E hc4piJz %1.10E n %1.8E vez %1.8E\n", iMinor, store_Azdot, temp.Azdot, temp.Az,
			store_hccLap, h_use*c*FOUR_PI*q*n_use.n*(v.viz - v.vez), n_use.n, v.vez);
		
	} else {

		memset(&(p_vie_out[iMinor]), 0, sizeof(v4));
		memset(&(p_vn_out[iMinor]), 0, sizeof(f64_vec3));

		f64 Jz = 0.0;

		if (info.flag == REVERSE_JZ_TRI)
		{

			// Azdotdot = c^2 (Lap Az + 4pi/c Jz)
			// ASSUME we are fed Iz_prescribed.
			//Jz = -Iz_prescribed / (real)(numEndZCurrentTriangles - numStartZCurrentTriangles);

			f64 AreaMinor = p_AreaMinor[iMinor];
			Jz = negative_Iz_per_triangle / AreaMinor;

			//		printf("temp.Azdot %1.10E ", temp.Azdot);
			temp.Azdot += h_use*c*FOUR_PI*Jz; // Iz would come from multiplying back by area and adding.

											  //		printf("%d Iz %1.14E Area %1.14E Jz %1.14E Azdot %1.14E \n",
											  //		iMinor,
											  //	negative_Iz_per_triangle, AreaMinor, Jz, temp.Azdot);
		};
	}
	// + h_use * ROCAzdot_antiadvect // == 0
	p_AAzdot_out[iMinor] = temp;

	// Would rather make this a separate routine beforehand.


	//data_1.Azdot = data_k.Azdot
	//	+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//		0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz + data_1.viz
	//			- data_k.vez - data_1.vez));

	//data_1.Azdot = data_k.Azdot
	//		+ h_use * ROCAzdot_antiadvect + h_use * c*c*(Lap_Az +
	//			0.5*FOURPI_OVER_C * q*data_use.n*(data_k.viz - data_k.vez));
	// intermediate
}*/


__global__ void kernelCreateEpsilonAndJacobi_Heat
(
	f64 const h_sub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_T_n,
	f64 * __restrict__ p_T_i,
	f64 * __restrict__ p_T_e,
	T3 * p_Tk, // T_k for substep

	NTrates * __restrict__ p_NTrates_diffusive,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,

	f64 * __restrict__ p__coeffself_n, // what about dividing by N?
	f64 * __restrict__ p__coeffself_i,
	f64 * __restrict__ p__coeffself_e,
	f64 * __restrict__ p__epsilon_n,
	f64 * __restrict__ p__epsilon_i,
	f64 * __restrict__ p__epsilon_e,
	f64 * __restrict__ p__Jacobi_n,
	f64 * __restrict__ p__Jacobi_i,
	f64 * __restrict__ p__Jacobi_e	,
	bool * __restrict__ p_bFailedTest,
	bool * __restrict__ p_bMask3,
	bool * __restrict__ p_bMaskblock,
	bool bUseMask
)
{
	// 2. Calculate epsilon: given the est of T, eps = T - (T_k +- h sum kappa dot grad T)

	// So this is a lot like saying, let's call the actual routine...
	// except we also want Jacobi which means we also want coeff on self in epsilon.

	// eps= T_putative - (T_k +- h sum kappa dot grad T_putative)

	// coeff on self we want to be linearized so it incorporates the assumption that it affects kappa.
	// deps/dT = sum [[dkappa/dT = 0.5 kappa/T] dot grad T + kappa dot d/dT grad T]

	// However this means if we know kappa dot grad T then we can * by 0.5/T to get dkappa/dT part
	// But we had to collect a separate value for kappa dot d/dT grad T.

	// We certainly need to somehow modify the existing kappa dot grad T routine here.
	// what about dividing by N?
	
	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;

	if (bUseMask) {

	//	if (iVertex == VERTCHOSEN) {
	//		printf("%d : bUseMask %d p_bMaskblock[blockIdx.x] %d blockIdx.x %d\n",
	//			iVertex, (bUseMask) ? 1 : 0, (p_bMaskblock[blockIdx.x]) ? 1 : 0, blockIdx.x);
	//	}

		if (p_bMaskblock[blockIdx.x] == 0) return;
	}

	bool bMask[3];

	if (bUseMask) {
		//memcpy(bMask, p_bMask3 + 3 * iVertex, sizeof(bool) * 3);
		bMask[0] = p_bMask3[iVertex];
		bMask[1] = p_bMask3[iVertex + NUMVERTICES];
		bMask[2] = p_bMask3[iVertex + NUMVERTICES*2];
	//	if (iVertex == VERTCHOSEN) {
	//		printf("%d : bUseMask %d p_bMask[2] %d \n",
	//			iVertex, (bUseMask) ? 1 : 0, (bMask[2]) ? 1 : 0);
	//	}
		// We need to re-do into species anyway. Afterwards.
		if ((bMask[0] == 0) &&
			(bMask[1] == 0) &&
			(bMask[2] == 0)) return; // do not modify anything
	}

	structural info = p_info_major[iVertex];
	
	if ((info.flag == DOMAIN_VERTEX)) { //|| (info.flag == OUTERMOST)) {

		NTrates Rates = p_NTrates_diffusive[iVertex];
		nvals n = p_n_major[iVertex];
		f64 Area = p_AreaMajor[iVertex];
		f64 N = n.n*Area;
		f64 Nn = n.n_n*Area;
		f64 Tn, Ti, Te, actual_Tn, actual_Ti, actual_Te, epsilon_n, epsilon_i, epsilon_e;
		T3 T_k;
		memcpy(&T_k, &(p_Tk[iVertex]), sizeof(T3));

		if ((bUseMask == 0) || (bMask[0])) {
			Tn = p_T_n[iVertex];
			actual_Tn = T_k.Tn + (h_sub / Nn)*Rates.NnTn;
			epsilon_n = Tn - actual_Tn;

			// Try this:
			p__Jacobi_n[iVertex] = -(h_sub / sqrt(Nn))*Rates.NnTn / p__coeffself_n[iVertex]; // should never be 0

			epsilon_n *= sqrt(Nn);
			p__epsilon_n[iVertex] = epsilon_n;
		} else {
			epsilon_n = 0.0;
			actual_Tn = 0.0; // not used
		}

		if ((bUseMask == 0) || (bMask[1])) {
			Ti = p_T_i[iVertex];
			actual_Ti = T_k.Ti + (h_sub / N)*Rates.NiTi;
			epsilon_i = Ti - actual_Ti;

			// Try this:
			p__Jacobi_i[iVertex] = -(h_sub / sqrt(N))*Rates.NiTi / p__coeffself_i[iVertex];

			// Weighted Least Squares:
			epsilon_i *= sqrt(N);
			p__epsilon_i[iVertex] = epsilon_i;
		} else {
			epsilon_i = 0.0;
			actual_Ti = 0.0; // not used
		};


		if ((bUseMask == 0) || (bMask[2])) {
			Te = p_T_e[iVertex];
			actual_Te = T_k.Te + (h_sub / N)*Rates.NeTe;
			epsilon_e = Te - actual_Te;

			// Try this:
			p__Jacobi_e[iVertex] = -(h_sub / sqrt(N))*Rates.NeTe / p__coeffself_e[iVertex];
			epsilon_e *= sqrt(N);
			p__epsilon_e[iVertex] = epsilon_e;

			//if ((iVertex == VERTCHOSEN)) 
			//	printf("iVertex %d Te %1.10E actual_Te %1.9E Tk %1.9E Rates %1.10E epsilon %1.11E Jacobi %1.10E\n",
			//		iVertex, Te, actual_Te, T_k.Te, Rates.NeTe, epsilon_e, p__Jacobi_e[iVertex]);
			
		} else {
			epsilon_e = 0.0;
			actual_Te = 0.0; // not used
		}
		// If sqrt N we care about is 1e4 and T we care about is 1e-14 then we get 1e-10 as the sqrt(N)T to add to create absolute threshold

		if (p_bFailedTest != 0) {
			if ((epsilon_n*epsilon_n > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Tn*actual_Tn*Nn + 1.0e-10*1.0e-10))
				||
				(epsilon_i*epsilon_i > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Ti*actual_Ti*N + 1.0e-10*1.0e-10))
				||
				(epsilon_e*epsilon_e > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Te*actual_Te*N + 1.0e-10*1.0e-10))
				)
				p_bFailedTest[blockIdx.x] = true;
			// Why 1.0e-10 in absolute error, for minimum value we care about:
			// N = 2.0e12*7e-5 = 1e8 
			// root N = 1e4
			// root N * 1e-14 erg = 1e-10 for (root N) T
		}
		//if (p_bFailedTest != 0) {
		//	if ((epsilon_n*epsilon_n > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Tn*actual_Tn*Nn + 1.0e-30)) ||
		//		(epsilon_i*epsilon_i > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Ti*actual_Ti*N + 1.0e-30)) ||
		//		(epsilon_e*epsilon_e > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_Te*actual_Te*N + 1.0e-30)) ||
		//		(actual_Tn < 0.0) || (actual_Ti < 0.0) || (actual_Te < 0.0))
		//		p_bFailedTest[blockIdx.x] = true;
		//}

		// It may be T<0 that is the probs, given that we have arbitrary strength of B-pull on some edge.
		
		// 1e-28 = 1e-14 1e-14 so that's small. Up to 1e-22 = 1e-9 1e-14.
		// 1e-8 T (so 1e-16 TT) is comparatively quite large -- just past single precision.
		// That seems about right for now.
			
	} else {
		p__epsilon_n[iVertex] = 0.0;
		p__epsilon_i[iVertex] = 0.0;
		p__epsilon_e[iVertex] = 0.0;
		p__Jacobi_n[iVertex] = 0.0;
		p__Jacobi_i[iVertex] = 0.0;
		p__Jacobi_e[iVertex] = 0.0;
	};
}

__global__ void kernelMultiplyVector(
	f64 * __restrict__ p_multiply,
	f64 const factor)
{
	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;

	p_multiply[iVertex] *= factor;
}

__global__ void kernelCreateEpsilonAndJacobi_Heat_1species
(
	f64 const h_sub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_T,
	f64 * p_Tk, // T_k for substep
	NTrates * __restrict__ p_NTrates_diffusive,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	f64 * __restrict__ p__coeffself,
	f64 * __restrict__ p__epsilon,
	f64 * __restrict__ p__Jacobi,
	bool * __restrict__ p_bFailedTest,
	bool * __restrict__ p_bMask,
	bool * __restrict__ p_bMaskblock,
	bool bUseMask,
	int species,
	bool bIncorporateEps
)
{
	// 2. Calculate epsilon: given the est of T, eps = T - (T_k +- h sum kappa dot grad T)

	// So this is a lot like saying, let's call the actual routine...
	// except we also want Jacobi which means we also want coeff on self in epsilon.

	// eps= T_putative - (T_k +- h sum kappa dot grad T_putative)

	// coeff on self we want to be linearized so it incorporates the assumption that it affects kappa.
	// deps/dT = sum [[dkappa/dT = 0.5 kappa/T] dot grad T + kappa dot d/dT grad T]

	// However this means if we know kappa dot grad T then we can * by 0.5/T to get dkappa/dT part
	// But we had to collect a separate value for kappa dot d/dT grad T.

	// We certainly need to somehow modify the existing kappa dot grad T routine here.
	// what about dividing by N?

	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;
	
	if (TESTHEAT) printf("%d bUseMask %d info.flag %d \n",
		iVertex, (bUseMask ? 1 : 0), p_info_major[iVertex].flag);

	if ((bUseMask) && (p_bMaskblock[blockIdx.x] == 0)) return;
	if (bUseMask) {
		if (p_bMask[iVertex] == 0)  return; // do not modify anything
	}

	structural info = p_info_major[iVertex];
	 
	if ((info.flag == DOMAIN_VERTEX)) { //|| (info.flag == OUTERMOST)) {

		NTrates Rates = p_NTrates_diffusive[iVertex];
		nvals n = p_n_major[iVertex];
		f64 Area = p_AreaMajor[iVertex];
		
		f64 T, actual_T, epsilon;
		f64 T_k = p_Tk[iVertex];

		f64 N = (species != 0)?(n.n*Area) : (n.n_n*Area);		
		T = p_T[iVertex];
		
		if (species == 0) actual_T = T_k + (h_sub / N)*Rates.NnTn;		
		if (species == 1) actual_T = T_k + (h_sub / N)*Rates.NiTi;		
		if (species == 2) actual_T = T_k + (h_sub / N)*Rates.NeTe;
		
		epsilon = T - actual_T;

		p__epsilon[iVertex] = epsilon;

		if (bIncorporateEps) {
			p__Jacobi[iVertex] = -epsilon / p__coeffself[iVertex]; // should never be 0 // match the other function for a minute
		} else {
			p__Jacobi[iVertex] = -actual_T;			
			// Try just doing Richardson beyond the 1st regressor.
		}
		if (TESTHEAT) printf("%d : T %1.10E T_k %1.10E epsilon %1.10E d/dt NiTi %1.10E hsub/N %1.10E coeffself %1.10E Jacobi %1.10E \n",
			iVertex, T, T_k, epsilon, Rates.NiTi, h_sub/N, p__coeffself[iVertex], p__Jacobi[iVertex]);

		if (p_bFailedTest != 0) {
			if (epsilon*epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_T*actual_T + 1.0e-14*1.0e-14))
					p_bFailedTest[blockIdx.x] = true;
		
			// Why 1.0e-10 in absolute error, for minimum value we care about:
			// N = 2.0e12*7e-5 = 1e8 
			// root N = 1e4
			// root N * 1e-14 erg = 1e-10 for (root N) T
		}
	} else {
		p__epsilon[iVertex] = 0.0;
		p__Jacobi[iVertex] = 0.0;
	};
}

__global__ void kernelCreateEpsilonHeat_1species
(
	f64 const h_sub,
	structural * __restrict__ p_info_major,
	f64 * __restrict__ p_T,
	f64 * __restrict__ p_Tk, // T_k for substep
	NTrates * __restrict__ p_NTrates_diffusive,
	nvals * __restrict__ p_n_major,
	f64 * __restrict__ p_AreaMajor,
	f64 * __restrict__ p__epsilon,
	bool * __restrict__ p_bFailedTest,
	bool * __restrict__ p_bMask,
	bool * __restrict__ p_bMaskblock,
	bool bUseMask,
	int species
)
{
	// 2. Calculate epsilon: given the est of T, eps = T - (T_k +- h sum kappa dot grad T)
	
	long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;

	if ((bUseMask) && (p_bMaskblock[blockIdx.x] == 0)) return;
	if (bUseMask) {
		if (p_bMask[iVertex] == 0)  return; // do not modify anything
	}

	structural info = p_info_major[iVertex];

	if ((info.flag == DOMAIN_VERTEX)) { //|| (info.flag == OUTERMOST)) {

		NTrates Rates = p_NTrates_diffusive[iVertex];
		nvals n = p_n_major[iVertex];
		f64 Area = p_AreaMajor[iVertex];

		f64 T, actual_T, epsilon;
		f64 T_k = p_Tk[iVertex];

		f64 N = (species != 0) ? (n.n*Area) : (n.n_n*Area);

		T = p_T[iVertex];

		if (species == 0) actual_T = T_k + (h_sub / N)*Rates.NnTn;
		if (species == 1) actual_T = T_k + (h_sub / N)*Rates.NiTi;
		if (species == 2) actual_T = T_k + (h_sub / N)*Rates.NeTe;

		epsilon = T - actual_T;
		
		// although putting this here just seems completely wrong.
		// epsilon *= sqrt(N);
		p__epsilon[iVertex] = epsilon;

		// we did not take account of sqrt N. Is that why we have misjudged ROC? Yes of course.

		// We have yet to experiment why it failed with Jacobi inc sqrt(N) --- not clear why at all.
		if ((TESTHEAT))
			printf("%d epsilon %1.14E T %1.10E T_k %1.12E hsub/N %1.14E dbydt{NiTi} %1.14E\n", iVertex, epsilon, T, T_k, h_sub / N, Rates.NiTi);

		if (p_bFailedTest != 0) {
			//if (epsilon*epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_T*actual_T*N + 1.0e-10*1.0e-10))
			//	p_bFailedTest[blockIdx.x] = true;
			// Why 1.0e-10 in absolute error, for minimum value we care about:
			// N = 2.0e12*7e-5 = 1e8 
			// root N = 1e4
			// root N * 1e-14 erg = 1e-10 for (root N) T

			// NO ROOT N INVOLVED:

			if (epsilon*epsilon > REL_THRESHOLD_HEAT*REL_THRESHOLD_HEAT*(actual_T*actual_T + 4.0e-14*4.0e-14))
					p_bFailedTest[blockIdx.x] = true;
		}
	}
	else {
		p__epsilon[iVertex] = 0.0;
	};
}


__global__ void AggregateSmashMatrix(
	f64 * __restrict__ p_Jacobianesque_list,
	f64 * __restrict__ p_eps,
	f64 * __restrict__ p_smash_matrix_block,
	f64 * __restrict__ p_smash_vector_block
) {
	__shared__ f64 smash_collect[SQUASH_POINTS*threadsPerTileMajor];
	__shared__ f64 Jacobian_data[SQUASH_POINTS*threadsPerTileMajor];

	// 1. Load in d eps/ d beta 
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;

	memcpy(&(Jacobian_data[threadIdx.x*SQUASH_POINTS]),
		&(p_Jacobianesque_list[iMinor*SQUASH_POINTS]), sizeof(f64)*SQUASH_POINTS);
	
	// 2. Loop:
	int j;
#pragma unroll
	for (int i = 0; i < SQUASH_POINTS; i++)
	{
		f64 use = Jacobian_data[threadIdx.x*SQUASH_POINTS + i];
		for (j = 0; j < SQUASH_POINTS; j++)
			smash_collect[threadIdx.x*SQUASH_POINTS + j] = use*Jacobian_data[threadIdx.x*SQUASH_POINTS + j];
		
		// Now add up row vectors we got:

		__syncthreads();

		int s = blockDim.x;
		int k = s / 2;

		while (s != 1) {
			if (threadIdx.x < k)
			{
#pragma unroll
				for (int y = 0; y < SQUASH_POINTS; y++)
					smash_collect[threadIdx.x*SQUASH_POINTS + y] += smash_collect[(threadIdx.x + k)*SQUASH_POINTS + y];
			};
			__syncthreads();

			// Modify for case blockdim not 2^n:
			if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
				for (int y = 0; y < SQUASH_POINTS; y++)
					smash_collect[threadIdx.x*SQUASH_POINTS + y] += smash_collect[(threadIdx.x + s - 1)*SQUASH_POINTS + y];
			};
			// In case k == 81, add [39] += [80]
			// Otherwise we only get to 39+40=79.
			s = k;
			k = s / 2;
			__syncthreads();
		};

		if (threadIdx.x == 0)
			memcpy(&(p_smash_matrix_block[blockIdx.x*SQUASH_POINTS*SQUASH_POINTS + i*SQUASH_POINTS]),
				smash_collect, sizeof(f64)*SQUASH_POINTS);
	};

	// And eps vs deps?
	f64 epsilon = p_eps[iMinor];
	
	for (j = 0; j < SQUASH_POINTS; j++)
		smash_collect[threadIdx.x*SQUASH_POINTS + j] = epsilon*Jacobian_data[threadIdx.x*SQUASH_POINTS + j];

	// Now add up row vectors we got:

	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
#pragma unroll
			for (int y = 0; y < SQUASH_POINTS; y++)
				smash_collect[threadIdx.x*SQUASH_POINTS + y] += smash_collect[(threadIdx.x + k)*SQUASH_POINTS + y];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			for (int y = 0; y < SQUASH_POINTS; y++)
				smash_collect[threadIdx.x*SQUASH_POINTS + y] += smash_collect[(threadIdx.x + s - 1)*SQUASH_POINTS + y];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
		memcpy(&(p_smash_vector_block[blockIdx.x*SQUASH_POINTS]), smash_collect, sizeof(f64)*SQUASH_POINTS);
}


__global__ void kernelCreateEpsilon_Heat_for_Jacobi
	(
		f64 const h_sub,
		structural * __restrict__ p_info_major,
		f64 * __restrict__ p_T_n,
		f64 * __restrict__ p_T_i,
		f64 * __restrict__ p_T_e,
		T3 * p_Tk, // T_k for substep

		NTrates * __restrict__ p_NTrates_diffusive,
		nvals * __restrict__ p_n_major,
		f64 * __restrict__ p_AreaMajor,

		f64 * __restrict__ p_eps_n,
		f64 * __restrict__ p_eps_i,
		f64 * __restrict__ p_eps_e,
		bool * __restrict__ p_bMask3,
		bool * __restrict__ p_bMaskblock,
		bool bUseMask
	)
	{
		if ((bUseMask) && (p_bMaskblock[blockIdx.x] == 0)) return;
		
		long const iVertex = blockDim.x*blockIdx.x + threadIdx.x;

		bool bMask[3];
		if (bUseMask) {			

			bMask[0] = p_bMask3[iVertex];
			bMask[1] = p_bMask3[iVertex + NUMVERTICES];
			bMask[2] = p_bMask3[iVertex + NUMVERTICES * 2];
			if ((bMask[0] == 0) && (bMask[1] == 0) && (bMask[2] == 0)) return;
		};

		structural info = p_info_major[iVertex];

		if (iVertex == VERTCHOSEN) printf("%d : bMask[2] %d info.flag %d \n",
			iVertex, (bMask[2]) ? 1 : 0, info.flag);

		if ((info.flag == DOMAIN_VERTEX)) { //|| (info.flag == OUTERMOST)) {

			T3 T_k;
			f64 Tn, Ti, Te;
			memcpy(&T_k, &(p_Tk[iVertex]), sizeof(T3));

			NTrates Rates = p_NTrates_diffusive[iVertex];
			
			nvals n = p_n_major[iVertex];
			f64 Area = p_AreaMajor[iVertex];
			f64 N = n.n*Area;
			f64 Nn = n.n_n*Area;

			if ((bUseMask == 0) || (bMask[2])) {
				Te = p_T_e[iVertex];
				f64 actual_Te = T_k.Te + (h_sub / N)*Rates.NeTe;
				f64 epsilon_e = Te - actual_Te;
				epsilon_e *= sqrt(N);
				p_eps_e[iVertex] = epsilon_e;

			//	if ((iVertex == VERTCHOSEN)) printf("%d : Te %1.9E actual %1.9E Tk %1.9E rates %1.9E epsilon %1.11E \n",
			//		iVertex,
			//		Te, actual_Te, T_k.Te, Rates.NeTe, epsilon_e);
			};

			if ((bUseMask == 0) || (bMask[0])) {
				Tn = p_T_n[iVertex];
				f64 actual_Tn = T_k.Tn + (h_sub / Nn)*Rates.NnTn;
				f64 epsilon_n = Tn - actual_Tn;
				epsilon_n *= sqrt(Nn);
				p_eps_n[iVertex] = epsilon_n;
			};

			if ((bUseMask == 0) || (bMask[1])) {
				Ti = p_T_i[iVertex];
				f64 actual_Ti = T_k.Ti + (h_sub / N)*Rates.NiTi;
				f64 epsilon_i = Ti - actual_Ti;
				// Weighted Least Squares:
				epsilon_i *= sqrt(N);
				p_eps_i[iVertex] = epsilon_i;
				
			//	if (iVertex == VERTCHOSEN) 
			//		printf("iVertex %d actual_Ti %1.9E Tk %1.9E Rates %1.10E epsilon %1.10E\n",
			//			iVertex, actual_Ti, T_k.Ti, Rates.NiTi, epsilon_i);
					
			};
		}; 
		// NOT NEEDED: ENSURE WE SET EPS TO 0 FIRST INSTEAD.
// else			p_eps_n[iVertex] = 0.0;
		//	p_eps_i[iVertex] = 0.0;
			//p_eps_e[iVertex] = 0.0;		
	}

__global__ void kernelCreateEpsilonAndJacobi(
	f64 const h_use,
	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az_array_next,
	f64 * __restrict__ p_Az_array,
	f64 * __restrict__ p_Azdot0,
	f64 * __restrict__ p_gamma,
	f64 * __restrict__ p_LapCoeffSelf,
	f64 * __restrict__ p_Lap_Aznext,
	f64 * __restrict__ p_epsilon,
	f64 * __restrict__ p_Jacobi_x,
	bool * __restrict__ p_bFail)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 eps;
	structural info = p_info[iMinor];
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL))
	{
		eps = 0.0; // p_Lap_Aznext[iMinor];
		p_Jacobi_x[iMinor] = 0.0; // -eps / p_LapCoeffSelf[iMinor];
//		if (iMinor == 0) printf("\nGPU: eps[0] %1.14E LapCoeffself %1.14E \n", eps, p_LapCoeffSelf[iMinor]);
		// but we reset it in ResetFrills called for regressor
	}
	else {
#ifdef MIDPT_A_AND_ACTUALLY_MIDPT_A_NOT_JUST_EFFECT_ON_AZDOT

		// WE COULD CHOOSE to leave it so that Az advances with Azdot_k+1 : we don't know a reason why not.

		eps = p_Az_array_next[iMinor] - p_Az_array[iMinor] 
			- h_use * p_gamma[iMinor] * p_Lap_Aznext[iMinor]
			- h_use * p_Azdot0[iMinor];
		
		p_Jacobi_x[iMinor] = -eps / (1.0 - h_use * p_gamma[iMinor] * p_LapCoeffSelf[iMinor]);
#else
		f64 Aznext = p_Az_array_next[iMinor];
		f64 gamma = p_gamma[iMinor];
		eps = Aznext - p_Az_array[iMinor] - h_use * gamma * p_Lap_Aznext[iMinor] - h_use*p_Azdot0[iMinor];

//		if (iMinor == VERTCHOSEN + BEGINNING_OF_CENTRAL) {
//			printf("iMinor %d eps %1.9E Aznext %1.9E Azk? %1.9E h_use %1.9E gamma %1.9E LapAz %1.9E Azdot0 %1.9E\n",
//				iMinor, eps, Aznext, p_Az_array[iMinor], h_use, gamma, p_Lap_Aznext[iMinor], p_Azdot0[iMinor]);
//		}

		p_Jacobi_x[iMinor] = -eps / (1.0 - h_use * gamma * p_LapCoeffSelf[iMinor]);
		
		if (p_Jacobi_x[iMinor] != p_Jacobi_x[iMinor]) printf("p_Jacobi_x[%d] was NaN : eps %1.9E gamma %1.9E LCS %1.9E LapAznext %1.9E Azdot0 %1.9E Aznext %1.9E\n",
			iMinor, eps, gamma, p_LapCoeffSelf[iMinor], p_Lap_Aznext[iMinor], p_Azdot0[iMinor], Aznext);

//		if (iMinor == 32641) printf("32641: eps %1.9E Az %1.12E Azk %1.12E h %1.10E gamma %1.10E LapAz %1.12E "
//			"h Azdot0 %1.10E\n",
//			eps, p_Az_array_next[iMinor], p_Az_array[iMinor],
//			h_use,gamma,
//			p_Lap_Aznext[iMinor],
//			h_use*p_Azdot0[iMinor]);

#endif
//		if (iMinor == 25526) printf("\n\n########\nJacobi_x 25526 GPU: %1.14E eps %1.14E gamma %1.14E LapCoeffself %1.14E\n",
//			p_Jacobi_x[iMinor], eps, p_gamma[iMinor], p_LapCoeffSelf[iMinor]);
//		if (iMinor == 86412) printf("Jacobi_x 86412 GPU: %1.14E eps %1.14E gamma %1.14E LapCoeffself %1.14E\n",
//			p_Jacobi_x[iMinor], eps, p_gamma[iMinor], p_LapCoeffSelf[iMinor]);
//		if (iMinor == 69531) printf("Jacobi_x 69531 GPU: %1.14E eps %1.14E gamma %1.14E LapCoeffself %1.14E\n",
//			p_Jacobi_x[iMinor], eps, p_gamma[iMinor], p_LapCoeffSelf[iMinor]);
// Typical value for Az is like 100+ so use 0.1 as minimum that we care about, times relthresh.
		if (eps*eps > RELTHRESH_AZ_d*RELTHRESH_AZ_d*(Aznext*Aznext + 10.0*10.0)) p_bFail[blockIdx.x] = true;

		// This does not seem to be triggering.
	};
	p_epsilon[iMinor] = eps;
}


__global__ void kernelCreateExplicitStepAz(
	f64 const hsub,
	f64 * __restrict__ pAzdot0,
	f64 * __restrict__ pgamma,
	f64 * __restrict__ pLapAz, // we based this off of half-time Az.
	f64 * __restrict__ p_result) // = h (Azdot0 + gamma*LapAz)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	p_result[iMinor] = hsub*(pAzdot0[iMinor] + pgamma[iMinor] * pLapAz[iMinor]);
}


__global__ void kernelCreateEpsilon_Az_CG(
	f64 const h_use,
	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az_plus,
	f64 * __restrict__ p_Az_k,
	f64 * __restrict__ p_Azdot0,
	f64 * __restrict__ p_gamma,
	f64 * __restrict__ p_Lap_Az,
	f64 * __restrict__ p_epsilon,
	f64 * __restrict__ p__sqrtfactor,
	bool * __restrict__ p_bFail,
	bool const bSaveFail)
{
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	f64 eps;
	structural info = p_info[iMinor];
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL))
	{
		eps = 0.0; // p_Lap_Aznext[iMinor];		
	}
	else {
		// WE COULD CHOOSE to leave it so that Az advances with Azdot_k+1 : we don't know a reason why not.
		f64 sqrtfactor = p__sqrtfactor[iMinor];
		f64 one_over_sqrt;
		if (sqrtfactor != 0.0) {
			one_over_sqrt = 1.0 / sqrtfactor;
		} else {
			one_over_sqrt = 1.0;
		};

		f64 Aznext = p_Az_plus[iMinor];
		eps = one_over_sqrt*(Aznext - p_Az_k[iMinor] - h_use * p_Azdot0[iMinor])
			- sqrtfactor * p_Lap_Az[iMinor]; // notice this is the integrated Lap											// 
			
		// eps^2 sqrtfactor sqrtfactor = original eps squared.
		if (bSaveFail)
			if (eps*eps*sqrtfactor*sqrtfactor > 1.0e-10*1.0e-10*(Aznext*Aznext + 1.0*1.0))
				p_bFail[blockIdx.x] = true;
		// An optimization is probably to store values in shared then amalgamate, send data to global on 1 thread. ?

		if (eps != eps) printf("iMinor %d eps %1.9E Aznext %1.9E gamma %1.9E sqrtfactor %1.9E over %1.9E info.flag %d LapAz %1.9E Azdot0 %1.9E\n",
			iMinor, eps, Aznext, p_gamma[iMinor], sqrtfactor, one_over_sqrt, info.flag, p_Lap_Az[iMinor], p_Azdot0[iMinor]);

	};
	p_epsilon[iMinor] = eps;
}


__global__ void kernelSetZero(
	f64 * __restrict__ data
) {
	long const index = blockDim.x*blockIdx.x + threadIdx.x;
	data[index] = 0.0;
}

__global__ void kernelCreate_further_regressor(
	structural * __restrict__ p_info,
	f64 h_use,
	f64 * __restrict__ p_regressor,
	f64 * __restrict__ p_Lap_regressor,
	f64 * __restrict__ p_LapCoeffSelf,
	f64 * __restrict__ p_gamma,
	f64 * __restrict__ p_regressor2)
{
	long const index = blockDim.x*blockIdx.x + threadIdx.x;
	/*
	f64 d_eps_by_d_beta;
	structural info = p_info[index];
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL))
	{
		d_eps_by_d_beta = 0.0; //  Lap_Jacobi[iMinor]; // try ignoring
		// Need to fill this in afterwards by ResetFrills?
	}
	else {
		d_eps_by_d_beta = (p_regressor[index] - h_use * p_gamma[index] * p_Lap_regressor[index]);
	};
	p_regressor2[index] = d_eps_by_d_beta / (1.0 - h_use * p_gamma[index] * p_LapCoeffSelf[index]);*/
	// Try just this instead:
	p_regressor2[index] = p_gamma[index] * p_Lap_regressor[index]; // d_eps_by_d_beta / (1.0 - h_use * p_gamma[index] * p_LapCoeffSelf[index]);

}



__global__ void kernelGetLap_minor(

	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapAz)
{

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];

	//	__shared__ f64 sum1[threadsPerTileMinor];
	//	__shared__ f64 sum2[threadsPerTileMinor];
	//	__shared__ f64 sum3[threadsPerTileMinor];

	// 4.5 per thread.
	// Not clear if better off with L1 or shared mem in this case?? Probably shared mem.

	// For now, stick with idea that vertices have just major indices that come after tris.

	// Minor indices are not made contiguous - although it might be better ultimately.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	shared_Az[threadIdx.x] = p_Az[iMinor];
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Az_verts[threadIdx.x] = p_Az[iVertex + BEGINNING_OF_CENTRAL];
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	f64_vec2 opppos, prevpos, nextpos, integ_grad_Az;

	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
// and only half the threads run first for the vertex part. That is a pretty good idea.


	if (threadIdx.x < threadsPerTileMajor) {
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		f64_vec2 endpt0, endpt1;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.

		ourAz = shared_Az_verts[threadIdx.x];
		short iprev = tri_len - 1;
		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevAz = shared_Az[izTri[iprev] - StartMinor];
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		}
		else {
			prevAz = p_Az[izTri[iprev]];
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			oppAz = shared_Az[izTri[i] - StartMinor];
			opppos = shared_pos[izTri[i] - StartMinor];
		}
		else {
			oppAz = p_Az[izTri[i]];
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		f64_vec2 store_centroid = opppos;

		endpt0 = THIRD * (info.pos + opppos + prevpos);
		f64_vec2 store_first_point = endpt0;
		short inext, iend = tri_len;
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) iend = tri_len - 2;
		
		// Bear in mind for OUTERMOST, the triangles go clockwise not anticlockwise.
		
		if ((info.flag == INNERMOST)) {
			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
			if (TESTLAP) printf("vertex %d endpt0 %1.9E %1.9E projendpt0 %1.9E %1.9E \n",
				iVertex, endpt0.x, endpt0.y, projendpt0.x, projendpt0.y);

			if (TESTLAP) printf("%d Innermost: AreaMinor += %1.10E AreaMinor %1.10E \n",
				iVertex, (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x, AreaMinor);

		};
		
		//	if (info.flag == OUTERMOST) {
		//		printf("DEBUG: iVertex %d info.neigh_len %d iend %d izTri[0] %d izTri[iend-1] %d izTri[iend-2] %d "
		//			"flags 0 %d 1 %d 2 %d 3 %d 4 %d 5 %d\n"
		//			"positions 01234 (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) \n"
		//			, iVertex, info.neigh_len,
		//			iend, izTri[0], izTri[iend - 1],
		//			izTri[iend - 2], 
		//			p_info[izTri[0]].flag, p_info[izTri[1]].flag, p_info[izTri[2]].flag,
		//			p_info[izTri[3]].flag, p_info[izTri[4]].flag,
		//			p_info[izTri[0]].pos.x, p_info[izTri[0]].pos.y, p_info[izTri[1]].pos.x, p_info[izTri[1]].pos.y,
		//			p_info[izTri[2]].pos.x, p_info[izTri[2]].pos.y, p_info[izTri[3]].pos.x, p_info[izTri[3]].pos.y,
		//			p_info[izTri[4]].pos.x, p_info[izTri[4]].pos.y
		//			);
		//	
		//		if (DIRICHLET == false) {
		//			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
		//		}
		//		else {
		//			f64 radius = info.pos.modulus();
		//			endpt0.project_to_radius(projendpt0,
		//				0.5*(FRILL_CENTROID_OUTER_RADIUS_d + radius)); // back of cell for Lap purposes
		//			// flatten the cell to get wall halfway out to 0 line.
		//		}

		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);

		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextAz = shared_Az[izTri[inext] - StartMinor];
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextAz = p_Az[izTri[inext]];
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			// We should always call ResetFrillsAz first on the argument, so that if next is a frill then
			// we got the correct value as nextAz.
			
			endpt1 = THIRD * (nextpos + info.pos + opppos);					
			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
					
	//		Indicates we think 1 is anticlockwise from 0. For OUTERMOST, it's pointing IN so the rest must do too, then we divide out the minus.
			
			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
			//if (TESTLAP) printf("vertex %d endpt0 %1.9E %1.9E endpt1 %1.9E %1.9E Area += %1.10E edge_normal.x %1.9E\n",
			//	iVertex, endpt0.x, endpt0.y, endpt1.x, endpt1.y,
			//	(0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x, edge_normal.x);

			if (TESTLAP) printf("iVertex %d izTri[%d] %d ourAz %1.8E oppAz %1.8E prevAz %1.8E nextAz %1.8E contrib %1.14E "
				"grad Az %1.9E %1.9E Area_quad %1.8E\n",
				iVertex, i, izTri[i],
				ourAz, oppAz, prevAz, nextAz,
				integ_grad_Az.dot(edge_normal) / area_quadrilateral,
				integ_grad_Az.x / area_quadrilateral,
				integ_grad_Az.y / area_quadrilateral,
				area_quadrilateral);

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			iprev = i;
		}; // next i

		if (info.flag == INNERMOST) {
			// Now add on the final sides to give area:

			f64_vec2 projendpt1;
			endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			if (TESTLAP) printf("vertex %d endpt1 %1.9E %1.9E projendpt1 %1.9E %1.9E \n",
				iVertex, endpt1.x, endpt1.y, projendpt1.x, projendpt1.y);

			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;

			if (TESTLAP) printf("vertex %d Innermost: AreaMinor += %1.10E \n",
				iVertex, (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x);

			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
			// unchanged... check later

			if (TESTLAP) printf(" vertex %d Innermost: AreaMinor += %1.10E AreaMinor %1.10E \n",
				iVertex, (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x,
				AreaMinor);

		}

		if (info.flag == OUTERMOST)
		{
			// 3 sides to add.

			//       3   4
			//     2       0
			//         1
			// endpt0=endpt1 is now the point north of edge facing 2.
			// opppos is centre of tri (3).
			
			oppAz = 0.0;
			nextAz = 0.0;

			if (RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, info.pos.modulus() + (FRILL_CENTROID_OUTER_RADIUS_d - info.pos.modulus())*1.16);
				endpt1 = THIRD*(opppos + info.pos + nextpos);
				
				oppAz = prevAz*(prevpos.modulus() / opppos.modulus());
				nextAz = ourAz*(info.pos.modulus() / nextpos.modulus());		
			};

			if (!RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, FRILL_CENTROID_OUTER_RADIUS_d);
				endpt1 = THIRD*(opppos + info.pos + nextpos);
				// map radially inwards so that radius is halfway out to the zero arc:
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);
			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) {
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				if (TESTLAP) printf("iVertex %d ourAz %1.8E oppAz %1.8E prev %1.8E next %1.8E contrib %1.14E "
					"grad Az %1.9E %1.9E edgenormal %1.8E %1.8E\n",
					iVertex, ourAz, oppAz, prevAz, nextAz,
					integ_grad_Az.dot(edge_normal) / area_quadrilateral,
					integ_grad_Az.x / area_quadrilateral,
					integ_grad_Az.y / area_quadrilateral, edge_normal.x, edge_normal.y);
			}
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;
			// NOW WE ARE GOING TO LOOK OUTWARDS
			
			inext = tri_len - 1;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			nextAz = 0.0;

			endpt1 = THIRD*(opppos + info.pos + nextpos);
			if (RADIALDECLINE) {
				//This was incorrect
				nextAz = p_Az[izTri[0]]*(store_centroid.modulus()/nextpos.modulus());
			}
			
			if (!RADIALDECLINE) {
				// map radially inwards so that radius is halfway out to the zero arc.
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) {
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				if (TESTLAP) printf("iVertex %d ourAz %1.8E oppAz %1.8E prev %1.8E next %1.8E contrib %1.14E "
					"grad Az %1.9E %1.9E edgenormal %1.8E %1.8E\n",
					iVertex, ourAz, oppAz, prevAz, nextAz,
					integ_grad_Az.dot(edge_normal) / area_quadrilateral,
					integ_grad_Az.x / area_quadrilateral,
					integ_grad_Az.y / area_quadrilateral, edge_normal.x, edge_normal.y);
			};
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			// That was the side looking out.

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;
			
			// WE ARE GOING TO LOOK NORTHEAST

			endpt1 = store_first_point;
			nextAz = p_Az[izTri[0]];
			nextpos = p_info[izTri[0]].pos;

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) {
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				if (TESTLAP) printf("iVertex %d ourAz %1.8E oppAz %1.8E prev %1.8E next %1.8E contrib %1.14E "
					"grad Az %1.9E %1.9E edgenormal %1.8E %1.8E\n",
					iVertex, ourAz, oppAz, prevAz, nextAz,
					integ_grad_Az.dot(edge_normal) / area_quadrilateral,
					integ_grad_Az.x / area_quadrilateral,
					integ_grad_Az.y / area_quadrilateral, edge_normal.x, edge_normal.y);
			};

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
		};
		
		// But this points up why CG doesn't roll properly. The presence of the AreaMinor factor makes
		// the equations not symmetric.

		if (TESTLAP) printf("LapAz_integ %1.10E AreaMinor %1.10E LapAz %1.10E \n", Our_integral_Lap_Az, AreaMinor,
			Our_integral_Lap_Az / AreaMinor);

		p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
//		p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor; // reset just because otherwise we're inconsistent about area/position in a subcycle

	}; // was thread in the first half of the block

	info = p_info[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];

	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// izNeighMinor[0] is actually vertex 0 if you are triangle 0.
		// Rethink:  
		// Try izNeighMinor[3] because this is meant to be neighbour 0.
		// Why are we doing all this? Just set = 0 out here.

		p_LapAz[iMinor] = 0.0;
	}
	else {

		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevAz = p_Az[izNeighMinor[iprev]];
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;
		if (prevpos.dot(prevpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
		{
			// outer frill
			if (RADIALDECLINE)
				prevAz = ourAz*(info.pos.modulus() / prevpos.modulus());
		}
		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[i] - StartMinor];
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				oppAz = p_Az[izNeighMinor[i]];
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
		if (opppos.dot(opppos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
		{
			// outer frill
			if (RADIALDECLINE)
				oppAz = ourAz*(info.pos.modulus() / opppos.modulus());
		}
#pragma unroll 

		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextAz = p_Az[izNeighMinor[inext]];
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			if (nextpos.dot(nextpos) > 0.999999*0.999999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			{
				// outer frill
				if (RADIALDECLINE)
					nextAz = ourAz*(info.pos.modulus() / nextpos.modulus());
			}

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			f64_vec2 integ_grad_Az;

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;

			f64_vec2 edge_normal;
			edge_normal.x = THIRD*(nextpos.y - prevpos.y);
			edge_normal.y = THIRD*(prevpos.x - nextpos.x);

			// This shouldn't be necessary anyway but is especially no good if it's not meant to be flat

			if (  
				((opppos.dot(opppos) < 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) || (DIRICHLET) ||
					(RADIALDECLINE)) &&
				(opppos.dot(opppos) > 1.00001*1.00001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d)
				)
			{
				// neighbour's not a frill, or it's Dirichlet or radial decline looking outwards.
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				if ((TESTLAP2) || (Our_integral_Lap_Az != Our_integral_Lap_Az)) {
					printf("iMinor %d [i] %d ourAz %1.9E theirs %1.9E prev %1.9E next %1.9E numer %1.9E contrib %1.10E areaquad %1.8E\n",
						iMinor, izNeighMinor[i], ourAz, oppAz, prevAz, nextAz,
						integ_grad_Az.dot(edge_normal),
						integ_grad_Az.dot(edge_normal) / area_quadrilateral,
						area_quadrilateral);					
				};
			}
			AreaMinor += SIXTH*((prevpos.x + info.pos.x + opppos.x) +
				(nextpos.x + info.pos.x + opppos.x))*edge_normal.x;

			prevAz = oppAz;
			oppAz = nextAz;
			prevpos = opppos;
			opppos = nextpos;
		};

		p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
	//	p_AreaMinor[iMinor] = AreaMinor; // reset for each substep	

	};
}

__global__ void kernelGetLap_minor_SYMMETRIC(
	 
	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapAz,
	f64 * __restrict__ p_AreaMinor, // need to save off to multiply back for symmetry
	bool const bDivideByArea
)
{
	// Symmetric version with circumcenters to define corners of minor cells
	// so as to use conjugate gradient for Az.

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	shared_Az[threadIdx.x] = p_Az[iMinor];
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Az_verts[threadIdx.x] = p_Az[iVertex + BEGINNING_OF_CENTRAL];
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	f64_vec2 opppos, prevpos, nextpos, integ_grad_Az;
	f64_vec2 endpt0, endpt1;

	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.
	printf("don't call this routine unless the mess is reformed so that drawing triangles between these points will actually produce a Delaunay triangulation. Because if it doesn't, circumcenters aren't in triangles and it will be nonsense.\n");

	if (threadIdx.x < threadsPerTileMajor) {
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		
		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.

		ourAz = shared_Az_verts[threadIdx.x];
		short iprev = tri_len - 1;
		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
//			prevAz = shared_Az[izTri[iprev] - StartMinor];
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		}
		else {
//			prevAz = p_Az[izTri[iprev]];
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			oppAz = shared_Az[izTri[i] - StartMinor];
			opppos = shared_pos[izTri[i] - StartMinor];
		}
		else {
			oppAz = p_Az[izTri[i]];
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		f64_vec2 store_centroid = opppos;
		// endpt0 = THIRD * (info.pos + opppos + prevpos);

		CalculateCircumcenter(&endpt0, info.pos, opppos, prevpos);

		f64_vec2 store_first_point = endpt0;
		short inext, iend = tri_len;
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) iend = tri_len - 2;

		// Bear in mind for OUTERMOST, the triangles go clockwise not anticlockwise.

		if ((info.flag == INNERMOST)) {
			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};
		
		//	if (info.flag == OUTERMOST) {
		//		printf("DEBUG: iVertex %d info.neigh_len %d iend %d izTri[0] %d izTri[iend-1] %d izTri[iend-2] %d "
		//			"flags 0 %d 1 %d 2 %d 3 %d 4 %d 5 %d\n"
		//			"positions 01234 (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) (%1.8E, %1.8E) \n"
		//			, iVertex, info.neigh_len,
		//			iend, izTri[0], izTri[iend - 1],
		//			izTri[iend - 2], 
		//			p_info[izTri[0]].flag, p_info[izTri[1]].flag, p_info[izTri[2]].flag,
		//			p_info[izTri[3]].flag, p_info[izTri[4]].flag,
		//			p_info[izTri[0]].pos.x, p_info[izTri[0]].pos.y, p_info[izTri[1]].pos.x, p_info[izTri[1]].pos.y,
		//			p_info[izTri[2]].pos.x, p_info[izTri[2]].pos.y, p_info[izTri[3]].pos.x, p_info[izTri[3]].pos.y,
		//			p_info[izTri[4]].pos.x, p_info[izTri[4]].pos.y
		//			);
		//	
		//		if (DIRICHLET == false) {
		//			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
		//		}
		//		else {
		//			f64 radius = info.pos.modulus();
		//			endpt0.project_to_radius(projendpt0,
		//				0.5*(FRILL_CENTROID_OUTER_RADIUS_d + radius)); // back of cell for Lap purposes
		//			// flatten the cell to get wall halfway out to 0 line.
		//		}

		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);

		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextAz = shared_Az[izTri[inext] - StartMinor];
				nextpos = shared_pos[izTri[inext] - StartMinor];
			}
			else {
				nextAz = p_Az[izTri[inext]];
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			
			CalculateCircumcenter(&endpt1, info.pos, opppos, nextpos);

			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// Symmetric, with circumcenters:
			//		normal_gradient = (oppAz - ourAz) / ((opppos - info.pos).modulus());
			//		Our_integral_Lap_Az += normal_gradient*edge_normal.modulus();
			// Reduce number of square roots:
			Our_integral_Lap_Az += (oppAz - ourAz)*sqrt((edge_normal.dot(edge_normal)) / ((opppos - info.pos).dot(opppos - info.pos)));

			if (Our_integral_Lap_Az != Our_integral_Lap_Az) printf("%d oppAz %1.8E ourAz %1.8E edge_normal.dot(en) %1.8E opposdot %1.8E \n",
				iMinor, oppAz, ourAz, edge_normal.dot(edge_normal), ((opppos - info.pos).dot(opppos - info.pos)));

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			if (TESTLAP) printf("iVertex %d izTri[%d] %d ourAz %1.8E oppAz %1.8E delta_out %1.8E delta_edge %1.8E\n",
				iVertex, i, izTri[i],
				ourAz, oppAz,
				(opppos - info.pos).modulus(),
				edge_normal.modulus()
			);

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			iprev = i;
		}; // next i

		if (info.flag == INNERMOST) {
			// Now add on the final sides to give area:

			f64_vec2 projendpt1;
			endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;
			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
			// unchanged... check later
		}

		if (info.flag == OUTERMOST)
		{
			// 3 sides to add.

			//       3   4
			//     2       0
			//         1
			// endpt0=endpt1 is now the point north of edge facing 2.
			// opppos is centre of tri (3).

			if (!RADIALDECLINE) {
				oppAz = 0.0;
				nextAz = 0.0;
				info.pos.project_to_radius(nextpos, FRILL_CENTROID_OUTER_RADIUS_d);
				CalculateCircumcenter(&endpt1, info.pos, opppos, nextpos); // THIRD*(opppos + info.pos + nextpos);
			}
			if (RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, info.pos.modulus() + (FRILL_CENTROID_OUTER_RADIUS_d-info.pos.modulus())*1.16 );
				CalculateCircumcenter(&endpt1, info.pos, opppos, nextpos); // THIRD*(opppos + info.pos + nextpos);
				oppAz = prevAz*(prevpos.modulus() / opppos.modulus());
				nextAz = ourAz*(info.pos.modulus() / nextpos.modulus());
			}
				// nextpos directly above our own but only on a level with the other frill centroids

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			if (DIRICHLET || RADIALDECLINE)
				Our_integral_Lap_Az += (oppAz - ourAz)*sqrt((edge_normal.dot(edge_normal)) / ((opppos - info.pos).dot(opppos - info.pos)));
			
			// "map radially inwards so that radius is halfway out to the zero arc:"
			// no can do... nor should we need to since the edge is equidistant from both points that generated it.

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			inext = tri_len - 1;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			}
			else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			nextAz = 0.0;

			CalculateCircumcenter(&endpt1, info.pos, opppos, nextpos); // THIRD*(opppos + info.pos + nextpos);

			if (RADIALDECLINE)
				nextAz = p_Az[izTri[0]] * (store_centroid.modulus() / nextpos.modulus());

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.

			if (DIRICHLET || (RADIALDECLINE))
				Our_integral_Lap_Az += (oppAz - ourAz)*sqrt((edge_normal.dot(edge_normal)) / ((opppos - info.pos).dot(opppos - info.pos)));

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			// That was the side looking out.

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			// WE ARE GOING TO LOOK NORTHEAST

			endpt1 = store_first_point;
			// nextAz = p_Az[izTri[0]]; // cancelled because nextAz is not used
			nextpos = p_info[izTri[0]].pos;

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.

			if (DIRICHLET || (RADIALDECLINE))
				Our_integral_Lap_Az += (oppAz - ourAz)*sqrt((edge_normal.dot(edge_normal)) / ((opppos - info.pos).dot(opppos - info.pos)));

			if (Our_integral_Lap_Az != Our_integral_Lap_Az) printf(" at dirichlet oppAz %1.8E ourAz %1.8E edge_normal.dot(en) %1.8E opposdot %1.8E \n",
				oppAz, ourAz, edge_normal.dot(edge_normal), ((opppos - info.pos).dot(opppos - info.pos)));

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
		};

		// But this points up why CG doesn't roll properly. The presence of the AreaMinor factor makes
		// the equations not symmetric.
		// MULTIPLY!!
		if (bDivideByArea) Our_integral_Lap_Az /= AreaMinor;
		p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az;// / AreaMinor;
		// WE NO LONGER DIVIDE BY AreaMinor

		p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor;

		if (AreaMinor < 0.0) printf("iVertex %d : AreaMinor %1.10E \n", iVertex, AreaMinor);

	}; // was thread in the first half of the block

	info = p_info[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];

	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// izNeighMinor[0] is actually vertex 0 if you are triangle 0.
		// Rethink:  
		// Try izNeighMinor[3] because this is meant to be neighbour 0.
		// Why are we doing all this? Just set = 0 out here.

		p_LapAz[iMinor] = 0.0;
	} else {

		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		} else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
	//			prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
	//			prevAz = p_Az[izNeighMinor[iprev]];
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[i] - StartMinor];
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		} else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			} else {
				oppAz = p_Az[izNeighMinor[i]];
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
		if (opppos.dot(opppos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
		{
			// outer frill
			if (RADIALDECLINE)
				oppAz = ourAz*(info.pos.modulus() / opppos.modulus());
		}

		CalculateCircumcenter(&endpt0, info.pos, opppos, prevpos);
#pragma unroll 
		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextAz = p_Az[izNeighMinor[inext]];
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			if (nextpos.dot(nextpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			{
				// outer frill
				if (RADIALDECLINE)
					nextAz = ourAz*(info.pos.modulus() / nextpos.modulus());
			}

			CalculateCircumcenter(&endpt1, info.pos, opppos, nextpos);

			// ______________________________________________________-
			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//f64_vec2 integ_grad_Az;

			//integ_grad_Az.x = 0.5*(
			//	(ourAz + nextAz)*(info.pos.y - nextpos.y)
			//	+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
			//	+ (oppAz + prevAz)*(opppos.y - prevpos.y)
			//	+ (nextAz + oppAz)*(nextpos.y - opppos.y)
			//	);

			//integ_grad_Az.y = -0.5*( // notice minus
			//	(ourAz + nextAz)*(info.pos.x - nextpos.x)
			//	+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
			//	+ (oppAz + prevAz)*(opppos.x - prevpos.x)
			//	+ (nextAz + oppAz)*(nextpos.x - opppos.x)
			//	);

			//f64 area_quadrilateral = 0.5*(
			//	(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
			//	+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
			//	+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
			//	+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
			//	);

			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			if (
				((opppos.dot(opppos) < 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) || (DIRICHLET) || (RADIALDECLINE)) &&
				(opppos.dot(opppos) > 1.00001*1.00001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d)
				)
			{
				// neighbour's not a frill, or it's Dirichlet looking outwards. Or radial decline.
				
				// Symmetric, with circumcenters:
				//		normal_gradient = (oppAz - ourAz) / ((opppos - info.pos).modulus());
				//		Our_integral_Lap_Az += normal_gradient*edge_normal.modulus();
				// Reduce number of square roots:
				Our_integral_Lap_Az += (oppAz - ourAz)*sqrt((edge_normal.dot(edge_normal)) / ((opppos - info.pos).dot(opppos - info.pos)));
				
				if (Our_integral_Lap_Az != Our_integral_Lap_Az) 
					printf("oppAz %1.8E ourAz %1.8E edge_normal.dot(en) %1.8E opposdot %1.8E \n",
						oppAz, ourAz, edge_normal.dot(edge_normal), ((opppos - info.pos).dot(opppos - info.pos)));

				// Is there a cunning way to get rid of sqrt. We know that edge_normal faces the same way as opppos-info.pos...
				// thus, (opppos-info.pos).dot(edge_normal) = product of moduli. Hmmmm.

			}
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
			//if (iMinor == 57364) printf("%d AreaMinor %1.8E contrib %1.8E, endpt0.x %1.9E endpt0.y %1.9E endpt1.x %1.9E endpt1.y %1.9E edge.x %1.8E info.pos %1.9E %1.9E oppos %1.9E %1.9E\n",
			//	iMinor, AreaMinor, (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x,
			//	endpt0.x, endpt0.y, endpt1.x, endpt1.y, edge_normal.x,
			//	info.pos.x, info.pos.y, opppos.x, opppos.y);

			endpt0 = endpt1;
		//	prevAz = oppAz;
			oppAz = nextAz;
			prevpos = opppos;
			opppos = nextpos;
		};
		if (bDivideByArea) Our_integral_Lap_Az /= AreaMinor;
		p_LapAz[iMinor] = Our_integral_Lap_Az;// / AreaMinor;
		p_AreaMinor[iMinor] = AreaMinor; // reset for each substep	 // careful what we pass it

		if (AreaMinor < 0.0) printf("%d : AreaMinor %1.10E \n", iMinor, AreaMinor);
	};
}

/*__global__ void kernelGetLap_minor__sum(

	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapAz,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_integralLapAz,
	f64 * __restrict__ p_integralVT,
	f64 * __restrict__ p_integralTV,
	f64 * __restrict__ p_integralTT
	)
{

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];
	
	__shared__ f64 integralLapAz[threadsPerTileMinor];
	__shared__ f64 integralLapVT[threadsPerTileMinor];
	__shared__ f64 integralLapTV[threadsPerTileMinor];
	__shared__ f64 integralLapTT[threadsPerTileMinor];

	//	__shared__ f64 sum1[threadsPerTileMinor];
	//	__shared__ f64 sum2[threadsPerTileMinor];
	//	__shared__ f64 sum3[threadsPerTileMinor];
	
	// 4.5 per thread.
	// Not clear if better off with L1 or shared mem in this case?? Probably shared mem.
	
	// For now, stick with idea that vertices have just major indices that come after tris.

	// Minor indices are not made contiguous - although it might be better ultimately.
	integralLapAz[threadIdx.x] = 0.0;
	integralLapVT[threadIdx.x] = 0.0;
	integralLapTV[threadIdx.x] = 0.0;
	integralLapTT[threadIdx.x] = 0.0;

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;
	
	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	shared_Az[threadIdx.x] = p_Az[iMinor];
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Az_verts[threadIdx.x] = p_Az[iVertex + BEGINNING_OF_CENTRAL];
	};
	
	__syncthreads();
	
	f64 ourAz, oppAz, prevAz, nextAz;
	f64_vec2 opppos, prevpos, nextpos;

	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.
	

	if (threadIdx.x < threadsPerTileMajor) {
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		f64_vec2 endpt0, endpt1;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.

		ourAz = shared_Az_verts[threadIdx.x];
		
		short iprev = tri_len - 1;

		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevAz = shared_Az[izTri[iprev] - StartMinor];
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		} else {
			prevAz = p_Az[izTri[iprev]];
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;
		
		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			oppAz = shared_Az[izTri[i] - StartMinor];
			opppos = shared_pos[izTri[i] - StartMinor];
		}	else {
			oppAz = p_Az[izTri[i]];
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		endpt0 = THIRD * (info.pos + opppos + prevpos);

		short inext, iend = tri_len;
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
			iend = tri_len - 2; // We ignore frills -- how is this okay?
			
			// It's OK for flat BC - just ignore frills always
#ifndef FLATAZBC
			printf("not ok\n");
#endif			
			if (info.flag == OUTERMOST) {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
			} else {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			};
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};
		
		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);
		
		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;
			
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextAz = shared_Az[izTri[inext] - StartMinor];
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextAz = p_Az[izTri[inext]];
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			
			endpt1 = THIRD * (nextpos + info.pos + opppos);
			f64_vec2 edge_normal, integ_grad_Az;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-
			
			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			integralLapVT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			if (TESTLAP) printf("iVertex %d izTri[%d] %d ourAz %1.8E oppAz %1.8E contrib %1.14E "
						"grad Az %1.9E %1.9E\n"
				"pos %1.9E %1.9E prevpos %1.9E %1.9E opppos %1.9E %1.9E nextpost %1.9E %1.9E\n",
						iVertex, i, izTri[i], 						
						ourAz, oppAz,												
						integ_grad_Az.dot(edge_normal) / area_quadrilateral,
						integ_grad_Az.x / area_quadrilateral, 
						integ_grad_Az.y / area_quadrilateral,
					info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y
				);
			
			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			++iprev;
		}; // next i
		
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {

			// Now add on the final sides to give area:

			//    3     4
			//     2 1 0
			// endpt0=endpt1 is now the point north of edge facing 2 anyway.

			f64_vec2 projendpt1;
			if (info.flag == OUTERMOST) {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_OUTER_RADIUS_d);
			} else {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			};

			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;
			
			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;

			// line between out-projected points

			// Not sure this AreaMinor is the right one. 
			// But this points up why CG doesn't roll properly. The presence of the AreaMinor factor makes
			// the equations not symmetric.

			// NEW ADDITION:
#ifdef RADIALDECLINEAZBC

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / FRILL_CENTROID_OUTER_RADIUS_d;
			}
#endif
#ifdef DIRICHLETAZBC

			// In this case let the value beyond OUTERMOST be taken as 0 for now;
			// we can then substitute the value of the 1D radial array if we can use that.

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / (2.0*(FRILL_CENTROID_OUTER_RADIUS_d-info.pos.modulus()));
			}
#endif
			// if it's flat do nothing
		};

		p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor; // reset just because otherwise we're inconsistent about area/position in a subcycle
		
	}; // was thread in the first half of the block

	info = p_info[iMinor];
	ourAz = shared_Az[threadIdx.x];
	
	long izNeighMinor[6];
	char szPBC[6];

	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		
		// izNeighMinor[0] is actually vertex 0 if you are triangle 0.
		// Rethink:  
		// Try izNeighMinor[3] because this is meant to be neighbour 0.
		// Why are we doing all this? Just set = 0 out here.

		p_LapAz[iMinor] = 0.0;
	} else {
	
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevAz = p_Az[izNeighMinor[iprev]];
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[i] - StartMinor];
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				oppAz = p_Az[izNeighMinor[i]];
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

#pragma unroll 

		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;
	
			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextAz = p_Az[izNeighMinor[inext]];
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			f64_vec2 integ_grad_Az;

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
			
			f64_vec2 edge_normal;
			edge_normal.x = THIRD*(nextpos.y - prevpos.y);
			edge_normal.y = THIRD*(prevpos.x - nextpos.x);

			// This shouldn't be necessary anyway but is especially no good if it's not meant to be flat
#ifndef FLATAZBC
			printf("This bit needs to change.\n");
#endif

			if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
				(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
			{
				// neighbour's not a frill
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				
				integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				if (izNeighMinor[i] >= BEGINNING_OF_CENTRAL) {
					integralLapTV[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				} else {
					integralLapTT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				};

				if (TESTTRI3) {
					printf("iMinor %d i %d izNeighMinor[i] %d ourAz %1.9E theirs %1.9E contrib %1.12E \n"
						"ourpos %1.9E %1.9E prev %1.9E %1.9E out %1.9E %1.9E nex %1.9E %1.9E"
						"PBC %d \n",
						iMinor, i, izNeighMinor[i], ourAz, oppAz,
						integ_grad_Az.dot(edge_normal) / area_quadrilateral,
						info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
						szPBC[i]);
				};
			};
			AreaMinor += SIXTH*((prevpos.x + info.pos.x + opppos.x) +
				(nextpos.x + info.pos.x + opppos.x))*edge_normal.x;
			
			prevAz = oppAz;
			oppAz = nextAz;
			prevpos = opppos;
			opppos = nextpos;
		};
		
		p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iMinor] = AreaMinor; // reset for each substep	
		
	};

	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + k];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + k];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + k];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + s - 1];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + s - 1];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + s - 1];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_integralLapAz[blockIdx.x] = integralLapAz[0];
		p_integralVT[blockIdx.x] = integralLapVT[0];
		p_integralTV[blockIdx.x] = integralLapTV[0];
		p_integralTT[blockIdx.x] = integralLapTT[0];
	}
}
*/
/*
__global__ void kernelGetLap_minor__sum_placecontribs(

	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapAz,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_integralLapAz,
	f64 * __restrict__ p_integralVT,
	f64 * __restrict__ p_integralTV,
	f64 * __restrict__ p_integralTT,
	f64 * __restrict__ p_contriblist
)
{

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];

	__shared__ f64 integralLapAz[threadsPerTileMinor];
	__shared__ f64 integralLapVT[threadsPerTileMinor];
	__shared__ f64 integralLapTV[threadsPerTileMinor];
	__shared__ f64 integralLapTT[threadsPerTileMinor];

	//	__shared__ f64 sum1[threadsPerTileMinor];
	//	__shared__ f64 sum2[threadsPerTileMinor];
	//	__shared__ f64 sum3[threadsPerTileMinor];

	// 4.5 per thread.
	// Not clear if better off with L1 or shared mem in this case?? Probably shared mem.

	// For now, stick with idea that vertices have just major indices that come after tris.

	// Minor indices are not made contiguous - although it might be better ultimately.
	integralLapAz[threadIdx.x] = 0.0;
	integralLapVT[threadIdx.x] = 0.0;
	integralLapTV[threadIdx.x] = 0.0;
	integralLapTT[threadIdx.x] = 0.0;

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	long izneighminorneigh[6];

	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	shared_Az[threadIdx.x] = p_Az[iMinor];
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Az_verts[threadIdx.x] = p_Az[iVertex + BEGINNING_OF_CENTRAL];
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	f64_vec2 opppos, prevpos, nextpos;

	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.


	if (threadIdx.x < threadsPerTileMajor) {
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		f64_vec2 endpt0, endpt1;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.

		ourAz = shared_Az_verts[threadIdx.x];

		short iprev = tri_len - 1;

		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevAz = shared_Az[izTri[iprev] - StartMinor];
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		}
		else {
			prevAz = p_Az[izTri[iprev]];
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			oppAz = shared_Az[izTri[i] - StartMinor];
			opppos = shared_pos[izTri[i] - StartMinor];
		}
		else {
			oppAz = p_Az[izTri[i]];
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		endpt0 = THIRD * (info.pos + opppos + prevpos);

		short inext, iend = tri_len;
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
			iend = tri_len - 2; // We ignore frills -- how is this okay?

								// It's OK for flat BC - just ignore frills always
#ifndef FLATAZBC
			printf("not ok\n");
#endif			
			if (info.flag == OUTERMOST) {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
			}
			else {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			};
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};

		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);

		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextAz = shared_Az[izTri[inext] - StartMinor];
				nextpos = shared_pos[izTri[inext] - StartMinor];
			}
			else {
				nextAz = p_Az[izTri[inext]];
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			endpt1 = THIRD * (nextpos + info.pos + opppos);
			f64_vec2 edge_normal, integ_grad_Az;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			integralLapVT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			memcpy(izneighminorneigh, p_izNeighMinor + 6 * izTri[i], sizeof(long) * 6);
			int j = 0;
			while ((j < 6) && (izneighminorneigh[j] != iVertex + BEGINNING_OF_CENTRAL)) j++;
			if (j == 6) {
				printf("ERROR ERROR ERROR %d %d \n", iVertex, izTri[i]);
			}
			else {
				p_contriblist[izTri[i] * 6 + j] = integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			};

			if (TESTLAP) printf("iVertex %d izTri[%d] %d ourAz %1.8E oppAz %1.8E contrib %1.14E "
				"grad Az %1.9E %1.9E \n",
				iVertex, i, izTri[i],
				ourAz, oppAz,
				integ_grad_Az.dot(edge_normal) / area_quadrilateral,
				integ_grad_Az.x / area_quadrilateral,
				integ_grad_Az.y / area_quadrilateral);

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			++iprev;
		}; // next i

		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {

			// Now add on the final sides to give area:

			//    3     4
			//     2 1 0
			// endpt0=endpt1 is now the point north of edge facing 2 anyway.

			f64_vec2 projendpt1;
			if (info.flag == OUTERMOST) {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_OUTER_RADIUS_d);
			}
			else {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			};

			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;

			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;

			// line between out-projected points

			// Not sure this AreaMinor is the right one. 
			// But this points up why CG doesn't roll properly. The presence of the AreaMinor factor makes
			// the equations not symmetric.

			// NEW ADDITION:
#ifdef RADIALDECLINEAZBC

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / FRILL_CENTROID_OUTER_RADIUS_d;
			}
#endif
#ifdef DIRICHLETAZBC

			// In this case let the value beyond OUTERMOST be taken as 0 for now;
			// we can then substitute the value of the 1D radial array if we can use that.

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / (2.0*(FRILL_CENTROID_OUTER_RADIUS_d - info.pos.modulus()));
			}
#endif
			// if it's flat do nothing
		};

		p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor; // reset just because otherwise we're inconsistent about area/position in a subcycle

	}; // was thread in the first half of the block

	info = p_info[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];

	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// izNeighMinor[0] is actually vertex 0 if you are triangle 0.
		// Rethink:  
		// Try izNeighMinor[3] because this is meant to be neighbour 0.
		// Why are we doing all this? Just set = 0 out here.

		p_LapAz[iMinor] = 0.0;
	}
	else {

		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevAz = p_Az[izNeighMinor[iprev]];
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[i] - StartMinor];
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				oppAz = p_Az[izNeighMinor[i]];
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

#pragma unroll 

		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextAz = p_Az[izNeighMinor[inext]];
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			f64_vec2 integ_grad_Az;

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;

			f64_vec2 edge_normal;
			edge_normal.x = THIRD*(nextpos.y - prevpos.y);
			edge_normal.y = THIRD*(prevpos.x - nextpos.x);

			// This shouldn't be necessary anyway but is especially no good if it's not meant to be flat
#ifndef FLATAZBC
			printf("This bit needs to change.\n");
#endif

			if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
				(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
			{
				// neighbour's not a frill
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				if (izNeighMinor[i] >= BEGINNING_OF_CENTRAL) {
					integralLapTV[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				}
				else {
					integralLapTT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				};

				if (TESTTRI3) {
					printf("iMinor %d izNeighMinor[i] %d ourAz %1.9E theirs %1.9E contrib %1.12E \n",
						iMinor, izNeighMinor[i], ourAz, oppAz,
						integ_grad_Az.dot(edge_normal) / area_quadrilateral);
				};
			};
			AreaMinor += SIXTH*((prevpos.x + info.pos.x + opppos.x) +
				(nextpos.x + info.pos.x + opppos.x))*edge_normal.x;

			prevAz = oppAz;
			oppAz = nextAz;
			prevpos = opppos;
			opppos = nextpos;
		};

		p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iMinor] = AreaMinor; // reset for each substep	

	};

	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + k];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + k];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + k];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + s - 1];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + s - 1];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + s - 1];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_integralLapAz[blockIdx.x] = integralLapAz[0];
		p_integralVT[blockIdx.x] = integralLapVT[0];
		p_integralTV[blockIdx.x] = integralLapTV[0];
		p_integralTT[blockIdx.x] = integralLapTT[0];
	}
}*/
/*
__global__ void kernelGetLap_minor__sum_detectcontribs(

	structural * __restrict__ p_info,
	f64 * __restrict__ p_Az,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapAz,
	f64 * __restrict__ p_AreaMinor,
	f64 * __restrict__ p_integralLapAz,
	f64 * __restrict__ p_integralVT,
	f64 * __restrict__ p_integralTV,
	f64 * __restrict__ p_integralTT,
	f64 * __restrict__ p_contriblist
)
{

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];

	__shared__ f64 integralLapAz[threadsPerTileMinor];
	__shared__ f64 integralLapVT[threadsPerTileMinor];
	__shared__ f64 integralLapTV[threadsPerTileMinor];
	__shared__ f64 integralLapTT[threadsPerTileMinor];

	//	__shared__ f64 sum1[threadsPerTileMinor];
	//	__shared__ f64 sum2[threadsPerTileMinor];
	//	__shared__ f64 sum3[threadsPerTileMinor];

	// 4.5 per thread.
	// Not clear if better off with L1 or shared mem in this case?? Probably shared mem.

	// For now, stick with idea that vertices have just major indices that come after tris.

	// Minor indices are not made contiguous - although it might be better ultimately.
	integralLapAz[threadIdx.x] = 0.0;
	integralLapVT[threadIdx.x] = 0.0;
	integralLapTV[threadIdx.x] = 0.0;
	integralLapTT[threadIdx.x] = 0.0;

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;
	
	long izneighminorneigh[6];

	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	shared_Az[threadIdx.x] = p_Az[iMinor];
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Az_verts[threadIdx.x] = p_Az[iVertex + BEGINNING_OF_CENTRAL];
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	f64_vec2 opppos, prevpos, nextpos;

	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.


	if (threadIdx.x < threadsPerTileMajor) {
		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		f64_vec2 endpt0, endpt1;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.

		ourAz = shared_Az_verts[threadIdx.x];

		short iprev = tri_len - 1;

		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevAz = shared_Az[izTri[iprev] - StartMinor];
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		}
		else {
			prevAz = p_Az[izTri[iprev]];
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			oppAz = shared_Az[izTri[i] - StartMinor];
			opppos = shared_pos[izTri[i] - StartMinor];
		}
		else {
			oppAz = p_Az[izTri[i]];
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		endpt0 = THIRD * (info.pos + opppos + prevpos);

		short inext, iend = tri_len;
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
			iend = tri_len - 2; // We ignore frills -- how is this okay?

								// It's OK for flat BC - just ignore frills always
#ifndef FLATAZBC
			printf("not ok\n");
#endif			
			if (info.flag == OUTERMOST) {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
			}
			else {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			};
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};

		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);

		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextAz = shared_Az[izTri[inext] - StartMinor];
				nextpos = shared_pos[izTri[inext] - StartMinor];
			}
			else {
				nextAz = p_Az[izTri[inext]];
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			endpt1 = THIRD * (nextpos + info.pos + opppos);
			f64_vec2 edge_normal, integ_grad_Az;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			integralLapVT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			
			if (TESTLAP) printf("iVertex %d izTri[%d] %d ourAz %1.8E oppAz %1.8E contrib %1.14E "
				"grad Az %1.9E %1.9E \n",
				iVertex, i, izTri[i],
				ourAz, oppAz,
				integ_grad_Az.dot(edge_normal) / area_quadrilateral,
				integ_grad_Az.x / area_quadrilateral,
				integ_grad_Az.y / area_quadrilateral);

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevAz = oppAz;
			oppAz = nextAz;

			++iprev;
		}; // next i

		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {

			// Now add on the final sides to give area:

			//    3     4
			//     2 1 0
			// endpt0=endpt1 is now the point north of edge facing 2 anyway.

			f64_vec2 projendpt1;
			if (info.flag == OUTERMOST) {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_OUTER_RADIUS_d);
			}
			else {
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			};

			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;

			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;

			// line between out-projected points

			// Not sure this AreaMinor is the right one. 
			// But this points up why CG doesn't roll properly. The presence of the AreaMinor factor makes
			// the equations not symmetric.

			// NEW ADDITION:
#ifdef RADIALDECLINEAZBC

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / FRILL_CENTROID_OUTER_RADIUS_d;
			}
#endif
#ifdef DIRICHLETAZBC

			// In this case let the value beyond OUTERMOST be taken as 0 for now;
			// we can then substitute the value of the 1D radial array if we can use that.

			if (info.flag == OUTERMOST)
			{
				// set outer value at Az = (r_here/r_outer)*Az_self
				// contrib to integral of Lap is [delta_edge * dAz/dr] = 
				// delta_edge*Az_self * (r here - r outer)/(r_outer^2 - r_outer*r_here)
				// = delta_edge*Az_self * (-1.0)/(r_outer)

				f64 delta_edge = (projendpt0 - projendpt1).modulus();
				Our_integral_Lap_Az -= delta_edge*ourAz / (2.0*(FRILL_CENTROID_OUTER_RADIUS_d - info.pos.modulus()));
			}
#endif
			// if it's flat do nothing
		};

		p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor; // reset just because otherwise we're inconsistent about area/position in a subcycle

	}; // was thread in the first half of the block

	info = p_info[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];

	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// izNeighMinor[0] is actually vertex 0 if you are triangle 0.
		// Rethink:  
		// Try izNeighMinor[3] because this is meant to be neighbour 0.
		// Why are we doing all this? Just set = 0 out here.

		p_LapAz[iMinor] = 0.0;
	}
	else {

		f64 Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevAz = p_Az[izNeighMinor[iprev]];
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[i] - StartMinor];
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				oppAz = p_Az[izNeighMinor[i]];
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

#pragma unroll 

		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextAz = p_Az[izNeighMinor[inext]];
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			f64_vec2 integ_grad_Az;

			integ_grad_Az.x = 0.5*(
				(ourAz + nextAz)*(info.pos.y - nextpos.y)
				+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(ourAz + nextAz)*(info.pos.x - nextpos.x)
				+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;

			f64_vec2 edge_normal;
			edge_normal.x = THIRD*(nextpos.y - prevpos.y);
			edge_normal.y = THIRD*(prevpos.x - nextpos.x);

			// This shouldn't be necessary anyway but is especially no good if it's not meant to be flat
#ifndef FLATAZBC
			printf("This bit needs to change.\n");
#endif

			if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
				(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
			{
				// neighbour's not a frill
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				integralLapAz[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				if (izNeighMinor[i] >= BEGINNING_OF_CENTRAL) {
					integralLapTV[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

					f64 contrib = integ_grad_Az.dot(edge_normal) / area_quadrilateral;
					f64 alleged = p_contriblist[iMinor * 6 + i];
					f64 sum = (alleged + contrib);
					if (fabs(sum) > 1.0e-4) printf("%d from %d : contrib %1.14E alleged %1.14E sum %1.8E\n",
						iMinor, izNeighMinor[i], contrib, alleged, sum);


				}
				else {
					integralLapTT[threadIdx.x] += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
				};

				if (TESTTRI3) {
					printf("iMinor %d izNeighMinor[i] %d ourAz %1.9E theirs %1.9E contrib %1.12E \n",
						iMinor, izNeighMinor[i], ourAz, oppAz,
						integ_grad_Az.dot(edge_normal) / area_quadrilateral);
				};
			};
			AreaMinor += SIXTH*((prevpos.x + info.pos.x + opppos.x) +
				(nextpos.x + info.pos.x + opppos.x))*edge_normal.x;

			prevAz = oppAz;
			oppAz = nextAz;
			prevpos = opppos;
			opppos = nextpos;
		};

		p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
		p_AreaMinor[iMinor] = AreaMinor; // reset for each substep	

	};

	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + k];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + k];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + k];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + k];
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			integralLapAz[threadIdx.x] += integralLapAz[threadIdx.x + s - 1];
			integralLapVT[threadIdx.x] += integralLapVT[threadIdx.x + s - 1];
			integralLapTV[threadIdx.x] += integralLapTV[threadIdx.x + s - 1];
			integralLapTT[threadIdx.x] += integralLapTT[threadIdx.x + s - 1];
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_integralLapAz[blockIdx.x] = integralLapAz[0];
		p_integralVT[blockIdx.x] = integralLapVT[0];
		p_integralTV[blockIdx.x] = integralLapTV[0];
		p_integralTT[blockIdx.x] = integralLapTT[0];
	}
}*/

__global__ void kernelComputeJacobianValues(
	structural * __restrict__ p_info,
//	f64 * __restrict__ p_Aznext,
//	f64 * __restrict__ p_Azk,
//	f64 * __restrict__ pAzdot0,
	f64 * __restrict__ pgamma,
	f64 const h_use,
	long * __restrict__ p_indic,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_Jacobianesque_list)
{

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];  // 16K for these - plentiful
//	__shared__ f64 shared_Az[threadsPerTileMinor];
//	__shared__ f64 shared_Az_verts[threadsPerTileMajor];    // 4.5 things
	
	f64 d_eps_by_dbeta_j[SQUASH_POINTS];  // 24 max

	// __shared__ f64 d_eps_by_dbeta_j_verts[SQUASH_POINTS*threadsPerTileMajor];

	// need to acquire sums of products of these so need 1 for every tri and vertex
	// ...
	// would have been better off by far not trying to preserve data, but simply splitting into 2 routines.

	// Maybe we should be accumulating in-between instead. Better.
		
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info[iMinor].pos;
	structural info;

	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
	};
	
	__syncthreads();
	
	f64_vec2 opppos, prevpos, nextpos;
	int iWhich, j;
	int iWhichPrev, iWhichSelf, iWhichNext, iWhichOpp;

	if (threadIdx.x < threadsPerTileMajor) {
		iWhichSelf = p_indic[iVertex + BEGINNING_OF_CENTRAL];

		memset(d_eps_by_dbeta_j, 0, sizeof(f64)*SQUASH_POINTS);

		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;
		f64_vec2 endpt0, endpt1;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);

		// Is this best way? better than going looking for periodic data on each tri.
//		ourAz = shared_Az_verts[threadIdx.x];
		short iprev = tri_len - 1;

		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		} else {
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;
		iWhichPrev = p_indic[izTri[iprev]];

		short i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			opppos = shared_pos[izTri[i] - StartMinor];
		} else {
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
		iWhichOpp = p_indic[izTri[i]];

		// Handle case that prev is a frill. What to do then?
		// Not sure which way numbers go.But either way if prev is a frill then it's our 0th tri that is the governor.
		f64 prevfactor = 1.0;
		f64 nextfactor;
		if ((info.flag == INNERMOST) &&
			(prevpos.dot(prevpos) < 1.0000001*1.0000001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
		{
			iWhichPrev = iWhichOpp;
		};
		if ((info.flag == OUTERMOST) && (prevpos.dot(prevpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			&& (RADIALDECLINE))
		{
			iWhichPrev = iWhichOpp;
			prevfactor = (opppos.modulus() / prevpos.modulus());
		};


		f64_vec2 store_centroid = opppos;

		endpt0 = THIRD * (info.pos + opppos + prevpos);

		f64_vec2 store_first_point = endpt0;

		short inext, iend = tri_len;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) 
			iend = tri_len - 2;
		
		f64_vec2 projendpt0, edge_normal;
		if ((info.flag == INNERMOST)) {
			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};
		if (iend > MAXNEIGH) printf("####################\nvertex %d iend = %d info.neigh_len = %d\n", iVertex, iend, info.neigh_len);
		
		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;		
			iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			iWhichNext = p_indic[izTri[inext]];

			nextfactor = 1.0;
			if ((info.flag == INNERMOST) &&
				(nextpos.dot(nextpos) < 1.0000001*1.0000001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
			{
				iWhichNext = iWhichOpp;
			};
			if ((info.flag == OUTERMOST) && 
				(nextpos.dot(nextpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
				&& (RADIALDECLINE))
			{
				iWhichNext = iWhichOpp;
				nextfactor = (opppos.modulus() / nextpos.modulus());
			};

			endpt1 = THIRD * (nextpos + info.pos + opppos);
			f64_vec2 edge_normal;
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-

			//integ_grad_Az.x = 0.5*(
			//	(ourAz + nextAz)*(info.pos.y - nextpos.y)
			//	+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
			//	+ (oppAz + prevAz)*(opppos.y - prevpos.y)
			//	+ (nextAz + oppAz)*(nextpos.y - opppos.y)
			//	);

			//integ_grad_Az.y = -0.5*( // notice minus
			//	(ourAz + nextAz)*(info.pos.x - nextpos.x)
			//	+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
			//	+ (oppAz + prevAz)*(opppos.x - prevpos.x)
			//	+ (nextAz + oppAz)*(nextpos.x - opppos.x)
			//	);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
	//		Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			if (iWhichPrev > 0) {
				d_eps_by_dbeta_j[iWhichPrev-1] += prevfactor*0.5*((opppos.y - info.pos.y)*edge_normal.x
					- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				// Just add unnomralized here				
			}
			if (iWhichOpp > 0) {
				d_eps_by_dbeta_j[iWhichOpp-1] += 0.5*((nextpos.y-prevpos.y)*edge_normal.x
					- (nextpos.x-prevpos.x)*edge_normal.y) / area_quadrilateral;
				// Just add unnomralized here
			}
			if (iWhichNext > 0) {
				d_eps_by_dbeta_j[iWhichNext-1] += nextfactor*0.5*((info.pos.y-opppos.y)*edge_normal.x
					+ (opppos.x-info.pos.x)*edge_normal.y) / area_quadrilateral;
				// Just add unnomralized here
			}
			if (iWhichSelf > 0) {
				d_eps_by_dbeta_j[iWhichSelf - 1] += 0.5*((prevpos.y-nextpos.y)*edge_normal.x
					- (prevpos.x-nextpos.x)*edge_normal.y) / area_quadrilateral;
				// Just add unnomralized here
			}
			
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
						
			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			iWhichPrev = iWhichOpp;
			iWhichOpp = iWhichNext;
		}; // next i

		if (info.flag == INNERMOST) {
			// Now add on the final sides to give area:

			f64_vec2 projendpt1;
			endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;
			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
			// unchanged... check later
		}


		if (info.flag == OUTERMOST)
		{
			// 3 sides to add.

			//       3   4
			//     2       0
			//         1
			// endpt0=endpt1 is now the point north of edge facing 2.
			// opppos is centre of tri (3).

			f64 opp_prev = 0.0, next_ours = 0.0;
			if (RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, info.pos.modulus() + (FRILL_CENTROID_OUTER_RADIUS_d - info.pos.modulus())*1.16);
				endpt1 = THIRD*(opppos + info.pos + nextpos);
				opp_prev = (prevpos.modulus() / opppos.modulus());
				next_ours = (info.pos.modulus() / nextpos.modulus());
			};

			if (!RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, FRILL_CENTROID_OUTER_RADIUS_d);
				endpt1 = THIRD*(opppos + info.pos + nextpos);
				// map radially inwards so that radius is halfway out to the zero arc:
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};
			
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			
			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			
			if (DIRICHLET || RADIALDECLINE) {
				// iWhichPrev is already set to previous iWhichOpp.
				// hold on to it
				if (iWhichPrev > 0) {
					d_eps_by_dbeta_j[iWhichPrev - 1]
						+= (0.5*((opppos.y - info.pos.y)*edge_normal.x
						- (opppos.x - info.pos.x)*edge_normal.y)
						+ 0.5*opp_prev*(
							(nextpos.y-prevpos.y)*edge_normal.x
							-(nextpos.x-prevpos.x)*edge_normal.y
								)
							)/ area_quadrilateral;
				}

				//iWhichSelf = p_indic[iVertex + BEGINNING_OF_CENTRAL]; // already set.
				if (iWhichSelf > 0) {
					d_eps_by_dbeta_j[iWhichSelf - 1] += (0.5*((prevpos.y - nextpos.y)*edge_normal.x
						- (prevpos.x - nextpos.x)*edge_normal.y)
						+ 0.5*next_ours*(
							(info.pos.y-opppos.y)*edge_normal.x
							- (info.pos.x-opppos.x)*edge_normal.y
							)						
						)/ area_quadrilateral;
				}
			};

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;

			// NOW WE ARE GOING TO LOOK OUTWARDS
			// iprev IS NOT UPDATED
			
			inext = tri_len - 1;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			
			f64 next_0 = 0.0;
			endpt1 = THIRD*(opppos + info.pos + nextpos);
			if (RADIALDECLINE) {
				next_0 = (store_centroid.modulus() / nextpos.modulus());
			}
			if (!RADIALDECLINE) {
				// map radially inwards so that radius is halfway out to the zero arc:
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};
//
//			 So now ... opp_prev is still there and must attribute the prev coefficient
//			 to the prev-1 index
//			
//			 next_ours is now opp, and must attribute the opp coefficient to ourselves
//			
//			 and the next coefficient now applies for index 0 with coeff next_0


			// We have not updated iprev.
			
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			// WE ARE NOW LOOKING DIRECTLY OUTWARDS.
			// "iprev" is now the previous one to prev and still relevant in case of radial decline
			// next_ours is now for the opposite one
			// next_0 relates the next position to the effect of the 0th value.

			if (RADIALDECLINE) {
				if (iWhichPrev > 0) {
					d_eps_by_dbeta_j[iWhichPrev - 1] += 
						0.5*opp_prev*((opppos.y - info.pos.y)*edge_normal.x
						- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				};
				if (iWhichSelf > 0) {
					d_eps_by_dbeta_j[iWhichSelf - 1] += 
						0.5*(1.0-next_ours)*((prevpos.y - nextpos.y)*edge_normal.x
						- (prevpos.x - nextpos.x)*edge_normal.y) / area_quadrilateral;
				};
				iWhichNext = p_indic[izTri[0]];
				if (iWhichNext > 0) {
					d_eps_by_dbeta_j[iWhichNext - 1] +=
						0.5*next_0*((info.pos.y - opppos.y)*edge_normal.x
							- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				};
			};

			if (DIRICHLET) {
				// May be nonsense.

				iWhich = p_indic[izTri[iprev]]; //frill!
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((opppos.y - info.pos.y)*edge_normal.x
						- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				};
				iWhich = p_indic[izTri[inext]]; // frill!
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((info.pos.y - opppos.y)*edge_normal.x
						+ (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				};
				iWhich = p_indic[iVertex + BEGINNING_OF_CENTRAL];
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((prevpos.y - nextpos.y)*edge_normal.x
						- (prevpos.x - nextpos.x)*edge_normal.y) / area_quadrilateral;
				};
			};
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			// That was the side looking out.

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			// WE ARE GOING TO LOOK NORTHEAST

			endpt1 = store_first_point;
			nextpos = p_info[izTri[0]].pos;

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (RADIALDECLINE) {
				// prevAz = next_ours* self [iWhichSelf]
				// oppAz = next_0 * 0th value [iWhichNext]
				// nextAz = 0th value [iWhichNext]
				if (iWhichSelf > 0) {
					d_eps_by_dbeta_j[iWhichSelf - 1] += 
						0.5*next_ours*((opppos.y - info.pos.y)*edge_normal.x
						- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				}
				if (iWhichNext > 0) {
					d_eps_by_dbeta_j[iWhichNext - 1] +=
						0.5*(next_0*((nextpos.y - prevpos.y)*edge_normal.x
							- (nextpos.x - prevpos.x)*edge_normal.y)
									+
						     (info.pos.y-opppos.y)*edge_normal.x
							- (opppos.y-info.pos.y)*edge_normal.y
							) / area_quadrilateral;
				}

			}
			if (DIRICHLET) {
				iWhich = p_indic[izTri[i]]; // frill!
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((nextpos.y - prevpos.y)*edge_normal.x
						- (nextpos.x - prevpos.x)*edge_normal.y) / area_quadrilateral;
				}
				iWhich = p_indic[izTri[inext]]; 
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((info.pos.y - opppos.y)*edge_normal.x
						+ (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				};
				iWhich = p_indic[iVertex + BEGINNING_OF_CENTRAL];
				if (iWhich > 0) {
					d_eps_by_dbeta_j[iWhich - 1] += 0.5*((prevpos.y - nextpos.y)*edge_normal.x
						- (prevpos.x - nextpos.x)*edge_normal.y) / area_quadrilateral;
				};
			}; 
			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
		};
		 
		f64 gamma = pgamma[iVertex + BEGINNING_OF_CENTRAL];
		for (iWhich = 0; iWhich < SQUASH_POINTS; iWhich++)
			d_eps_by_dbeta_j[iWhich] *= -h_use*gamma/AreaMinor;
		
		// d eps_i / d x_j =  [i==j]*1 - h gamma d[Lap here]/dx_j
		if (p_indic[iVertex + BEGINNING_OF_CENTRAL] > 0) d_eps_by_dbeta_j[p_indic[iVertex + BEGINNING_OF_CENTRAL] - 1] += 1.0;
		// p_indic[iVertex + BEGINNING_OF_CENTRAL]-1 is the number of its volley. Stupid system.

		// For simplicity let's say we save off into global memory.
		memcpy(&(p_Jacobianesque_list[(iVertex + BEGINNING_OF_CENTRAL)*SQUASH_POINTS]),
			d_eps_by_dbeta_j, sizeof(f64)*SQUASH_POINTS); // d eps_i / dbeta_j

	//	if (iVertex + BEGINNING_OF_CENTRAL == MyMaxIndex) {
	//		for (j = 0; j < SQUASH_POINTS; j++)
	//			printf("%d : coeff %d : %1.9E \n", iVertex + BEGINNING_OF_CENTRAL, j,
	//				d_eps_by_dbeta_j[j]);
	//	}

	}; // was thread in the first half of the block
	
	
	memset(d_eps_by_dbeta_j, 0, sizeof(f64)*SQUASH_POINTS);

	f64 prevfactor, oppfactor, nextfactor;
	
	info = p_info[iMinor];
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) return;
	
	iWhichSelf = p_indic[iMinor];
	//	p_LapAz[iMinor] = 0.0;
	// } else {
		f64 AreaMinor = 0.0;
		short inext, i = 0, iprev = 5;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		} else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			} else {
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;
		
		if ((prevpos.dot(prevpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			&& (RADIALDECLINE))
		{
			// outer frill under radial decline:
			//prevAz = ourAz*(info.pos.modulus() / nextpos.modulus());

			// do this by resetting iWhichPrev and a factor. !!
			iWhichPrev = iWhichSelf;
			prevfactor = (info.pos.modulus() / nextpos.modulus());
		} else {
			iWhichPrev = p_indic[izNeighMinor[iprev]];
			prevfactor = 1.0;
		}

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		} else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			} else {
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
		if ((opppos.dot(opppos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			&& (RADIALDECLINE))
		{
			// outer frill			
			//oppAz = ourAz*(info.pos.modulus() / nextpos.modulus());
			iWhichOpp = iWhichSelf;
			oppfactor = (info.pos.modulus() / nextpos.modulus());
		} else {
			iWhichOpp = p_indic[izNeighMinor[i]];
			oppfactor = 1.0;
		}
#pragma unroll 
		for (i = 0; i < 6; i++)
		{
			
			inext = i + 1; if (inext > 5) inext = 0;
			iprev = i - 1; if (iprev < 0) iprev = 5;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			} else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				} else {
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			if ((nextpos.dot(nextpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
				&& (RADIALDECLINE))
			{
				// outer frill
				//nextAz = ourAz*(info.pos.modulus() / nextpos.modulus());
				iWhichNext = iWhichSelf;
				nextfactor = (info.pos.modulus() / nextpos.modulus());
			} else {
				iWhichNext = p_indic[izNeighMinor[inext]];
				nextfactor = 1.0;
			};
			// ______________________________________________________-

			//f64_vec2 integ_grad_Az;
			//integ_grad_Az.x = 0.5*(
			//	(ourAz + nextAz)*(info.pos.y - nextpos.y)
			//	+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
			//	+ (oppAz + prevAz)*(opppos.y - prevpos.y)
			//	+ (nextAz + oppAz)*(nextpos.y - opppos.y)
			//	);
			//integ_grad_Az.y = -0.5*( // notice minus
			//	(ourAz + nextAz)*(info.pos.x - nextpos.x)
			//	+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
			//	+ (oppAz + prevAz)*(opppos.x - prevpos.x)
			//	+ (nextAz + oppAz)*(nextpos.x - opppos.x)
			//	);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			f64_vec2 edge_normal;
			edge_normal.x = THIRD*(nextpos.y - prevpos.y);
			edge_normal.y = THIRD*(prevpos.x - nextpos.x);
			
			if (((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) 
				|| (DIRICHLET) || (RADIALDECLINE))
				&&
				(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
			{
			//	Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
		
				if (iWhichPrev > 0) {
					d_eps_by_dbeta_j[iWhichPrev - 1] += 0.5*((opppos.y - info.pos.y)*edge_normal.x
						- (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
				}
				if (iWhichOpp > 0) {
					d_eps_by_dbeta_j[iWhichOpp - 1] += 0.5*((nextpos.y - prevpos.y)*edge_normal.x
						- (nextpos.x - prevpos.x)*edge_normal.y) / area_quadrilateral;
				}
				if (iWhichNext > 0) {
					d_eps_by_dbeta_j[iWhichNext - 1] += 0.5*((info.pos.y - opppos.y)*edge_normal.x
								+ (opppos.x - info.pos.x)*edge_normal.y) / area_quadrilateral;
					};									
				if (iWhichSelf > 0) {
					d_eps_by_dbeta_j[iWhichSelf - 1] += 0.5*((prevpos.y - nextpos.y)*edge_normal.x
						- (prevpos.x - nextpos.x)*edge_normal.y) / area_quadrilateral;
				}
			};
			AreaMinor += SIXTH*((prevpos.x + info.pos.x + opppos.x) +
				(nextpos.x + info.pos.x + opppos.x))*edge_normal.x;			
			prevpos = opppos;
			opppos = nextpos;
			iWhichPrev = iWhichOpp;
			prevfactor = oppfactor;
			iWhichOpp = iWhichNext;
			oppfactor = nextfactor;
		};

		f64 gamma = pgamma[iMinor];
		for (iWhich = 0; iWhich < SQUASH_POINTS; iWhich++)
			d_eps_by_dbeta_j[iWhich] *= -h_use*gamma / AreaMinor;

		// d eps_i / d x_j =  [i==j]*1 - h gamma d[Lap here]/dx_j
		//if (p_indic[iMinor] > SQUASH_POINTS) {
		//	printf("ERROR %d p_indic[iMinor] %d \n", iMinor, p_indic[iMinor]);
		//	// $$$$$$$$$$$
		//	//    DEBUG
		//	// $$$$$$$$$$$
		//} else {
		if (iWhichSelf > 0) d_eps_by_dbeta_j[iWhichSelf - 1] += 1.0;
		//}
		// p_indic[iMinor]-1 is the number of its volley. Stupid system.
		// For simplicity let's say we save off into global memory.

	//	if (p_indic[iMinor]>0) printf("indic %d found at %d; deps = %1.9E\n", p_indic[iMinor], iMinor,
	//		d_eps_by_dbeta_j[p_indic[iMinor] - 1]);
//
		memcpy(&(p_Jacobianesque_list[iMinor*SQUASH_POINTS]),d_eps_by_dbeta_j, 
			sizeof(f64)*SQUASH_POINTS); // d eps_i / dbeta_j

	//	if (iMinor == MyMaxIndex) {
	//		for (j = 0; j < SQUASH_POINTS; j++)
	//			printf("%d : coeff %d : %1.9E \n", iMinor, j, d_eps_by_dbeta_j[j]);
	//	};
	//};
	
}


__global__ void kernelGetLapCoeffs_and_min(
	structural * __restrict__ p_info,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor,
	f64 * __restrict__ p_LapCoeffSelf,
	f64 * __restrict__ p_min_array,
	long * __restrict__ p_min_index)
{
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	__shared__ char shared_flag[threadsPerTileMinor];

	__shared__ f64 mincoeffself[threadsPerTileMinor];
	__shared__ long iMin[threadsPerTileMinor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;


	structural info = p_info[iMinor];
	shared_pos[threadIdx.x] = info.pos;
	shared_flag[threadIdx.x] = info.flag;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
	};

	__syncthreads();

	f64_vec2 opppos, prevpos, nextpos;
	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.

	mincoeffself[threadIdx.x] = 0.0;
	iMin[threadIdx.x] = -1;

	if (threadIdx.x < threadsPerTileMajor) {

		f64 Our_integral_Lap_Az_contrib_from_own_Az = 0.0;
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + MAXNEIGH*iVertex, sizeof(long)*MAXNEIGH);
		memcpy(szPBC, p_szPBCtri_vertex + MAXNEIGH*iVertex, sizeof(char)*MAXNEIGH);
		// Is this best way? better than going looking for periodic data on each tri.

		short iprev = tri_len - 1;
		if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
		{
			prevpos = shared_pos[izTri[iprev] - StartMinor];
		} else {
			prevpos = p_info[izTri[iprev]].pos;
		}
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		short inext, i = 0;
		if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
		{
			opppos = shared_pos[izTri[i] - StartMinor];
		}
		else {
			opppos = p_info[izTri[i]].pos;
		}
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

		f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
		f64_vec2 endpt1, edge_normal;

		f64_vec2 store_first_point = endpt0;

		short iend = tri_len;
		f64_vec2 projendpt0;
		if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) iend = tri_len - 2;

		// Bear in mind for OUTERMOST, the triangles go clockwise not anticlockwise.

		if ((info.flag == INNERMOST)) {
			endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			edge_normal.x = endpt0.y - projendpt0.y;
			edge_normal.y = projendpt0.x - endpt0.x;
			AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
		};

		for (i = 0; i < iend; i++)
		{
			// Tri 0 is anticlockwise of neighbour 0, we think
			inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			} else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

			endpt1 = THIRD * (nextpos + info.pos + opppos);

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA
			f64_vec2 integ_grad_Az;
			integ_grad_Az.x = 0.5*(
				(1.0)*(info.pos.y - nextpos.y)
				+ (1.0)*(prevpos.y - info.pos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				(1.0)*(info.pos.x - nextpos.x)
				+ (1.0)*(prevpos.x - info.pos.x)
				);
			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);

			Our_integral_Lap_Az_contrib_from_own_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;


	//		if (iVertex + BEGINNING_OF_CENTRAL == CHOSEN) {
	//			printf("%d contrib %1.14E %d \nourpos %1.14E %1.14E opppos %1.14E %1.14E \n"
	//				"prevpos nextpos %1.14E %1.14E %1.14E %1.14E\n"
	//				"szPBC[i] %d area_quadrilateral %1.14E \n", 
	//				iVertex + BEGINNING_OF_CENTRAL, 
	//				integ_grad_Az.dot(edge_normal) / area_quadrilateral,
	//				izTri[i], 
	//				info.pos.x,info.pos.y,opppos.x,opppos.y,
	//				prevpos.x,prevpos.y,nextpos.x,nextpos.y,
	//				(int)szPBC[i],area_quadrilateral);				
	//		}

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			++iprev;
			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
		}; // next i

		if (info.flag == INNERMOST) {
			// Now add on the final sides to give area:

			f64_vec2 projendpt1;
			endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			edge_normal.x = projendpt1.y - endpt1.y;
			edge_normal.y = endpt1.x - projendpt1.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;
			edge_normal.x = projendpt0.y - projendpt1.y;
			edge_normal.y = projendpt1.x - projendpt0.x;
			AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
			// unchanged... check later
		}

		if (info.flag == OUTERMOST)
		{
			f64_vec2 integ_grad_Az;
			// 3 sides to add.

			//       3   4
			//     2       0
			//         1
			// endpt0=endpt1 is now the point north of edge facing 2.
			// opppos is centre of tri (3).
			f64 facc = 0.0;
			if (RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, info.pos.modulus() + (FRILL_CENTROID_OUTER_RADIUS_d - info.pos.modulus())*1.16);
				endpt1 = THIRD*(opppos + info.pos + nextpos);
				facc = (info.pos.modulus() / nextpos.modulus());
			}
			if (!RADIALDECLINE) {
				info.pos.project_to_radius(nextpos, FRILL_CENTROID_OUTER_RADIUS_d);
				endpt1 = THIRD*(opppos + info.pos + nextpos);

				// map radially inwards so that radius is halfway out to the zero arc:
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};
			
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				+ (1.0)*(prevpos.y - nextpos.y)
				+ facc*(info.pos.y-opppos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				+(1.0)*(prevpos.x - nextpos.x)
				+ facc*(info.pos.x - opppos.x)
				);
			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) Our_integral_Lap_Az_contrib_from_own_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;

			inext = tri_len - 1;
			if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
			{
				nextpos = shared_pos[izTri[inext] - StartMinor];
			}
			else {
				nextpos = p_info[izTri[inext]].pos;
			}
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			
			endpt1 = THIRD*(opppos + info.pos + nextpos);

			if (!RADIALDECLINE) {
				// map radially inwards so that radius is halfway out to the zero arc.
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);
			};
			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				 (1.0)*(prevpos.y - nextpos.y)
				+ facc*(nextpos.y-prevpos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				 (1.0)*(prevpos.x - nextpos.x)
				+ facc*(nextpos.x-prevpos.x)
				);
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) Our_integral_Lap_Az_contrib_from_own_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			// That was the side looking out.

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;

			endpt1 = store_first_point;
			nextpos = p_info[izTri[0]].pos;

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;
			// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			integ_grad_Az.x = 0.5*(
				(1.0)*(prevpos.y - nextpos.y)
				+ facc*(opppos.y - info.pos.y)
				);
			integ_grad_Az.y = -0.5*( // notice minus
				(1.0)*(prevpos.x - nextpos.x)
				+ facc*(opppos.x - info.pos.x)
				);
			area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			if (DIRICHLET || RADIALDECLINE) Our_integral_Lap_Az_contrib_from_own_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

		};

		p_LapCoeffSelf[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az_contrib_from_own_Az / AreaMinor;

		mincoeffself[threadIdx.x] = p_LapCoeffSelf[iVertex + BEGINNING_OF_CENTRAL];
		iMin[threadIdx.x] = iVertex + BEGINNING_OF_CENTRAL;
		// All vertices can count for this.

	}; // was thread in the first half of the block

	info = p_info[iMinor];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// Look at simulation.cpp
		// Treatment of FRILLS : 

		p_LapCoeffSelf[iMinor] = -1.0;
		// LapCoefftri[iMinor][3] = 1.0; // neighbour 0
	}
	else {

		f64 Our_integral_Lap_Az_contrib_from_own_Az = 0.0;
		f64 AreaMinor = 0.0;
		f64 prevfac = 0.0, nextfac = 0.0, oppfac = 0.0;

		short iprev = 5; short inext, i = 0;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevpos = p_info[izNeighMinor[iprev]].pos;
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

		if (prevpos.dot(prevpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
		{
			// outer frill
			if (RADIALDECLINE)
				prevfac = (info.pos.modulus() / prevpos.modulus());
		}
		
		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				opppos = p_info[izNeighMinor[i]].pos;
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
		if (opppos.dot(opppos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
		{
			// outer frill
			if (RADIALDECLINE)
				oppfac = (info.pos.modulus() / opppos.modulus());
		}

#pragma unroll 
		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;

			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextpos = p_info[izNeighMinor[inext]].pos;
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
			if (nextpos.dot(nextpos) > 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d)
			{
				// outer frill
				if (RADIALDECLINE)
					nextfac = (info.pos.modulus() / nextpos.modulus());
			}
			// New definition of endpoint of minor edge:
			f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;
			endpt0 = THIRD * (prevpos + info.pos + opppos);
			endpt1 = THIRD * (nextpos + info.pos + opppos);

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			// ______________________________________________________-

			//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
			//	Our_integral_grad_Az += Az_edge * edge_normal;
			//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
			//integ_grad_Az.x = 0.5*(
			//	(ourAz + nextAz)*(info.pos.y - nextpos.y)
			//	+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
			//	+ (oppAz + prevAz)*(opppos.y - prevpos.y)
			//	+ (nextAz + oppAz)*(nextpos.y - opppos.y)
			//	);
			//integ_grad_Az.y = -0.5*( // notice minus
			//	(ourAz + nextAz)*(info.pos.x - nextpos.x)
			//	+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
			//	+ (oppAz + prevAz)*(opppos.x - prevpos.x)
			//	+ (nextAz + oppAz)*(nextpos.x - opppos.x)
			//	);
			integ_grad_Az.x = 0.5*(prevpos.y - nextpos.y);

			integ_grad_Az.y = -0.5*(prevpos.x - nextpos.x);

			integ_grad_Az.x = 0.5*(
				(1.0 + nextfac)*(info.pos.y - nextpos.y)
				+ (prevfac + 1.0)*(prevpos.y - info.pos.y)
				+ (oppfac + prevfac)*(opppos.y - prevpos.y)
				+ (nextfac + oppfac)*(nextpos.y - opppos.y)
				);

			integ_grad_Az.y = -0.5*( // notice minus
				(1.0 + nextfac)*(info.pos.x - nextpos.x)
				+ (prevfac + 1.0)*(prevpos.x - info.pos.x)
				+ (oppfac + prevfac)*(opppos.x - prevpos.x)
				+ (nextfac + oppfac)*(nextpos.x - opppos.x)
				);

			f64 area_quadrilateral = 0.5*(
				(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				);
			
			if (
				((opppos.dot(opppos) < 0.99999*0.99999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) || (DIRICHLET)
					|| (RADIALDECLINE)) &&
				(opppos.dot(opppos) > 1.00001*1.00001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d)
				)
			//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				Our_integral_Lap_Az_contrib_from_own_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

			endpt0 = endpt1;
			prevpos = opppos;
			opppos = nextpos;
			prevfac = oppfac;
			oppfac = nextfac;
			iprev = i;
			// There is an even quicker way which is to rotate pointers. No memcpy needed.
		};

		p_LapCoeffSelf[iMinor] = Our_integral_Lap_Az_contrib_from_own_Az / AreaMinor;

		if (p_LapCoeffSelf[iMinor] < mincoeffself[threadIdx.x])
		{
			mincoeffself[threadIdx.x] = p_LapCoeffSelf[iMinor];
			iMin[threadIdx.x] = iMinor;
		};
	};

	__syncthreads();


	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			if (mincoeffself[threadIdx.x] > mincoeffself[threadIdx.x + k])
			{
				mincoeffself[threadIdx.x] = mincoeffself[threadIdx.x + k];
				iMin[threadIdx.x] = iMin[threadIdx.x + k];
			}			
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			
			if (mincoeffself[threadIdx.x] > mincoeffself[threadIdx.x + s-1])
			{
				mincoeffself[threadIdx.x] = mincoeffself[threadIdx.x + s-1];
				iMin[threadIdx.x] = iMin[threadIdx.x + s-1];
			}
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_min_array[blockIdx.x] = mincoeffself[threadIdx.x];
		p_min_index[blockIdx.x] = iMin[threadIdx.x];
	}	
}

/*
__global__ void kernelGetLapCoeffs_and_min_DEBUG(
	structural * __restrict__ p_info,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtri_vertex,
	char * __restrict__ p_szPBCtriminor, //B
	f64 * __restrict__ p_LapCoeffSelf,
	f64 * __restrict__ p_min_array,
	long * __restrict__ p_min_index) //B
{
	Note many changes since this was used; delete and go again.

	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	__shared__ char shared_flag[threadsPerTileMinor];

	__shared__ f64 mincoeffself[threadsPerTileMinor];
	__shared__ long iMin[threadsPerTileMinor]; // B

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	 // code A
	structural info = p_info[iMinor];
	shared_pos[threadIdx.x] = info.pos;
	shared_flag[threadIdx.x] = info.flag;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
	};

	__syncthreads();

	f64_vec2 opppos, prevpos, nextpos;
	// Better if we use same share to do both tris and verts
	// Idea: let's make it called for # minor threads, each loads 1 shared value,
	// and only half the threads run first for the vertex part. That is a pretty good idea.

	mincoeffself[threadIdx.x] = 0.0;
	iMin[threadIdx.x] = -1;
	

	if (threadIdx.x < threadsPerTileMajor) {

		f64 Our_integral_Lap_Az_contrib_from_own_Az = 0.0;
		f64 AreaMinor = 0.0;
		
		p_LapCoeffSelf[iVertex + BEGINNING_OF_CENTRAL] = 0.0;

		mincoeffself[threadIdx.x] = p_LapCoeffSelf[iVertex + BEGINNING_OF_CENTRAL];
		iMin[threadIdx.x] = iVertex + BEGINNING_OF_CENTRAL; // B

		// All vertices can count for this.

	}; // was thread in the first half of the block
	

	    // 2nd commenting
	info = p_info[iMinor];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {

		// Look at simulation.cpp
		// Treatment of FRILLS : 

		p_LapCoeffSelf[iMinor] = -1.0;
		// LapCoefftri[iMinor][3] = 1.0; // neighbour 0
	}
	else {

		f64 Our_integral_Lap_Az_contrib_from_own_Az = 0.0;
		f64 AreaMinor = 0.0;

		p_LapCoeffSelf[iMinor] = 0.0;

		if (p_LapCoeffSelf[iMinor] < mincoeffself[threadIdx.x])
		{
			mincoeffself[threadIdx.x] = p_LapCoeffSelf[iMinor];
			iMin[threadIdx.x] = iMinor;
		};
	};
	
	// still fails without the following.

	
	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			if (mincoeffself[threadIdx.x] > mincoeffself[threadIdx.x + k])
			{
				mincoeffself[threadIdx.x] = mincoeffself[threadIdx.x + k];
				iMin[threadIdx.x] = iMin[threadIdx.x + k];
			}
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {

			if (mincoeffself[threadIdx.x] > mincoeffself[threadIdx.x + s - 1])
			{
				mincoeffself[threadIdx.x] = mincoeffself[threadIdx.x + s - 1];
				iMin[threadIdx.x] = iMin[threadIdx.x + s - 1];
			}
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_min_array[blockIdx.x] = mincoeffself[threadIdx.x];
		p_min_index[blockIdx.x] = iMin[threadIdx.x];
	}

}*/
// Correct disposition of routines:
// --- union of T and [v + v_overall] -- uses n_shards --> pressure, momflux, grad Te
// --- union of T and [v + v_overall] -- uses n_n shards --> neutral pressure, neutral momflux
// --- Az,Azdot + v_overall -- runs for whole domain ---> Lap A, curl A, grad A, grad Adot, ROCAz, ROCAzdot
//    ^^ base off of GetLap_minor.

// Worst case number of vars:
// (4+2)*1.5+6.5 <-- because we use v_vertex. + 3 for positions. 
// What can we stick in L1? n_cent we could.
// We should be aiming a ratio 3:1 from shared:L1, if registers are small.
// For tris we are using n_shards from shared points.
// And it is for tris that we require vertex data v to be present.
// Idea: vertex code determines array of 12 relevant n and sticks them into shared.
// Only saved us 1 var. 9 + 6 + 3 = 18.
// Still there is premature optimization here -- none of this happens OFTEN.

// ever called?
/*
__global__ void kernelCreate_pressure_gradT_and_gradA_LapA_CurlA_minor(

	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_minor,
	AAdot * __restrict__ p_AAdot,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor, // Just so we can handle insulator

	f64_vec2 * __restrict__ p_GradTe,
	f64_vec2 * __restrict__ p_GradAz,
	f64 * __restrict__ p_LapAz,

	f64 * __restrict__ ROCAzduetoAdvection,
	f64 * __restrict__ ROCAzdotduetoAdvection,
	f64_vec2 * __restrict__ p_v_overall_minor,

	f64_vec3 * __restrict__ p_B,
	f64 * __restrict__ p_AreaMinor
)
{
	// Getting this down to 8 vars we could have 512 threads (12 vars/thread total with vertex vars)
	// Down to 6 -> 9 total -> 600+ threads
	// Worry later.

	__shared__ T2 shared_T[threadsPerTileMinor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64 shared_Azdot[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];

	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];
	// Problem: we only have room for 1 at a time. Have to run again with n_n. Too bad.
	// Live with it and push through.
	// This applies to both vertices and triangles. And putting in L1 unshared is not better.
	// We can imagine doing it some other way but using shards is true to the design that was created on CPU.
	// Of course this means we'd be better off putting
	// We could also argue that with shards for n_ion in memory we are better off doing an overwrite and doing stuff for nv also.
	// never mind that for now

	__shared__ T2 shared_T_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];
	__shared__ f64 shared_Azdot_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	// There is a good argument for splitting out A,Adot to a separate routine.
	// That way we could have 10.5 => 585 ie 576 = 288*2 threads.

	// Here we got (2+1+1+2)*1.5 = 9 , + 6.5 = 15.5 -> 384 minor threads max.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	{
		AAdot temp = p_AAdot[iMinor];
		shared_Az[threadIdx.x] = temp.Az;
		shared_Azdot[threadIdx.x] = temp.Azdot;
	}
	{
		T3 T_ = p_T_minor[iMinor];
		shared_T[threadIdx.x].Te = T_.Te;
		shared_T[threadIdx.x].Ti = T_.Ti;
	}

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		AAdot temp = p_AAdot[iVertex + BEGINNING_OF_CENTRAL];
		shared_Az_verts[threadIdx.x] = temp.Az;
		shared_Azdot_verts[threadIdx.x] = temp.Azdot;
		T3 T_ = p_T_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_T_verts[threadIdx.x].Te = T_.Te;
		shared_T_verts[threadIdx.x].Ti = T_.Ti; // MOVED THIS OUT OF the following branch to see it match CPU
		if (info.flag == DOMAIN_VERTEX) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
		}
		else {
			// save several bus trips;
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			//shared_T_verts[threadIdx.x].Te = 0.0;
			//shared_T_verts[threadIdx.x].Ti = 0.0;
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
		};
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	f64 ourAzdot, oppAzdot, prevAzdot, nextAzdot;
	f64_vec2 opppos, prevpos, nextpos;
	T2 oppT, prevT, nextT;
	//nvals our_n, opp_n, prev_n, next_n;
	f64_vec2 Our_integral_curl_Az, Our_integral_grad_Az, Our_integral_grad_Azdot, Our_integral_grad_Te;
	f64 Our_integral_Lap_Az;

	if (threadIdx.x < threadsPerTileMajor) {
		Our_integral_curl_Az.x = 0.0;
		Our_integral_curl_Az.y = 0.0;
		Our_integral_grad_Az.x = 0.0;
		Our_integral_grad_Az.y = 0.0;
		Our_integral_grad_Azdot.x = 0.0;
		Our_integral_grad_Azdot.y = 0.0;
		Our_integral_grad_Te.x = 0.0;
		Our_integral_grad_Te.y = 0.0;
		Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		f64_vec3 MAR_ion, MAR_elec;
		memcpy(&(MAR_ion), &(p_MAR_ion[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
		memcpy(&(MAR_elec), &(p_MAR_elec[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		ourAz = shared_Az_verts[threadIdx.x];
		ourAzdot = shared_Azdot_verts[threadIdx.x];

		if (info.flag == DOMAIN_VERTEX) {

			short iprev = tri_len - 1;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevT = shared_T[izTri[iprev] - StartMinor];
				prevAz = shared_Az[izTri[iprev] - StartMinor];
				prevAzdot = shared_Azdot[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				T3 prev_T = p_T_minor[izTri[iprev]];
				prevT.Te = prev_T.Te; prevT.Ti = prev_T.Ti;
				AAdot temp = p_AAdot[izTri[iprev]];
				prevAz = temp.Az;
				prevAzdot = temp.Azdot;
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			short inext, i = 0;
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppT = shared_T[izTri[i] - StartMinor];
				oppAz = shared_Az[izTri[i] - StartMinor];
				oppAzdot = shared_Azdot[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				T3 opp_T = p_T_minor[izTri[i]];
				oppT.Te = opp_T.Te; oppT.Ti = opp_T.Ti;
				AAdot temp = p_AAdot[izTri[i]];
				oppAz = temp.Az;
				oppAzdot = temp.Azdot;
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

			// Think carefully: DOMAIN vertex cases for n,T ...

			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt1, endpt0 = THIRD * (info.pos + opppos + prevpos);

			short iend = tri_len;
			f64_vec2 projendpt0, edge_normal;
			if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
				iend = tri_len - 2;
				if (info.flag == OUTERMOST) {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
				}
				else {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
				}
				edge_normal.x = endpt0.y - projendpt0.y;
				edge_normal.y = projendpt0.x - endpt0.x;
				AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
			};

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextT = shared_T[izTri[inext] - StartMinor];
					nextAz = shared_Az[izTri[inext] - StartMinor];
					nextAzdot = shared_Azdot[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					T3 next_T = p_T_minor[izTri[inext]];
					nextT.Te = next_T.Te; nextT.Ti = next_T.Ti;
					AAdot temp = p_AAdot[izTri[inext]];
					nextAz = temp.Az;
					nextAzdot = temp.Azdot;
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
				//	Our_integral_grad_Az += Az_edge * edge_normal;
				//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
				//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA
				f64_vec2 integ_grad_Az;

				integ_grad_Az.x = 0.5*(
					(ourAz + nextAz)*(info.pos.y - nextpos.y)
					+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
					+ (oppAz + prevAz)*(opppos.y - prevpos.y)
					+ (nextAz + oppAz)*(nextpos.y - opppos.y)
					);
				integ_grad_Az.y = -0.5*( // notice minus
					(ourAz + nextAz)*(info.pos.x - nextpos.x)
					+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
					+ (oppAz + prevAz)*(opppos.x - prevpos.x)
					+ (nextAz + oppAz)*(nextpos.x - opppos.x)
					);
				f64 area_quadrilateral = 0.5*(
					(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
					+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
					+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
					+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
					);
				//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				T2 T0, T1;
				f64 n1;
				T0.Te = THIRD* (prevT.Te + shared_T_verts[threadIdx.x].Te + oppT.Te);
				T1.Te = THIRD * (nextT.Te + shared_T_verts[threadIdx.x].Te + oppT.Te);
				T0.Ti = THIRD * (prevT.Ti + shared_T_verts[threadIdx.x].Ti + oppT.Ti);
				T1.Ti = THIRD * (nextT.Ti + shared_T_verts[threadIdx.x].Ti + oppT.Ti);
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);

				// So this is pretty stupid ---
				// If shardmodel went for flat then we have decided that there is no pressure gradient affecting v here.
				// Mind you we didn't expect it to be flat nearly as often as it is flat.
				// Think carefully about what pressure we want to feel.
				// It makes a kind of sense if you have a cliff of density then you feel it in the triangle in between.
				// But that won't push points apart. It just sends stuff through the wall. 

				//		It's a shame we can't just use actual n values to infer gradient over a region. 
				//		It probably creates wobbles in v as well, because if we move fast particles at edge then we leave
				//		Behind a still-lower v in the vertex-centered minor.
				//		The scheme is kind of skewiffifying.

				// Assume neighs 0,1 are relevant to border with tri 0 minor

				// To get integral grad we add the averages along the edges times edge_normals
				MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
				MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);

				//				if (iVertex == VERT1) {
				//					printf("GPUpressure %d MAR_ion.x %1.12E contrib.x %1.12E n0 %1.12E Ti0 %1.9E n1 %1.9E Ti1 %1.9E edge_normal.x %1.12E \n",
				//						CHOSEN, MAR_ion.x,
				//						-0.5*(n0*T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal.x,
				//						n0, T0.Ti, n1, T1.Ti, edge_normal.x);
				//				}

				Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;

				// if (iVertex + BEGINNING_OF_CENTRAL == CHOSEN)
				//	printf("GPU %d : GradTe contrib %1.14E %1.14E Te %1.14E opp %1.14E next %1.14E prev %1.14E edge_normal %1.14E %1.14E\n", iVertex + BEGINNING_OF_CENTRAL,
				//	0.5*(T0.Te + T1.Te) * edge_normal.x,
				//0.5*(T0.Te + T1.Te) * edge_normal.y, 
				//	shared_T_verts[threadIdx.x].Te, oppT.Te, nextT.Te, prevT.Te,
				//edge_normal.x, edge_normal.y);

				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				f64 Azdot_edge = SIXTH * (2.0*ourAzdot + 2.0*oppAzdot + prevAzdot + nextAzdot);
				Our_integral_grad_Azdot += Azdot_edge * edge_normal;
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az += Az_edge * (endpt1 - endpt0);

				// Missing a factor of 3 possibly?
				// ??????????????????????????????????????????????????????????????


				//	if (Az_edge != Az_edge) 
				//		printf("GPU vert %d Az_edge %1.14E oppAz %1.14E endpt1 %1.14E %1.14E Integ_curl %1.14E %1.14E\n",
				//			iVertex, Az_edge, oppAz, endpt1.x,endpt1.y, Our_integral_curl_Az.x, Our_integral_curl_Az.y
				//		);

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				n0 = n1;

				prevpos = opppos;
				prevAz = oppAz;
				prevAzdot = oppAzdot;
				prevT = oppT;

				opppos = nextpos;
				oppAz = nextAz;
				oppAzdot = nextAzdot;
				oppT = nextT;
			}; // next i

			   //if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {

			   //	// This will never happen because we just asked info.flag == DOMAIN_VERTEX !!

			   //	// Now add on the final sides to give area:

			   //	//    3     4
			   //	//     2 1 0
			   //	// endpt0=endpt1 is now the point north of edge facing 2 anyway.
			   //	f64_vec2 projendpt1;

			   //	if (info.flag == OUTERMOST) {
			   //		endpt1.project_to_radius(projendpt1, FRILL_CENTROID_OUTER_RADIUS_d);
			   //	}
			   //	else {
			   //		endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
			   //	};
			   //	edge_normal.x = projendpt1.y - endpt1.y;
			   //	edge_normal.y = endpt1.x - projendpt1.x;
			   //	AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;

			   //	edge_normal.x = projendpt0.y - projendpt1.y;
			   //	edge_normal.y = projendpt1.x - projendpt0.x;
			   //	AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
			   //	// line between out-projected points
			   //};

			p_GradAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_grad_Az / AreaMinor;
			p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
			p_GradTe[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_grad_Te / AreaMinor;
			p_B[iVertex + BEGINNING_OF_CENTRAL] = Make3(Our_integral_curl_Az / AreaMinor, BZ_CONSTANT);
			p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor;
			//	if (iVertex + BEGINNING_OF_CENTRAL == CHOSEN) printf("Our_integral_grad_Te.x %1.14E AreaMinor %1.14E\n\n",
			//	Our_integral_grad_Te.x, AreaMinor);

			// wow :
			f64_vec2 overall_v_ours = p_v_overall_minor[iVertex + BEGINNING_OF_CENTRAL];
			ROCAzduetoAdvection[iVertex + BEGINNING_OF_CENTRAL] = overall_v_ours.dot(Our_integral_grad_Az / AreaMinor);
			ROCAzdotduetoAdvection[iVertex + BEGINNING_OF_CENTRAL] = overall_v_ours.dot(Our_integral_grad_Azdot / AreaMinor);

			// No neutral stuff in this kernel, momrates should be set now:
			memcpy(p_MAR_ion + iVertex + BEGINNING_OF_CENTRAL, &MAR_ion, sizeof(f64_vec3));
			memcpy(p_MAR_elec + iVertex + BEGINNING_OF_CENTRAL, &MAR_elec, sizeof(f64_vec3));

		}
		else {
			// NOT domain vertex: Do Az, Azdot only:			
			short iprev = tri_len - 1;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevAz = shared_Az[izTri[iprev] - StartMinor];
				prevAzdot = shared_Azdot[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				AAdot temp = p_AAdot[izTri[iprev]];
				prevAz = temp.Az;
				prevAzdot = temp.Azdot;
				prevpos = p_info_minor[izTri[iprev]].pos;
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;
			short inext, i = 0;
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppAz = shared_Az[izTri[i] - StartMinor];
				oppAzdot = shared_Azdot[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				AAdot temp = p_AAdot[izTri[i]];
				oppAz = temp.Az;
				oppAzdot = temp.Azdot;
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec2 endpt1;
			short iend = tri_len;
			f64_vec2 projendpt0, edge_normal;
			if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
				iend = tri_len - 2;
				if (info.flag == OUTERMOST) {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
				}
				else {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
				}
				edge_normal.x = endpt0.y - projendpt0.y;
				edge_normal.y = projendpt0.x - endpt0.x;
				AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;

			};
			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextAz = shared_Az[izTri[inext] - StartMinor];
					nextAzdot = shared_Azdot[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					AAdot temp = p_AAdot[izTri[inext]];
					nextAz = temp.Az;
					nextAzdot = temp.Azdot;
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-
				f64_vec2 integ_grad_Az;
				//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
				//	Our_integral_grad_Az += Az_edge * edge_normal;
				//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise
				//	pData[iDestTri].B -= Az_edge * (endpt1 - endpt0); // MUST DIVIDE BY AREA
				integ_grad_Az.x = 0.5*(
					(ourAz + nextAz)*(info.pos.y - nextpos.y)
					+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
					+ (oppAz + prevAz)*(opppos.y - prevpos.y)
					+ (nextAz + oppAz)*(nextpos.y - opppos.y)
					);
				integ_grad_Az.y = -0.5*( // notice minus
					(ourAz + nextAz)*(info.pos.x - nextpos.x)
					+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
					+ (oppAz + prevAz)*(opppos.x - prevpos.x)
					+ (nextAz + oppAz)*(nextpos.x - opppos.x)
					);
				f64 area_quadrilateral = 0.5*(
					(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
					+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
					+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
					+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
					);
				//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				// To get integral grad we add the averages along the edges times edge_normals
				//			f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				//			f64 Azdot_edge = SIXTH * (2.0*ourAzdot + 2.0*oppAzdot + prevAzdot + nextAzdot);
				//			Our_integral_grad_Azdot += Azdot_edge * edge_normal;
				//			Our_integral_grad_Az += Az_edge * edge_normal;
				//			Our_integral_curl_Az += Az_edge * (endpt1 - endpt0);

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				endpt0 = endpt1;
				prevpos = opppos;
				prevAz = oppAz;
				prevAzdot = oppAzdot;
				opppos = nextpos;
				oppAz = nextAz;
				oppAzdot = nextAzdot;
			}; // next i

			if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
				// Now add on the final sides to give area:
				//    3     4
				//     2 1 0
				// endpt0=endpt1 is now the point north of edge facing 2 anyway.
				f64_vec2 projendpt1;
				if (info.flag == OUTERMOST) {
					endpt1.project_to_radius(projendpt1, FRILL_CENTROID_OUTER_RADIUS_d);
				}
				else {
					endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
				};
				edge_normal.x = projendpt1.y - endpt1.y;
				edge_normal.y = endpt1.x - projendpt1.x;
				AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;

				edge_normal.x = projendpt0.y - projendpt1.y;
				edge_normal.y = projendpt1.x - projendpt0.x;
				AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
				// line between out-projected points
			};

			p_GradAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_grad_Az / AreaMinor; // 0,0
			p_LapAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_Lap_Az / AreaMinor;
			p_B[iVertex + BEGINNING_OF_CENTRAL] = Make3(Our_integral_curl_Az / AreaMinor, BZ_CONSTANT); // 0,0, BZ
			p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor;

			ROCAzduetoAdvection[iVertex + BEGINNING_OF_CENTRAL] = 0.0;
			ROCAzdotduetoAdvection[iVertex + BEGINNING_OF_CENTRAL] = 0.0;

			p_GradTe[iVertex + BEGINNING_OF_CENTRAL] = Vector2(0.0, 0.0);

		}; // // was it domain vertex or Az-only

	};//  if (threadIdx.x < threadsPerTileMajor) 
	  // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	  // __syncthreads(); // end of first vertex part
	  // Do we need syncthreads? Not overwriting any shared data here...

	  // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	//	T2 prevT, nextT, oppT;
	//f64 prevAz, nextAz, oppAz, ourAz;
	//f64 prevAzdot, nextAzdot, oppAzdot, ourAzdot;

	f64_vec3 MAR_ion, MAR_elec;
	// this is not a clever way of doing it. Want more careful.

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		if ((izNeighMinor[3] >= StartMinor) && (izNeighMinor[3] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[3] - StartMinor];
		}
		else {

			AAdot temp = p_AAdot[izNeighMinor[3]];
			oppAz = temp.Az;
		};
		p_LapAz[iMinor] = oppAz - ourAz;

		ROCAzduetoAdvection[iMinor] = 0.0;
		ROCAzdotduetoAdvection[iMinor] = 0.0;
		p_GradAz[iMinor] = Vector2(0.0, 0.0);
		memset(&(p_B[iMinor]), 0, sizeof(f64_vec3));
		p_GradTe[iMinor] = Vector2(0.0, 0.0);
		p_AreaMinor[iMinor] = 1.0e-12;
		memset(&(p_MAR_ion[iMinor]), 0, sizeof(f64_vec3));
		memset(&(p_MAR_elec[iMinor]), 0, sizeof(f64_vec3));
	}
	else {
		Our_integral_curl_Az.x = 0.0;
		Our_integral_curl_Az.y = 0.0;
		Our_integral_grad_Azdot.x = 0.0;
		Our_integral_grad_Azdot.y = 0.0;
		Our_integral_grad_Az.x = 0.0;
		Our_integral_grad_Az.y = 0.0;
		Our_integral_grad_Te.x = 0.0;
		Our_integral_grad_Te.y = 0.0;
		Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		f64 AreaMinor_for_A = 0.0;

		short iprev, inext, i;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			memcpy(&MAR_ion, p_MAR_ion + iMinor, sizeof(f64_vec3));
			memcpy(&MAR_elec, p_MAR_elec + iMinor, sizeof(f64_vec3));

			iprev = 5;
			i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prevT = shared_T[izNeighMinor[iprev] - StartMinor];
				prevAzdot = shared_Azdot[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevAzdot = shared_Azdot_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevT = shared_T_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					T3 prev_T = p_T_minor[izNeighMinor[iprev]];
					prevT.Te = prev_T.Te; prevT.Ti = prev_T.Ti;
					AAdot temp = p_AAdot[izNeighMinor[iprev]];
					prevAz = temp.Az;
					prevAzdot = temp.Azdot;
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				oppAz = shared_Az[izNeighMinor[i] - StartMinor];
				oppT = shared_T[izNeighMinor[i] - StartMinor];
				oppAzdot = shared_Azdot[izNeighMinor[i] - StartMinor];
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppAzdot = shared_Azdot_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppT = shared_T_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					T3 opp_T = p_T_minor[izNeighMinor[i]];
					oppT.Te = opp_T.Te; oppT.Ti = opp_T.Ti;
					AAdot temp = p_AAdot[izNeighMinor[i]];
					oppAz = temp.Az;
					oppAzdot = temp.Azdot;
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);

			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;
			// indexminor sequence:
			// 0 = corner 0
			// 1 = neighbour 2
			// 2 = corner 1
			// 3 = neighbour 0
			// 4 = corner 2
			// 5 = neighbour 1

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;

				// Worry about pathological cases later.
				// &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

				// Pathological case: OUTERMOST vertex where neigh_len is not correct to take as == tri_len

				// !

				// ///////////////////////////////////////////////////////////////////////////////////////////
				// [0] is on our clockwise side rel to [1]. That means it is anticlockwise for the vertex. 
				// That means we interpolate with the value from next tri around.
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));
					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				// Worry about pathological cases later.

				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;

				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);

			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};
				//This matches a diagram:
				//             
				//     2---(4)----(3)---1 = corner 1 = indexminor 2: (2,3)
				//      \  /       \   /
				//       \/         \ /
				//       (5\       (2/   indexminor 1 = neighbour 2: (1,2)
				//         \        /
				//          \0)--(1/
				//           \   _/
				//             0  = corner 0 = indexminor0
			};

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
					nextT = shared_T[izNeighMinor[inext] - StartMinor];
					nextAzdot = shared_Azdot[izNeighMinor[inext] - StartMinor];
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextAzdot = shared_Azdot_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextT = shared_T_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {

						AAdot temp = p_AAdot[izNeighMinor[inext]];
						nextAz = temp.Az;
						nextAzdot = temp.Azdot;
						T3 next_T = p_T_minor[izNeighMinor[inext]];
						nextT.Te = next_T.Te; nextT.Ti = next_T.Ti;

						next_T = p_T_minor[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;
				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
				//	Our_integral_grad_Az += Az_edge * edge_normal;
				//	Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0); // looks anticlockwise

				integ_grad_Az.x = 0.5*(
					(ourAz + nextAz)*(info.pos.y - nextpos.y)
					+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
					+ (oppAz + prevAz)*(opppos.y - prevpos.y)
					+ (nextAz + oppAz)*(nextpos.y - opppos.y)
					);
				integ_grad_Az.y = -0.5*( // notice minus
					(ourAz + nextAz)*(info.pos.x - nextpos.x)
					+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
					+ (oppAz + prevAz)*(opppos.x - prevpos.x)
					+ (nextAz + oppAz)*(nextpos.x - opppos.x)
					);
				f64 area_quadrilateral = 0.5*(
					(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
					+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
					+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
					+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
					);

				//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				//if ((i % 2 == 0) || ((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
				if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
					(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
					Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;
			
				// We have to not muck around with prevpos because here it's being used for A.
				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				f64 Azdot_edge = SIXTH * (2.0*ourAzdot + 2.0*oppAzdot + prevAzdot + nextAzdot);
				Our_integral_grad_Azdot += Azdot_edge * edge_normal;
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0);

				AreaMinor_for_A += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x; 


				T3 T0, T1; // waste of registers
				f64 n1;
				T0.Te = THIRD* (prevT.Te + shared_T[threadIdx.x].Te + oppT.Te);
				T1.Te = THIRD * (nextT.Te + shared_T[threadIdx.x].Te + oppT.Te);
				T0.Ti = THIRD * (prevT.Ti + shared_T[threadIdx.x].Ti + oppT.Ti);
				T1.Ti = THIRD * (nextT.Ti + shared_T[threadIdx.x].Ti + oppT.Ti); // assumes point is at simple average of tri and vert centres.

				n0 = n_array[i];
				n1 = n_array[inext]; // !

				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
					{
						// typical edge

						MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
						MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
						Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;
						
					} else {
						// Looking into the insulator we see a reflection of nT. Here we look into an out-of-domain tri or vert below ins.
						
						if (flag == CROSSING_INS) {

							// make the edge go from the upper point, down to the insulator.

//							endpt0 = THIRD * (prevpos + info.pos + opppos);
//							endpt1 = THIRD * (nextpos + info.pos + opppos);
//							edge_normal.x = endpt1.y - endpt0.y;
//							edge_normal.y = endpt0.x - endpt1.x;

							// Basically radius changes almost linearly as we move from endpt1 to endpt0.
							f64 r1 = endpt0.modulus();
							f64 r2 = endpt1.modulus();

							if (r1 > r2) {
								// 0 is higher								
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER-r2) / (r1-r2))*(endpt0 - endpt1);
								endpt1 = point; // use this value again later for AreaMinor if nothing else
							} else {
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
								endpt0 = point;
							};
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;

							// set nT on the edge: try just the average of the two nT, weighted by distance to own centre.
							// Recall periodic when we look at distance to own centre.

							f64 nTi_edge = 0.5*(p_n_minor[iMinor].n*shared_T[threadIdx.x].Ti + p_n_minor[izNeighMinor[i]].n*oppT.Ti);
							f64 nTe_edge = 0.5*(p_n_minor[iMinor].n*shared_T[threadIdx.x].Te + p_n_minor[izNeighMinor[i]].n*oppT.Te);

							MAR_ion -= Make3(nTi_edge*over_m_i*edge_normal, 0.0);
							MAR_elec -= Make3(nTe_edge*over_m_e*edge_normal, 0.0);
							Our_integral_grad_Te += 0.5*(shared_T[threadIdx.x].Te + oppT.Te) * edge_normal;

						} else {
							// looking out the bottom of the insulator triangle at a within-insulator vertex or triangle.
							// so we want to project the point up to the insulator.
							
							// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
							char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
							char nextflag = p_info_minor[izNeighMinor[inext]].flag;

							// Let's distinguish several cases:
							if (prevflag == CROSSING_INS)
							{
								// endpt0 is THIRD * (prevpos + info.pos + opppos) 

								// move towards the position that is 2 previous --- ie the vertex above.
								// (Don't forget PBC.)
								int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
								f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
								if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
								if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;
								
								f64 r1 = prevprevpos.modulus();
								f64 r2 = endpt0.modulus();
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
								endpt0 = point;

							} else {
								// prevflag will say it is below ins. 
								// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
								info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);

							};

							if (nextflag == CROSSING_INS)
							{
								// We still want to move towards vertex above. But now it's 2 next
								// Don't forget PBC
								int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
								f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
								if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
								if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;

								f64 r1 = nextnextpos.modulus();
								f64 r2 = endpt1.modulus();
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
								endpt1 = point;

							} else {
								// safest way: put point at the projection of our own position to insulator, maybe slightly off.
								info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);
								
							}

							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;

							f64 nTi_edge = p_n_minor[iMinor].n*shared_T[threadIdx.x].Ti;
							f64 nTe_edge = p_n_minor[iMinor].n*shared_T[threadIdx.x].Te;

							MAR_ion -= Make3(nTi_edge*over_m_i*edge_normal, 0.0);
							MAR_elec -= Make3(nTe_edge*over_m_e*edge_normal, 0.0);
							Our_integral_grad_Te += shared_T[threadIdx.x].Te * edge_normal;

							// will be a 0 contribution if endpt1 = endpt0, that's ok.
						};
					};
				} else {
					
					// Typical tri.
					MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
					MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
					Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;

				};

				if (TESTTRI) {
					printf("GPU : %d : contribs MAR_ion.y %1.11E MAR_elec.y %1.11E \n"
						"n0 %1.10E n1 %1.10E Ti0 %1.10E Ti1 %1.10E edgenormal.y %1.10E\n",
						CHOSEN,
						-0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal.y,
						-0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal.y,
						n0, n1, T0.Ti, T1.Ti, edge_normal.y);
				}

				// Having a real problem with AreaMinor.
				// All very well how it is used here but the one we should record, for creating N and hence pressure effects ..
				// is the one that comes from rolling points upwards.
				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x; // Area to save.

				
				// See a way that FP accuracy was eroded: we take a difference of two close things already to get edge_normal.
				// can that be cleverly avoided? For all calcs?
				
				endpt0 = endpt1;
				n0 = n1;

				iprev = i;
				prevpos = opppos;
				prevAz = oppAz;
				prevAzdot = oppAzdot;
				prevT = oppT;

				opppos = nextpos;
				oppAz = nextAz;
				oppAzdot = nextAzdot;
				oppT = nextT;
			};

			/*if (info.flag == CROSSING_INS) {
				// In this case set v_r = 0 and set a_TP_r = 0 and dv/dt _r = 0 in general
				//f64_vec2 rhat = info.pos / info.pos.modulus();
				MAR_ion -= Make3(
					(MAR_ion.dotxy(info.pos) /
					(info.pos.x*info.pos.x + info.pos.y*info.pos.y))*info.pos, 0.0);
				MAR_elec -= Make3(
					(MAR_elec.dotxy(info.pos) /
					(info.pos.x*info.pos.x + info.pos.y*info.pos.y))*info.pos, 0.0);

				no

				// and we looked at insulator values for T so Grad Te was meaningless:
				Our_integral_grad_Te.x = 0.0;
				Our_integral_grad_Te.y = 0.0;

				// I think we do need to make v_r = 0. It's common sense that it IS 0
				// since we site our v_r estimate on the insulator. Since it is sited there,
				// it is used for traffic into the insulator by n,nT unless we pick out
				// insulator-abutting cells on purpose.

				// However, we then should make an energy correction -- at least if
				// momentum is coming into this minor cell and being destroyed.

				// Doesn't quite work like that. We do not destroy, we just do not store a value for the mom in the domain part of cell.
			};*/
/*
			p_GradAz[iMinor] = Our_integral_grad_Az / AreaMinor_for_A;
			p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor_for_A;
			p_GradTe[iMinor] = Our_integral_grad_Te / AreaMinor;
			p_B[iMinor] = Make3(Our_integral_curl_Az / AreaMinor_for_A, BZ_CONSTANT);
			p_AreaMinor[iMinor] = AreaMinor;

			// wow :
			f64_vec2 overall_v_ours = p_v_overall_minor[iMinor];
			ROCAzduetoAdvection[iMinor] = overall_v_ours.dot(Our_integral_grad_Az / AreaMinor);
			ROCAzdotduetoAdvection[iMinor] = overall_v_ours.dot(Our_integral_grad_Azdot / AreaMinor);

			// No neutral stuff in this kernel, momrates should be set now:
			memcpy(p_MAR_ion + iMinor, &(MAR_ion), sizeof(f64_vec3));
			memcpy(p_MAR_elec + iMinor, &(MAR_elec), sizeof(f64_vec3));
		}
		else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================

			iprev = 5; i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prevAzdot = shared_Azdot[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevAzdot = shared_Azdot_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					AAdot temp = p_AAdot[izNeighMinor[iprev]];
					prevAz = temp.Az;
					prevAzdot = temp.Azdot;
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				oppAz = shared_Az[izNeighMinor[i] - StartMinor];
				oppAzdot = shared_Azdot[izNeighMinor[i] - StartMinor];
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppAzdot = shared_Azdot_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					AAdot temp = p_AAdot[izNeighMinor[i]];
					oppAz = temp.Az;
					oppAzdot = temp.Azdot;
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;


#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
					nextAzdot = shared_Azdot[izNeighMinor[inext] - StartMinor];
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextAzdot = shared_Azdot_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						AAdot temp = p_AAdot[izNeighMinor[inext]];
						nextAz = temp.Az;
						nextAzdot = temp.Azdot;
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:

				f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
				//	Our_integral_grad_Az += Az_edge * edge_normal;
				//	Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0); // looks anticlockwise

				integ_grad_Az.x = 0.5*(
					(ourAz + nextAz)*(info.pos.y - nextpos.y)
					+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
					+ (oppAz + prevAz)*(opppos.y - prevpos.y)
					+ (nextAz + oppAz)*(nextpos.y - opppos.y)
					);
				integ_grad_Az.y = -0.5*( // notice minus
					(ourAz + nextAz)*(info.pos.x - nextpos.x)
					+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
					+ (oppAz + prevAz)*(opppos.x - prevpos.x)
					+ (nextAz + oppAz)*(nextpos.x - opppos.x)
					);
				f64 area_quadrilateral = 0.5*(
					(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
					+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
					+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
					+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
					);

				//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				//		if ((i % 2 == 0) || // vertex neigh 
				//				((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
				if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
					(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
					Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				f64 Azdot_edge = SIXTH * (2.0*ourAzdot + 2.0*oppAzdot + prevAzdot + nextAzdot);
				Our_integral_grad_Azdot += Azdot_edge * edge_normal;
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0);
				// minus

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;


				endpt0 = endpt1;

				iprev = i;
				prevpos = opppos;
				prevAz = oppAz;
				prevAzdot = oppAzdot;

				opppos = nextpos;
				oppAz = nextAz;
				oppAzdot = nextAzdot;

			};

			p_GradAz[iMinor] = Our_integral_grad_Az / AreaMinor;
			p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
			p_B[iMinor] = Make3(Our_integral_curl_Az / AreaMinor, BZ_CONSTANT);
			p_AreaMinor[iMinor] = AreaMinor;

			ROCAzduetoAdvection[iMinor] = 0.0;
			ROCAzdotduetoAdvection[iMinor] = 0.0;
		} // non-domain tri
	}; // was it FRILL

	   // Okay. While we have n_shards in memory we could proceed to overwrite with vxy.
	   // But get running first before using union and checking same.
}*/

__global__ void kernelCreate_pressure_gradT_and_gradA_CurlA_minor_noadvect(

	structural * __restrict__ p_info_minor,
	T3 * __restrict__ p_T_minor,
	AAdot * __restrict__ p_AAdot,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor, // Just so we can handle insulator

	bool * __restrict__ bz_pressureflag, 

	f64_vec2 * __restrict__ p_GradTe,
	f64_vec2 * __restrict__ p_GradAz,

	f64_vec3 * __restrict__ p_B
)
{
	// Getting this down to 8 vars we could have 512 threads (12 vars/thread total with vertex vars)
	
	__shared__ T2 shared_T[threadsPerTileMinor];
	__shared__ f64 shared_Az[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];

	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];  // 5 + 13/2 + below, 2.5 -> 14 doubles.
	
	// Tile size is 256 though so 14 doubles will allow 1x to run. We have extra shared space if we need it.

	// We could also argue that with shards for n_ion in memory we are better off doing an overwrite and doing stuff for nv also.
	// never mind that for now. <-- ?

	// 2019: Hang on. Why did I use shards? It's quite a good idea. If we are flat then the pressure lands more on the triangles
	// at the interface. Makes a consistent set of values of n to pave the space.

	__shared__ T2 shared_T_verts[threadsPerTileMajor];
	__shared__ f64 shared_Az_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	// There is a good argument for splitting out A,Adot to a separate routine.
	// That way we could have 10.5 => 585 ie 576 = 288*2 threads.
	// Here we got (2+1+1+2)*1.5 = 9 , + 6.5 = 15.5 -> 384 minor threads max.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	{
		AAdot temp = p_AAdot[iMinor];
		shared_Az[threadIdx.x] = temp.Az;
	}
	{
		T3 T_ = p_T_minor[iMinor];
		shared_T[threadIdx.x].Te = T_.Te;
		shared_T[threadIdx.x].Ti = T_.Ti;
	}

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		AAdot temp = p_AAdot[iVertex + BEGINNING_OF_CENTRAL];
		shared_Az_verts[threadIdx.x] = temp.Az;
		T3 T_ = p_T_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_T_verts[threadIdx.x].Te = T_.Te;
		shared_T_verts[threadIdx.x].Ti = T_.Ti; // MOVED THIS OUT OF the following branch to see it match CPU
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
		}
		else {
			// save several bus trips;
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			//shared_T_verts[threadIdx.x].Te = 0.0;
			//shared_T_verts[threadIdx.x].Ti = 0.0;
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
		};
	};

	__syncthreads();

	f64 ourAz, oppAz, prevAz, nextAz;
	//f64 ourAzdot, oppAzdot, prevAzdot, nextAzdot;
	f64_vec2 opppos, prevpos, nextpos, edge_normal;
	T2 oppT, prevT, nextT;
	//nvals our_n, opp_n, prev_n, next_n;
	f64_vec2 Our_integral_curl_Az, Our_integral_grad_Az, Our_integral_grad_Te;
	f64 Our_integral_Lap_Az;

	if (threadIdx.x < threadsPerTileMajor) {
		Our_integral_curl_Az.x = 0.0;
		Our_integral_curl_Az.y = 0.0;
		Our_integral_grad_Az.x = 0.0;
		Our_integral_grad_Az.y = 0.0;
		Our_integral_grad_Te.x = 0.0;
		Our_integral_grad_Te.y = 0.0;
		Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		f64_vec3 MAR_ion, MAR_elec;
		memcpy(&(MAR_ion), &(p_MAR_ion[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
		memcpy(&(MAR_elec), &(p_MAR_elec[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		ourAz = shared_Az_verts[threadIdx.x];

		bool bPressure = bz_pressureflag[iVertex];
		// True for DOMAIN_VERTEX, unless you've got a crossing_cath in which case it's false.

		if (bPressure) {

			short iprev = tri_len - 1;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevT = shared_T[izTri[iprev] - StartMinor];
				prevAz = shared_Az[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				T3 prev_T = p_T_minor[izTri[iprev]];
				prevT.Te = prev_T.Te; prevT.Ti = prev_T.Ti;
				AAdot temp = p_AAdot[izTri[iprev]];
				prevAz = temp.Az;
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			short inext, i = 0;
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppT = shared_T[izTri[i] - StartMinor];
				oppAz = shared_Az[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			} else {
				T3 opp_T = p_T_minor[izTri[i]];
				oppT.Te = opp_T.Te; oppT.Ti = opp_T.Ti;
				AAdot temp = p_AAdot[izTri[i]];
				oppAz = temp.Az;
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;
			 
			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt1, endpt0 = THIRD * (info.pos + opppos + prevpos);

			short iend = tri_len;

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextT = shared_T[izTri[inext] - StartMinor];
					nextAz = shared_Az[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				} else {
					T3 next_T = p_T_minor[izTri[inext]];
					nextT.Te = next_T.Te; nextT.Ti = next_T.Ti;
					AAdot temp = p_AAdot[izTri[inext]];
					nextAz = temp.Az;
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				T2 T0, T1;
				f64 n1;
				T0.Te = THIRD* (prevT.Te + shared_T_verts[threadIdx.x].Te + oppT.Te);
				T1.Te = THIRD * (nextT.Te + shared_T_verts[threadIdx.x].Te + oppT.Te);
				T0.Ti = THIRD * (prevT.Ti + shared_T_verts[threadIdx.x].Ti + oppT.Ti);
				T1.Ti = THIRD * (nextT.Ti + shared_T_verts[threadIdx.x].Ti + oppT.Ti);
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);

				// So this is pretty stupid ---
				// If shardmodel went for flat then we have decided that there is no pressure gradient affecting v here.
				// Mind you we didn't expect it to be flat nearly as often as it is flat.
				// Think carefully about what pressure we want to feel.
				// It makes a kind of sense if you have a cliff of density then you feel it in the triangle in between.
				// ***************************************************************************************
				//         But that won't push points apart. It just sends stuff through the wall. 
				// ***************************************************************************************
				// Hmm.

		//		It's a shame we can't just use actual n values to infer gradient over a region. 
		//		It probably creates wobbles in v as well, because if we move fast particles at edge then we leave
		//		Behind a still-lower v in the vertex-centered minor. <-- yes, this instability is clear in practice.
		//		The scheme is kind of skewiffifying.
				
				// Assume neighs 0,1 are relevant to border with tri 0 minor

				// To get integral grad we add the averages along the edges times edge_normals
				MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
				MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				
				if (TESTPRESSUREY) {
					printf("Pressure vertex %d MAR_ion.y %1.9E contrib.y %1.9E n0 %1.9E Ti0 %1.9E n1 %1.9E Ti1 %1.9E edge_normal.y %1.9E \n",
						VERTCHOSEN, MAR_ion.y,
						-0.5*(n0*T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal.y,
						n0, T0.Ti, n1, T1.Ti, edge_normal.y);
				}

				Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;

				// if (iVertex + BEGINNING_OF_CENTRAL == CHOSEN)
				//	printf("GPU %d : GradTe contrib %1.14E %1.14E Te %1.14E opp %1.14E next %1.14E prev %1.14E edge_normal %1.14E %1.14E\n", iVertex + BEGINNING_OF_CENTRAL,
					//	0.5*(T0.Te + T1.Te) * edge_normal.x,
						//0.5*(T0.Te + T1.Te) * edge_normal.y, 
					//	shared_T_verts[threadIdx.x].Te, oppT.Te, nextT.Te, prevT.Te,
						//edge_normal.x, edge_normal.y);

				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0);
				// Introduced minus because we otherwise are getting negative of curl.

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				n0 = n1;

				prevpos = opppos;
				prevAz = oppAz;
				prevT = oppT;

				opppos = nextpos;
				oppAz = nextAz;
				oppT = nextT;
			}; // next i
			
			p_GradAz[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_grad_Az / AreaMinor;
			p_GradTe[iVertex + BEGINNING_OF_CENTRAL] = Our_integral_grad_Te / AreaMinor;
			p_B[iVertex + BEGINNING_OF_CENTRAL] = Make3(Our_integral_curl_Az / AreaMinor, BZ_CONSTANT);
		
			// No neutral stuff in this kernel, momrates should be set now:
			memcpy(p_MAR_ion + iVertex + BEGINNING_OF_CENTRAL, &MAR_ion, sizeof(f64_vec3));
			memcpy(p_MAR_elec + iVertex + BEGINNING_OF_CENTRAL, &MAR_elec, sizeof(f64_vec3));

		} else {
			Vector2 zero(0.0, 0.0);
			p_GradAz[iVertex + BEGINNING_OF_CENTRAL] = zero;
			p_GradTe[iVertex + BEGINNING_OF_CENTRAL] = zero;
			p_B[iVertex + BEGINNING_OF_CENTRAL] = Make3(zero, BZ_CONSTANT);
			
			// we certainly could still calculate B, though that was not how this was before.

		}; // bPressure

	};//  if (threadIdx.x < threadsPerTileMajor) 
	   // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	   // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	ourAz = shared_Az[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	//	T2 prevT, nextT, oppT;
	//f64 prevAz, nextAz, oppAz, ourAz;
	//f64 prevAzdot, nextAzdot, oppAzdot, ourAzdot;

	f64_vec3 MAR_ion,MAR_elec;
	// this is not a clever way of doing it. Want more careful.

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		if ((izNeighMinor[3] >= StartMinor) && (izNeighMinor[3] < EndMinor))
		{
			oppAz = shared_Az[izNeighMinor[3] - StartMinor];
		} else {
			AAdot temp = p_AAdot[izNeighMinor[3]];
			oppAz = temp.Az;
		};
		// p_LapAz[iMinor] = oppAz - ourAz; // OBSOLETE ---- need to delete this from routine.
		
		p_GradAz[iMinor] = Vector2(0.0, 0.0);
		memset(&(p_B[iMinor]), 0, sizeof(f64_vec3));
		p_GradTe[iMinor] = Vector2(0.0, 0.0);
	//	p_AreaMinor[iMinor] = 1.0e-12;
		memset(&(p_MAR_ion[iMinor]), 0, sizeof(f64_vec3));
		memset(&(p_MAR_elec[iMinor]), 0, sizeof(f64_vec3));
	} else {
		Our_integral_curl_Az.x = 0.0;
		Our_integral_curl_Az.y = 0.0;
		Our_integral_grad_Az.x = 0.0;
		Our_integral_grad_Az.y = 0.0;
		Our_integral_grad_Te.x = 0.0;
		Our_integral_grad_Te.y = 0.0;
		Our_integral_Lap_Az = 0.0;
		f64 AreaMinor = 0.0;
		f64 AreaMinor_for_A = 0.0;

		short iprev, inext, i;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			memcpy(&MAR_ion, p_MAR_ion + iMinor, sizeof(f64_vec3));
			memcpy(&MAR_elec, p_MAR_elec + iMinor, sizeof(f64_vec3));

			iprev = 5;
			i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prevT = shared_T[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevT = shared_T_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					T3 prev_T = p_T_minor[izNeighMinor[iprev]];
					prevT.Te = prev_T.Te; prevT.Ti = prev_T.Ti;
					AAdot temp = p_AAdot[izNeighMinor[iprev]];
					prevAz = temp.Az;
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				oppAz = shared_Az[izNeighMinor[i] - StartMinor];
				oppT = shared_T[izNeighMinor[i] - StartMinor];
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppT = shared_T_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					T3 opp_T = p_T_minor[izNeighMinor[i]];
					oppT.Te = opp_T.Te; oppT.Ti = opp_T.Ti;
					AAdot temp = p_AAdot[izNeighMinor[i]];
					oppAz = temp.Az;
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);

			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;
			// indexminor sequence:
			// 0 = corner 0
			// 1 = neighbour 2
			// 2 = corner 1
			// 3 = neighbour 0
			// 4 = corner 2
			// 5 = neighbour 1

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;

				// Worry about pathological cases later.
				// &&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&

				// Pathological case: OUTERMOST vertex where neigh_len is not correct to take as == tri_len

				// !

				// ///////////////////////////////////////////////////////////////////////////////////////////
				// [0] is on our clockwise side rel to [1]. That means it is anticlockwise for the vertex. 
				// That means we interpolate with the value from next tri around.
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);

				if (TESTTRI) 
					printf("%d 01A n_array 01 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[0], n_array[1], cornerindex.i1,
						shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I],
						shared_n_shards[cornerindex.i1 - StartMajor].n_cent);

			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));

					// the first two entries

					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);					
				} else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						
						// n1 goes with "prev" -- did I do that on purpose?
						
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					} else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
																		
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);						
					};
				};

				if (TESTTRI)
					printf("%d 01B n_array 01 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[0], n_array[1], cornerindex.i1,
						p_n_shards[cornerindex.i1].n[who_am_I],
						p_n_shards[cornerindex.i1].n_cent);

			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				// Worry about pathological cases later.
				
				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
								+   shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
								+   shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
								+   shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
								+   shared_n_shards[cornerindex.i2 - StartMajor].n_cent);

				if (TESTTRI)
					printf("%d 23A n_array 23 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[2], n_array[3], cornerindex.i2,
						shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I],
						shared_n_shards[cornerindex.i2 - StartMajor].n_cent);

			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};

				if (TESTTRI)
					printf("%d 23B n_array 23 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[2], n_array[3], cornerindex.i2,
						p_n_shards[cornerindex.i2].n[who_am_I],
						p_n_shards[cornerindex.i2].n_cent);
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;

				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);

				if (TESTTRI)
					printf("%d 45A n_array 45 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[4], n_array[5], cornerindex.i3,
						shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I],
						shared_n_shards[cornerindex.i3 - StartMajor].n_cent);

			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				} else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};

				if (TESTTRI)
					printf("%d 45B n_array 45 %1.9E %1.9E corner %d n_shards[who_am_I] %1.9E n_cent %1.9E",
						iMinor, n_array[4], n_array[5], cornerindex.i3,
						p_n_shards[cornerindex.i3].n[who_am_I],
						p_n_shards[cornerindex.i3].n_cent);

				//This matches a diagram:
				//             
				//     2---(4)----(3)---1 = corner 1 = indexminor 2: (2,3)
				//      \  /       \   /
				//       \/         \ /
				//       (5\       (2/   indexminor 1 = neighbour 2: (1,2)
				//         \        /
				//          \0)--(1/
				//           \   _/
				//             0  = corner 0 = indexminor0
			};

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
					nextT = shared_T[izNeighMinor[inext] - StartMinor];
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextT = shared_T_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {

						AAdot temp = p_AAdot[izNeighMinor[inext]];
						nextAz = temp.Az;
						T3 next_T = p_T_minor[izNeighMinor[inext]];
						nextT.Te = next_T.Te; nextT.Ti = next_T.Ti;

						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;
				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0);

				AreaMinor_for_A += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;


				//integ_grad_Az.x = 0.5*(
				//	(ourAz + nextAz)*(info.pos.y - nextpos.y)
				//	+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
				//	+ (oppAz + prevAz)*(opppos.y - prevpos.y)
				//	+ (nextAz + oppAz)*(nextpos.y - opppos.y)
				//	);
				//integ_grad_Az.y = -0.5*( // notice minus
				//	(ourAz + nextAz)*(info.pos.x - nextpos.x)
				//	+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
				//	+ (oppAz + prevAz)*(opppos.x - prevpos.x)
				//	+ (nextAz + oppAz)*(nextpos.x - opppos.x)
				//	);
				//f64 area_quadrilateral = 0.5*(
				//	(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
				//	+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
				//	+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
				//	+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
				//	);

				////f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
				////if ((i % 2 == 0) || ((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
				//if ( (opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
				//	 (opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
				//	Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				//// Modified here ..

				T3 T0, T1; // waste of registers
				f64 n1;
				T0.Te = THIRD* (prevT.Te + shared_T[threadIdx.x].Te + oppT.Te);
				T1.Te = THIRD * (nextT.Te + shared_T[threadIdx.x].Te + oppT.Te);
				T0.Ti = THIRD * (prevT.Ti + shared_T[threadIdx.x].Ti + oppT.Ti);
				T1.Ti = THIRD * (nextT.Ti + shared_T[threadIdx.x].Ti + oppT.Ti);

				n0 = n_array[i];
				n1 = n_array[inext]; // !

				// To get integral grad we add the averages along the edges times edge_normals

				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
					{
						// typical edge

						MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
						MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
						Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;

					} else {
						// Looking into the insulator we see a reflection of nT. Here we look into an out-of-domain tri or vert below ins.
						// Or allowed a below-ins value to affect something anyway.
						
						if (flag == CROSSING_INS) {

							// make the edge go from the upper point, down to the insulator.

							// Basically radius changes almost linearly as we move from endpt1 to endpt0.
							f64 r1 = endpt0.modulus();
							f64 r2 = endpt1.modulus();

							if (r1 > r2) {
								// 0 is higher								
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(endpt0 - endpt1);
								endpt1 = point; // use this value again later for AreaMinor if nothing else
							}
							else {
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
								endpt0 = point;
							};
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;

							// set nT on the edge: try just the average of the two nT, weighted by distance to own centre.
							// Recall periodic when we look at distance to own centre.

							f64 nTi_edge = 0.5*(p_n_minor[iMinor].n*shared_T[threadIdx.x].Ti + p_n_minor[izNeighMinor[i]].n*oppT.Ti);
							f64 nTe_edge = 0.5*(p_n_minor[iMinor].n*shared_T[threadIdx.x].Te + p_n_minor[izNeighMinor[i]].n*oppT.Te);

							MAR_ion -= Make3(nTi_edge*over_m_i*edge_normal, 0.0);
							MAR_elec -= Make3(nTe_edge*over_m_e*edge_normal, 0.0);
							Our_integral_grad_Te += 0.5*(shared_T[threadIdx.x].Te + oppT.Te) * edge_normal;

						} else {
							// looking out the bottom of the insulator triangle at a within-insulator vertex or triangle.
							// so we want to project the point up to the insulator.

							// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
							char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
							char nextflag = p_info_minor[izNeighMinor[inext]].flag;

							// Let's distinguish several cases:
							if (prevflag == CROSSING_INS)
							{
								// endpt0 is THIRD * (prevpos + info.pos + opppos) 

								// move towards the position that is 2 previous --- ie the vertex above.
								// (Don't forget PBC.)
								int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
								f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
								if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
								if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;

								f64 r1 = prevprevpos.modulus();
								f64 r2 = endpt0.modulus();
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
								endpt0 = point;

							} else {
								// prevflag will say it is below ins. 
								// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
								info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);

							};

							if (nextflag == CROSSING_INS)
							{
								// We still want to move towards vertex above. But now it's 2 next
								// Don't forget PBC
								int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
								f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
								if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
								if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;
								 
								f64 r1 = nextnextpos.modulus();
								f64 r2 = endpt1.modulus();
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
								endpt1 = point;

							} else {
								// safest way: put point at the projection of our own position to insulator, maybe slightly off.
								info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);

							}
							f64 nTi_edge = p_n_minor[iMinor].n*shared_T[threadIdx.x].Ti;
							f64 nTe_edge = p_n_minor[iMinor].n*shared_T[threadIdx.x].Te;

							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;

							MAR_ion -= Make3(nTi_edge*over_m_i*edge_normal, 0.0);
							MAR_elec -= Make3(nTe_edge*over_m_e*edge_normal, 0.0);
							Our_integral_grad_Te += shared_T[threadIdx.x].Te * edge_normal;

							// will be a 0 contribution if endpt1 = endpt0, that's ok.
						};
					}; // domain triangle opposite or not
				} else {

					// Typical tri.
					MAR_ion -= Make3(0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal, 0.0);
					MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
					Our_integral_grad_Te += 0.5*(T0.Te + T1.Te) * edge_normal;

				}; 
				if (TESTTRI) {
					printf("pressure %d : contribs MAR_ion.x %1.11E MAR_elec.x %1.11E \n"
						"contribs MAR_ion.y %1.11E MAR_elec.y %1.11E \n"
						"n0 %1.10E n1 %1.10E Ti0 %1.10E Ti1 %1.10E edgenormal %1.9E %1.9E\n",
						iMinor,
						-0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal.x,
						-0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal.x,
						-0.5*(n0 * T0.Ti + n1 * T1.Ti)*over_m_i*edge_normal.y,
						-0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal.y,
						n0, n1, T0.Ti, T1.Ti, edge_normal.x, edge_normal.y);
				}

			
			//	if (Az_edge != Az_edge) {
			//		printf("GPU : %d : Az_edge %1.9E ourAz %1.9E oppAz %1.9E \n ourintegralgradTe %1.9E %1.9E contrib %1.9E %1.9E T01 %1.9E %1.9E edgenormal %1.9E %1.9E\n"
			//			"prevT.Te %1.9E ourT.Te %1.9E oppT.Te %1.9E nextT.Te %1.9E \n",
			//			iMinor, Az_edge, ourAz, oppAz,
			//	Our_integral_grad_Te.x, Our_integral_grad_Te.y,
			//			0.5*(T0.Te + T1.Te) * edge_normal.x, 0.5*(T0.Te + T1.Te) * edge_normal.y,
			//			T0.Te, T1.Te, edge_normal.x, edge_normal.y,
			//			prevT.Te, shared_T[threadIdx.x].Te,oppT.Te,nextT.Te
			//		);
			//	}

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
//
//				if ((TESTTRI)) 
//					printf("GPU AreaMinor %d : %1.14E from += %1.14E : endpt0.x %1.14E endpt1.x %1.14E edge_normal.x %1.14E\n"
//						"endpt1.y endpt0.y %1.14E %1.14E \n",
//					iMinor, AreaMinor, (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x,
//					endpt0.x, endpt1.x, edge_normal.x,
//						endpt1.y, endpt0.y);

				// See a way that FP accuracy was eroded: we take a difference of two close things already to get edge_normal.
				// can that be cleverly avoided? For all calcs?


				endpt0 = endpt1;
				n0 = n1;

				iprev = i;
				prevpos = opppos;
				prevAz = oppAz;
//				prevAzdot = oppAzdot;
				prevT = oppT;

				opppos = nextpos;
				oppAz = nextAz;
//				oppAzdot = nextAzdot;
				oppT = nextT; 
			};

			// No setting a_r = 0

			p_GradAz[iMinor] = Our_integral_grad_Az / AreaMinor_for_A;
			p_GradTe[iMinor] = Our_integral_grad_Te / AreaMinor;
			p_B[iMinor] = Make3(Our_integral_curl_Az / AreaMinor_for_A, BZ_CONSTANT);
		//	p_AreaMinor[iMinor] = AreaMinor;

			// No neutral stuff in this kernel, momrates should be set now:
			memcpy(p_MAR_ion + iMinor, &(MAR_ion), sizeof(f64_vec3));
			memcpy(p_MAR_elec + iMinor, &(MAR_elec), sizeof(f64_vec3));
		}
		else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================

		// 	We do not need B or Grad A outside of the domain. !

			iprev = 5; i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevAz = shared_Az[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevAz = shared_Az_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					AAdot temp = p_AAdot[izNeighMinor[iprev]];
					prevAz = temp.Az;
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				oppAz = shared_Az[izNeighMinor[i] - StartMinor];
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					oppAz = shared_Az_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					AAdot temp = p_AAdot[izNeighMinor[i]];
					oppAz = temp.Az;
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;


#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextAz = shared_Az[izNeighMinor[inext] - StartMinor];
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextAz = shared_Az_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						AAdot temp = p_AAdot[izNeighMinor[inext]];
						nextAz = temp.Az;
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:

				f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				//	Az_edge = SIXTH * (2.0*ourdata.Az + 2.0*oppAz + prevAz + nextAz);
				//	Our_integral_grad_Az += Az_edge * edge_normal;
				//	Our_integral_curl_Az += Az_edge * (endpt1 - endpt0); // looks anticlockwise

		//		integ_grad_Az.x = 0.5*(
		//			(ourAz + nextAz)*(info.pos.y - nextpos.y)
		//			+ (prevAz + ourAz)*(prevpos.y - info.pos.y)
		//			+ (oppAz + prevAz)*(opppos.y - prevpos.y)
		//			+ (nextAz + oppAz)*(nextpos.y - opppos.y)
		//			);
		//		integ_grad_Az.y = -0.5*( // notice minus
		//			(ourAz + nextAz)*(info.pos.x - nextpos.x)
		//			+ (prevAz + ourAz)*(prevpos.x - info.pos.x)
		//			+ (oppAz + prevAz)*(opppos.x - prevpos.x)
		//			+ (nextAz + oppAz)*(nextpos.x - opppos.x)
		//			);
		//		f64 area_quadrilateral = 0.5*(
		//			(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
		//			+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
		//			+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
		//			+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
		//			);

		//		//f64_vec2 grad_Az = integ_grad_Az / area_quadrilateral;
		////		if ((i % 2 == 0) || // vertex neigh 
		////				((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
		//		if ((opppos.dot(opppos) < 0.9999*0.9999*FRILL_CENTROID_OUTER_RADIUS_d*FRILL_CENTROID_OUTER_RADIUS_d) &&
		//			(opppos.dot(opppos) > 1.0001*1.0001*FRILL_CENTROID_INNER_RADIUS_d*FRILL_CENTROID_INNER_RADIUS_d))
		//			Our_integral_Lap_Az += integ_grad_Az.dot(edge_normal) / area_quadrilateral;

				f64 Az_edge = SIXTH * (2.0*ourAz + 2.0*oppAz + prevAz + nextAz);
				Our_integral_grad_Az += Az_edge * edge_normal;
				Our_integral_curl_Az -= Az_edge * (endpt1 - endpt0);

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				endpt0 = endpt1;

				prevpos = opppos;
				prevAz = oppAz;
			
				opppos = nextpos;
				oppAz = nextAz;
			
			};

			p_GradAz[iMinor] = Our_integral_grad_Az / AreaMinor;
		//	p_LapAz[iMinor] = Our_integral_Lap_Az / AreaMinor;
			p_B[iMinor] = Make3(Our_integral_curl_Az / AreaMinor, BZ_CONSTANT);
//			p_AreaMinor[iMinor] = AreaMinor;
		} // non-domain tri
	}; // was it FRILL

	   // Okay. While we have n_shards in memory we could proceed to overwrite with vxy.
	   // But get running first before using union and checking same.

}

__global__ void kernelCreate_momflux_minor(

	structural * __restrict__ p_info_minor,
	v4 * __restrict__ p_vie_minor,
	f64_vec2 * __restrict__ p_v_overall_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	f64_vec3 * __restrict__ p_MAR_neut,
	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor,
	NTrates * __restrict__ NT_addition_tri // inevitable
)
{
	__shared__ v4 shared_vie[threadsPerTileMinor];
	__shared__ f64_vec2 shared_v_overall[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];
	__shared__ v4 shared_vie_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_v_overall_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_vie[threadIdx.x] = p_vie_minor[iMinor];
	shared_v_overall[threadIdx.x] = p_v_overall_minor[iMinor];

	// Perhaps the real answer is this. Advection and therefore advective momflux
	// do not need to be recalculated very often at all. At 1e6 cm/s, we aim for 1 micron,
	// get 1e-10s to actually do the advection !!
	// So an outer cycle. Still limiting the number of total things in a minor tile. We might like 384 = 192*2.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if (info.flag == DOMAIN_VERTEX) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
			memcpy(&(shared_vie_verts[threadIdx.x]), &(p_vie_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(v4));
			shared_v_overall_verts[threadIdx.x] = p_v_overall_minor[iVertex + BEGINNING_OF_CENTRAL];
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
			memset(&(shared_vie_verts[threadIdx.x]), 0, sizeof(v4)); // this was always a bug as long as we had traffic near outermost!
			memset(&(shared_v_overall_verts[threadIdx.x]), 0, sizeof(f64_vec2)); // it actually is zero at outermost
			if (info.flag == OUTERMOST)
				memcpy(&(shared_vie_verts[threadIdx.x]), &(p_vie_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(v4));
		};
	};

	__syncthreads();

	v4 our_v, opp_v, prev_v, next_v;
	f64_vec2 our_v_overall, prev_v_overall, next_v_overall, opp_v_overall;
	f64_vec2 opppos, prevpos, nextpos;

	if (threadIdx.x < threadsPerTileMajor) {

		three_vec3 ownrates;
		memcpy(&(ownrates.ion), &(p_MAR_ion[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
		memcpy(&(ownrates.elec), &(p_MAR_elec[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		our_v = shared_vie_verts[threadIdx.x];
		our_v_overall = shared_v_overall_verts[threadIdx.x];

		if (info.flag == DOMAIN_VERTEX) {

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prev_v = shared_vie[izTri[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				prev_v = p_vie_minor[izTri[iprev]];
				prev_v_overall = p_v_overall_minor[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v.vxy = Clockwise_d*prev_v.vxy;
				prev_v_overall = Clockwise_d*prev_v_overall;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v.vxy = Anticlockwise_d*prev_v.vxy;
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opp_v = shared_vie[izTri[i] - StartMinor];
				opp_v_overall = shared_v_overall[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				opp_v = p_vie_minor[izTri[i]];
				opp_v_overall = p_v_overall_minor[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v.vxy = Clockwise_d*opp_v.vxy;
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v.vxy = Anticlockwise_d*opp_v.vxy;
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			// Think carefully: DOMAIN vertex cases for n,T ...
			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

			f64 vez0, viz0, vez1, viz1;
			f64_vec2 vxy0, vxy1, endpt1, edge_normal;

			short iend = tri_len;			
			// We deal with DOMAIN_VERTEX only!!

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					next_v = shared_vie[izTri[inext] - StartMinor];
					next_v_overall = shared_v_overall[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					next_v = p_vie_minor[izTri[inext]];
					next_v_overall = p_v_overall_minor[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v.vxy = Clockwise_d*next_v.vxy;
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v.vxy = Anticlockwise_d*next_v.vxy;
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				f64 n1;
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);

				// Assume neighs 0,1 are relevant to border with tri 0 minor.
				// *********
				// Verify that tri 0 is formed from our vertex, neigh 0 and neigh 1; - tick I think
				// *********
				
				vxy0 = THIRD * (our_v.vxy + prev_v.vxy + opp_v.vxy);
				vxy1 = THIRD * (our_v.vxy + opp_v.vxy + next_v.vxy);

				vez0 = THIRD * (our_v.vez + opp_v.vez + prev_v.vez);// not used?
				viz0 = THIRD * (our_v.viz + opp_v.viz + prev_v.viz);// not used?

				vez1 = THIRD * (our_v.vez + opp_v.vez + next_v.vez);// not used?
				viz1 = THIRD * (our_v.viz + opp_v.viz + next_v.viz); // not used?

				f64 relvnormal = 0.5*(vxy0 + vxy1
					- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
					- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
					).dot(edge_normal);

				// In reasonable conditions I suppose that is something sensible.
				// However if we use n v_edge relvnormal then from a fast upwind cell we are always ejecting the slowest material!
				// That is unstable.
				// We could profitably create a minmod model of velocity. 
				// However for now let's try pretending there is a shock front (so use average v for advection) and the upwind nv
				// to advect is just the upwind cell average.




				// FIX FOR NOW, 22/11/20 :
				// We do not allow traffic from insulator-crossing triangles to/from vertex minors.
				// This is because we can't have an intermediate cell of momentum within a density cell that has only one end.
				// ===========================================================================================================


				int neighflag = p_info_minor[izTri[i]].flag;
				if (neighflag == DOMAIN_TRIANGLE) {

					if (relvnormal > 0.0) {
						// losing stuff 
						ownrates.ion -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.viz);
						ownrates.elec -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.vez);

						if (TESTADVECTZ) {
							printf("GPUadvect %d izTri[%d] %d USING our vez %1.9E [ oppvez %1.9E ] relvnormaldot %1.9E \n"
								"gaining mom %1.9E | n0 %1.9E n1 %1.9E n %1.9E ncent %1.9E edge_normal %1.8E %1.8E vuse %1.8E %1.8E\n",
								iVertex, i, izTri[i], our_v.vez, opp_v.vez, relvnormal,
								-0.5*relvnormal*(n0 + n1)*our_v.vez, n0, n1, 0.5*(n0 + n1), shared_n_shards[threadIdx.x].n_cent,
								edge_normal.x, edge_normal.y,
								0.5*(vxy0 + vxy1
									- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
									- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))).x,
								0.5*(vxy0 + vxy1
									- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
									- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))).y);
						};
					}
					else {
						ownrates.ion -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.viz);
						ownrates.elec -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.vez);

						if (TESTADVECTZ) {
							printf("GPUadvect %d izTri[%d] %d our vez %1.9E [USING oppvez %1.9E ] relvnormaldot %1.9E \n"
								"gaining mom %1.9E | n0 %1.9E n1 %1.9E n %1.9E ncent %1.9E edge_normal %1.8E %1.8E vuse %1.8E %1.8E\n",
								VERTCHOSEN, i, izTri[i], our_v.vez, opp_v.vez, relvnormal,
								-0.5*relvnormal*(n0 + n1)*opp_v.vez, n0, n1, 0.5*(n0 + n1), shared_n_shards[threadIdx.x].n_cent,
								edge_normal.x, edge_normal.y,
								0.5*(vxy0 + vxy1
									- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
									- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))).x,
								0.5*(vxy0 + vxy1
									- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
									- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))).y);
						};

						//We are using upwind v ... however, n0 came from ourselves because we look out of our own minor into a triangle.

						// Why it's minus? : relvnormal was less than zero but we gain a positive amt of opp_v.
					};
				};
				// vie.vez = (vie_k.vez*Nk + h_use * MAR.z) / Nplus;

				// OLD, unstable :
				//ownrates.ion -= 0.5*relvnormal*(n0 *(Make3(vxy0 - our_v.vxy, viz0 - our_v.viz) + n1*(Make3(vxy1 - our_v.vxy, viz1 - our_v.viz))));
					
				if (TESTADVECT) {
					printf("GPUadvect %d izTri[%d] %d ownrates.ion.y %1.9E contrib.y %1.9E %1.9E [ours,>0 out,<0] \n"
							"relvnormal %1.10E n0 %1.9E n1 %1.9E vxy0.y %1.8E vxy1.y %1.8E\n"
							"edge_normal %1.8E %1.8E  our_v.y %1.8E opp_v.y %1.8E prev_v.y %1.8E next_v.y %1.8E\n",
								VERTCHOSEN, i, izTri[i], ownrates.ion.y,
								-0.5*relvnormal*(n0 + n1)*our_v.vxy.y, -0.5*relvnormal*(n0 + n1)*opp_v.vxy.y,
								relvnormal, n0, n1, vxy0.y, vxy1.y,
								edge_normal.x, edge_normal.y,
								our_v.vxy.y, opp_v.vxy.y, prev_v.vxy.y, next_v.vxy.y);
				};

				// ______________________________________________________
				//// whether the v that is leaving is greater than our v ..
				//// Formula:
				//// dv/dt = (d(Nv)/dt - dN/dt v) / N
				//// We include the divide by N when we enter the accel routine.

				// Somehow we've created an unstable situ. We are chucking out high-nv at the top. higher n and lower v than in our triangle.
				// Should we insist on upwind v as what is carried?
				// 
				
				endpt0 = endpt1;
				n0 = n1;

				prevpos = opppos;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;

				opppos = nextpos;
				opp_v = next_v;
				opp_v_overall = next_v_overall;
			}; // next i
			   // AreaMinor is not saved, or even calculated for tris.
			   // No neutral stuff in this kernel, momrates should be set now:
			memcpy(p_MAR_ion + iVertex + BEGINNING_OF_CENTRAL, &(ownrates.ion), sizeof(f64_vec3));
			memcpy(p_MAR_elec + iVertex + BEGINNING_OF_CENTRAL, &(ownrates.elec), sizeof(f64_vec3));
		} else {
			// NOT domain vertex: Do nothing
		};
	}; // was it domain vertex or Az-only
	   // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	   // __syncthreads(); // end of first vertex part
	   // Do we need syncthreads? Not overwriting any shared data here...

	   // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	our_v = shared_vie[threadIdx.x];
	our_v_overall = shared_v_overall[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	// Why the apparently stupid choice to make another variable? :
	three_vec3 ownrates_minor;
	memcpy(&(ownrates_minor.ion), &(p_MAR_ion[iMinor]), sizeof(f64_vec3));
	memcpy(&(ownrates_minor.elec), &(p_MAR_elec[iMinor]), sizeof(f64_vec3));

	f64 vez0, viz0, viz1, vez1;
	f64_vec2 vxy0, vxy1;
	if (TESTTRI) printf("iMinor %d info.flag %d \nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
		iMinor, info.flag);
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	}
	else {

		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			if (TESTTRI) printf("iMinor %d info.flag %d \nxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
				iMinor, info.flag);

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				memcpy(&prev_v, &(shared_vie[izNeighMinor[iprev] - StartMinor]), sizeof(v4));
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&prev_v, &(shared_vie_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
					prev_v_overall = shared_v_overall_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					memcpy(&prev_v, &(p_vie_minor[izNeighMinor[iprev]]), sizeof(v4));
					prev_v_overall = p_v_overall_minor[izNeighMinor[iprev]];
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v.vxy = Clockwise_d*prev_v.vxy;
				prev_v_overall = Clockwise_d*prev_v_overall;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v.vxy = Anticlockwise_d*prev_v.vxy;
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			}

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				memcpy(&opp_v, &(shared_vie[izNeighMinor[i] - StartMinor]), sizeof(v4));
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
				opp_v_overall = shared_v_overall[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&opp_v, &(shared_vie_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
					opp_v_overall = shared_v_overall_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					memcpy(&opp_v, &(p_vie_minor[izNeighMinor[i]]), sizeof(v4));
					opp_v_overall = p_v_overall_minor[izNeighMinor[i]];
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v.vxy = Clockwise_d*opp_v.vxy;
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v.vxy = Anticlockwise_d*opp_v.vxy;
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);
			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				// Worry about pathological cases later.
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));
					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				// Worry about pathological cases later.
				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					} else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				// Worry about pathological cases later.
				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;
				iprev = i - 1; if (iprev < 0) iprev = 5;
				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					memcpy(&next_v, &(shared_vie[izNeighMinor[inext] - StartMinor]), sizeof(v4));
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					next_v_overall = shared_v_overall[izNeighMinor[inext] - StartMinor];
				} else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						memcpy(&next_v, &(shared_vie_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
						next_v_overall = shared_v_overall_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						memcpy(&next_v, &(p_vie_minor[izNeighMinor[inext]]), sizeof(v4));
						next_v_overall = p_v_overall_minor[izNeighMinor[inext]];
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v.vxy = Clockwise_d*next_v.vxy;
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v.vxy = Anticlockwise_d*next_v.vxy;
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1, edge_normal;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-
				n0 = n_array[i];
				n1 = n_array[inext]; // 0,1 are either side of corner 0. What is seq of MinorNeigh ? tick
					 // Assume neighs 0,1 are relevant to border with tri 0 minor.
				
				vxy0 = THIRD * (our_v.vxy + prev_v.vxy + opp_v.vxy);
				vxy1 = THIRD * (our_v.vxy + opp_v.vxy + next_v.vxy);

				vez0 = THIRD * (our_v.vez + opp_v.vez + prev_v.vez);
				viz0 = THIRD * (our_v.viz + opp_v.viz + prev_v.viz);

				vez1 = THIRD * (our_v.vez + opp_v.vez + next_v.vez);
				viz1 = THIRD * (our_v.viz + opp_v.viz + next_v.viz); // Not used for anything, apparently.

				f64 relvnormal = 0.5*(vxy0 + vxy1
					- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
					- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
					).dot(edge_normal);

				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
					{
						// Note that average instead of upwind, is of course unstable.


						// FIX FOR NOW, 22/11/20 :
						// We do not allow traffic from insulator-crossing triangles to/from vertex minors.
						// This is because we can't have an intermediate cell of momentum within a density cell that has only one end.
						// ===========================================================================================================


						if (izNeighMinor[i] < BEGINNING_OF_CENTRAL) // triangle
						{

							if (relvnormal > 0.0) {
								// losing stuff n
								ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.viz);
								ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.vez);
							}
							else {
								ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.viz);
								ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.vez);
								// Why it's minus?
								// relvnormal was less than zero but we gain a positive amt of opp_v.
							};
						};


					} else {
						if (flag == CROSSING_INS) {
							// make the edge go from the upper point, down to the insulator.
							// Basically radius changes almost linearly as we move from endpt1 to endpt0.
							f64 r1 = endpt0.modulus();
							f64 r2 = endpt1.modulus();

							f64_vec2 v_overall0, v_overall1;
							v_overall0 = THIRD * (our_v_overall + prev_v_overall + opp_v_overall);
							v_overall1 = THIRD * (our_v_overall + next_v_overall + opp_v_overall);
							if (r1 > r2) {
								// 0 is higher								
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(endpt0 - endpt1);
								endpt1 = point; // use this value again later for AreaMinor if nothing else
							
								// endpt1 is defined in this way, so its motion must be defined accordingly.
								// The v_overall of the below-insulator point is actually 0.
								f64 r3 = nextpos.modulus();
								v_overall1 = ((DEVICE_RADIUS_INSULATOR_OUTER - r3) / (r1 - r3))*v_overall0;
								// but has no radial component:
								v_overall1 -= (v_overall1.dot(endpt1)) / (endpt1.dot(endpt1))*endpt1;
							} else {
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
								endpt0 = point;

								f64 r3 = prevpos.modulus();
								v_overall0 = ((DEVICE_RADIUS_INSULATOR_OUTER - r3) / (r2 - r3))*v_overall1;
								// but has no radial component:
								v_overall0 -= (v_overall0.dot(endpt0)) / (endpt0.dot(endpt0))*endpt1;
							};
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;


							// have not yet handled how to do momflux between two CROSSING_INS tris.
							// the above vxy1 etc will be invalid because of taking data from insulator points.

							// Does that mean we will get weird effects? Probably. Have to think here then.

							// Reset relvnormal:

							if (prev_v.vez == 0.0) vxy0 = 0.5*(our_v.vxy + opp_v.vxy);
							if (next_v.vez == 0.0) vxy1 = 0.5*(our_v.vxy + opp_v.vxy);
									//vxy0 = THIRD * (our_v.vxy + prev_v.vxy + opp_v.vxy);
							
							if (n0 == 0.0) // generated from shardmodel from inside the insulator, then it should come out 0.
								n0 = 0.5*(p_n_minor[iMinor].n + p_n_minor[izNeighMinor[i]].n);
							if (n1 == 0.0)
								n1 = 0.5*(p_n_minor[iMinor].n + p_n_minor[izNeighMinor[i]].n);

							relvnormal = 0.5*(vxy0 + vxy1 - v_overall0 - v_overall1).dot(edge_normal);

							if (relvnormal > 0.0) {
								ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.viz);
								ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.vez);
							} else {
								ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.viz);
								ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.vez);
							};

						} else {
							// Looking down into insulator. 
							 
							// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
							char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
							char nextflag = p_info_minor[izNeighMinor[inext]].flag;

							int debugprevflag = 0, debugnextflag = 0;
							f64_vec2 endpt0store, endpt1store;

							endpt0store = endpt0;
							endpt1store = endpt1;

							// Let's distinguish several cases:
							if (prevflag == CROSSING_INS)
							{
								int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
								f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
								if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
								if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;
								f64 r1 = prevprevpos.modulus();
								f64 r2 = endpt0.modulus();
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
								endpt0 = point;

								debugprevflag = 1;
							} else {
								// prevflag will say it is below ins. 
								// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
								info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);
							};

							if (nextflag == CROSSING_INS)
							{
								// We still want to move towards vertex above. But now it's 2 next
								int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
								f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
								if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
								if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;
								f64 r1 = nextnextpos.modulus();
								f64 r2 = endpt1.modulus();
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
								endpt1 = point;

								debugnextflag = 1;
							} else {
								// safest way: put point at the projection of our own position to insulator, maybe slightly off.
								info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);
							};
							// will be a 0 contribution if endpt1 = endpt0, that's ok.

							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; 
							// should be facing towards (0,0).
							
							// Insulator arc isn't moving, no v_overall.

							relvnormal = our_v.vxy.dot(edge_normal);

							if (relvnormal > 0.0) {
								f64 n_edge = p_n_minor[iMinor].n;

								// Only the vr component is reversed!!!
								// f64 vr = -our_v.vxy.dot(edge_normal) / edge_normal.modulus();								
								// rhat = -edge_normal/edge_normal.modulus();
								// v-= vr rhat 
								f64_vec2 vr_rhat = edge_normal*((our_v.vxy.dot(edge_normal)) /
											(edge_normal.dot(edge_normal)));
								// positive amt * negative r vector = negative amt * positive r vector.

								f64 vr_squared = our_v.vxy.dot(edge_normal)*our_v.vxy.dot(edge_normal) /
									edge_normal.dot(edge_normal);

								ownrates_minor.ion -= 2.0*relvnormal*n_edge*Make3(vr_rhat,0.0);
								ownrates_minor.elec -= 2.0*relvnormal*n_edge*Make3(vr_rhat, 0.0);

								// Now add heat:

								NTrates dNT = NT_addition_tri[iMinor];
								// change in 0.5 Nmvv = 0.5mv d/dt(Nv) = m*vr*vr*n_edge*relvnormal  since v dot vr rhat = vr^2
								// change in 1.5 NT should cancel this.
								dNT.NiTi += 0.6666666666667*m_i*vr_squared*n_edge*relvnormal;
								dNT.NeTe += 0.6666666666667*m_e*vr_squared*n_edge*relvnormal;
								
					//			printf("iMinor %d dNiTi %1.9E cont %1.9E vr_squared %1.9E n %1.8E relvn %1.8E our_v %1.8E %1.8E \n"
					//				"debugflags %d %d endpt0 %1.8E %1.8E endpt1 %1.8E %1.8E previously %1.8E %1.8E, %1.8E %1.8E edgenormal %1.8E %1.8E \n",
					//				iMinor, dNT.NiTi, 0.6666666666667*vr_squared*n_edge*relvnormal,
					//				vr_squared, n_edge, relvnormal, our_v.vxy.x, our_v.vxy.y,
					//				debugprevflag, debugnextflag, endpt0.x, endpt0.y, endpt1.x, endpt1.y,
					//				endpt0store.x, endpt0store.y, endpt1store.x, endpt1store.y, edge_normal.x, edge_normal.y);

								NT_addition_tri[iMinor] = dNT;
							};
							// If we are pulling away from the ins, do nothing!
						};
					};
				} else {

					// Typical edge.

					if (relvnormal > 0.0) {
						// losing stuff 
						ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.viz);
						ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(our_v.vxy, our_v.vez);
					}
					else {
						ownrates_minor.ion -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.viz);
						ownrates_minor.elec -= 0.5*relvnormal*(n0 + n1)*Make3(opp_v.vxy, opp_v.vez);
						// Why it's minus?
						// relvnormal was less than zero but we gain a positive amt of opp_v.
					};
				};
				
				if (((TESTTRI))) 
					printf("advectiveGPU %d i %d iznm %d info.flag %d neigh.flag %d  contrib %1.10E edge_nml %1.8E %1.8E\n"
						   "relvnormal %1.10E v_use %1.9E %1.9E n0 %1.12E n1 %1.12E our_vez %1.10E opp_vez %1.10E\n"
						   "~~~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&~&&~&~&~&~&~&~&~&~&~&~~~\n", 
							CHOSEN,i, izNeighMinor[i],     
						info.flag, p_info_minor[izNeighMinor[i]].flag,
							-0.5*relvnormal*(n0 + n1)*((relvnormal>0.0)?our_v.vez:opp_v.vez),
							edge_normal.x, edge_normal.y,
							relvnormal,
							(vxy0 + vxy1
								- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
								- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
								).x,
							(vxy0 + vxy1
								- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
								- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
								).y,
							n0, n1, our_v.vez, opp_v.vez);
						
				endpt0 = endpt1;

				prevpos = opppos;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;

				opppos = nextpos;
				opp_v = next_v;
				opp_v_overall = next_v_overall;
			};

			memcpy(&(p_MAR_ion[iMinor]), &(ownrates_minor.ion), sizeof(f64_vec3));
			memcpy(&(p_MAR_elec[iMinor]), &(ownrates_minor.elec), sizeof(f64_vec3));
		}
		else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================
		} // non-domain tri
	}; // was it FRILL
	
}


// Not optimized: !!
#define FACTOR_HALL (1.0/0.96)
#define FACTOR_PERP (1.2/0.96)
//#define DEBUGNANS


__global__ void kernelCalculate_deps_WRT_beta_Visc(
	f64 const hsub,
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_parallel_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_ita_parallel_elec_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_elec_minor,   // nT / nu ready to look up
	f64_vec3 * __restrict__ p_B_minor,

	nvals * __restrict__ p_n_minor, // got this
	f64 * __restrict__ p_AreaMinor, // got this -> N, Nn

	f64_vec3 * __restrict__ p_Jacobi_ion, 
	f64_vec3 * __restrict__ p_Jacobi_elec, 

	f64_vec3 * __restrict__ p_d_eps_by_d_beta_i_,
	f64_vec3 * __restrict__ p_d_eps_by_d_beta_e_
	)
{
	// We only need 3 in shared now, can re-do when we do elec
	__shared__ f64_vec3 shared_vJ[threadsPerTileMinor]; // sort of thing we want as input
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_B[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];

	__shared__ f64_vec3 shared_vJ_verts[threadsPerTileMajor]; // load & reload in Jacobi regressor v instead of v
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_B_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

	// Putting some stuff in shared may speed up if there are spills. !!

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64_vec3 our_v, opp_v, prev_v, next_v;
	f64_vec2 opppos, prevpos, nextpos;
	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!
	
	f64_vec3 d_eps_by_d_beta;
	
	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_vJ[threadIdx.x] = p_Jacobi_ion[iMinor];    // is memcpy faster or slower than operator= ?
	shared_B[threadIdx.x] = p_B_minor[iMinor].xypart();
	shared_ita_par[threadIdx.x] = p_ita_parallel_ion_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_ion_minor[iMinor];

	// Perhaps the real answer is this. Advection and therefore advective momflux
	// do not need to be recalculated very often at all. At 1e6 cm/s, we aim for 1 micron,
	// get 1e-10s to actually do the advection !!
	// So an outer cycle. Still limiting the number of total things in a minor tile. We might like 384 = 192*2.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_B_verts[threadIdx.x] = p_B_minor[iVertex + BEGINNING_OF_CENTRAL].xypart();
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
		{
			memcpy(&(shared_vJ_verts[threadIdx.x]), &(p_Jacobi_ion[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		}
		else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_vJ_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	// How shall we arrange to do v_n, which is isotropic? Handle this first...
	// Is the v_n coefficient negligible? Check.

	// We actually have to think how to handle the x-y dimension. PopOhms will handle it.

	// We can re-use some shared data -- such as pos and B -- to do both ions and electrons
	// But they use different ita_par and different vez, viz. 
	// Often we don't need to do magnetised ion viscosity when we do magnetised electron.

	// IONS FIRST:

	if (threadIdx.x < threadsPerTileMajor) {
	

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!

		if ((info.flag == DOMAIN_VERTEX) && (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{
			// We are losing energy if there is viscosity into OUTERMOST.
			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

			our_v = shared_vJ_verts[threadIdx.x]; // optimization: use replace or #define to get rid of storing this again.

			d_eps_by_d_beta = our_v; // eps = v_k+1 - v_k - h/N MAR

			// Rate of change of eps_x = Jacobi_x

			f64 Factor = hsub / (p_n_minor[iVertex + BEGINNING_OF_CENTRAL].n * p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] * m_ion);

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prev_v = shared_vJ[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				prev_v = p_Jacobi_ion[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				RotateClockwise(prev_v);
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				RotateAnticlockwise(prev_v);
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opp_v = shared_vJ[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				opp_v = p_Jacobi_ion[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				RotateClockwise(opp_v);
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				RotateAnticlockwise(opp_v);
			}

			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec3 omega_ci;

#pragma unroll 
			for (i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				// Now sort out anticlock vars:
				{
					f64_vec2 opp_B;
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_B = shared_B[izTri[i] - StartMinor];
						if (shared_ita_par_verts[threadIdx.x] < shared_ita_par[izTri[i] - StartMinor])
						{
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izTri[i] - StartMinor];
							nu = shared_nu[izTri[i] - StartMinor];
						};
					}
					else {
						opp_B = p_B_minor[izTri[i]].xypart();
						f64 ita_theirs = p_ita_parallel_ion_minor[izTri[i]];
						f64 nu_theirs = p_nu_ion_minor[izTri[i]];
						if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = ita_theirs;
							nu = nu_theirs;
						};
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				} 

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					next_v = shared_vJ[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					next_v = p_Jacobi_ion[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					RotateClockwise(next_v);
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					RotateAnticlockwise(next_v);
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);

				if (ita_par > 0.0) {

					// Order of calculations may help things to go out/into scope at the right times so careful with that.
					f64_vec2 gradvx, gradvy, gradviz;

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(our_v.x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + our_v.x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(our_v.x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + our_v.x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(our_v.y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + our_v.y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(our_v.y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + our_v.y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					//
					//				if (TEST) printf(
					//					"iVertex %d our_v.y next prev opp %1.8E %1.8E %1.8E %1.8E\n"
					//					"area_quad %1.8E \n"
					//					"info.pos %1.8E %1.8E opppos %1.8E %1.8E prev %1.8E %1.8E next %1.8E %1.8E\n",
					//					iVertex, our_v.vxy.y, next_v.vxy.y, prev_v.vxy.y, opp_v.vxy.y,
					//					area_quadrilateral,
					//					info.pos.x, info.pos.y, opppos.x, opppos.y, prevpos.x, prevpos.y, nextpos.x, nextpos.y);
					//
					gradviz.x = 0.5*(
						(our_v.z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + our_v.z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradviz.y = -0.5*(
						(our_v.z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + our_v.z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					if ((VISCMAG == 0) || (omega_ci.dot(omega_ci) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradviz.x);
						Pi_zy = -ita_par*(gradviz.y);

						f64_vec2 edge_normal;
						edge_normal.x = endpt1.y - endpt0.y;
						edge_normal.y = endpt0.x - endpt1.x;

						//visc_contrib.y = -over_m_i*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);

						d_eps_by_d_beta.x += Factor*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y); // - h/N visc_contrib I think
						d_eps_by_d_beta.y += Factor*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						d_eps_by_d_beta.z += Factor*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);

					}
					else {

						f64 omegamod;
						f64_vec3 unit_b, unit_perp, unit_Hall;
						{
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

							f64 omegasq = omega_ci.dot(omega_ci);
							omegamod = sqrt(omegasq);
							unit_b = omega_ci / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.

																 // store omegamod instead.
																 //	ita_perp = FACTOR_PERP * ita_par * nu*nu / (omegasq + nu*nu);
																 //	ita_cross = FACTOR_HALL * ita_par * nu*omegamod / (omegasq + nu*nu);
						}

						f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
																									// but we can make do with 3x partials
																									// 2. Now get partials in magnetic coordinates 
						{
							f64_vec3 intermed;

							// use: d vb / da = b transpose [ dvi/dxj ] a
							// Prototypical element: a.x b.y dvy/dx
							// b.x a.y dvx/dy

							intermed.x = unit_b.dotxy(gradvx);
							intermed.y = unit_b.dotxy(gradvy);
							intermed.z = unit_b.dotxy(gradviz);
							{
								f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

								dvb_by_db = unit_b.dot(intermed);
								dvperp_by_db = unit_perp.dot(intermed);
								dvHall_by_db = unit_Hall.dot(intermed);

								W_bb += 4.0*THIRD*dvb_by_db;
								W_bP += dvperp_by_db;
								W_bH += dvHall_by_db;
								W_PP -= 2.0*THIRD*dvb_by_db;
								W_HH -= 2.0*THIRD*dvb_by_db;
							}
							{
								f64 dvb_by_dperp, dvperp_by_dperp,
									dvHall_by_dperp;
								// Optimize by getting rid of different labels.

								intermed.x = unit_perp.dotxy(gradvx);
								intermed.y = unit_perp.dotxy(gradvy);
								intermed.z = unit_perp.dotxy(gradviz);

								dvb_by_dperp = unit_b.dot(intermed);
								dvperp_by_dperp = unit_perp.dot(intermed);
								dvHall_by_dperp = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvperp_by_dperp;
								W_PP += 4.0*THIRD*dvperp_by_dperp;
								W_HH -= 2.0*THIRD*dvperp_by_dperp;
								W_bP += dvb_by_dperp;
								W_PH += dvHall_by_dperp;
							}
							{
								f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

								intermed.x = unit_Hall.dotxy(gradvx);
								intermed.y = unit_Hall.dotxy(gradvy);
								intermed.z = unit_Hall.dotxy(gradviz);

								dvb_by_dHall = unit_b.dot(intermed);
								dvperp_by_dHall = unit_perp.dot(intermed);
								dvHall_by_dHall = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvHall_by_dHall;
								W_PP -= 2.0*THIRD*dvHall_by_dHall;
								W_HH += 4.0*THIRD*dvHall_by_dHall;
								W_bH += dvb_by_dHall;
								W_PH += dvperp_by_dHall;
							}
						}

						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							{
								f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

								Pi_b_b += -ita_par*W_bb;
								Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
								Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
								Pi_H_P += -ita_1*W_PH;
							}
							{
								f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_2*W_bP;
								Pi_H_b += -ita_2*W_bH;
							}
							{
								f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
								Pi_P_P -= ita_3*W_PH;
								Pi_H_H += ita_3*W_PH;
								Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
							}
							{
								f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_4*W_bH;
								Pi_H_b += ita_4*W_bP;
							}
						}

						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

																 // Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						// ownrates will be divided by N to give dv/dt
						// visc_contrib.z = over_m_i*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
						//	ownrates_visc += visc_contrib;

						d_eps_by_d_beta.x += -Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						d_eps_by_d_beta.y += -Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						d_eps_by_d_beta.z += -Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						// We should have created device function for the visc calc since it is repeated now at least 8 times.

						// Note that momflux here already had -, visc_contrib did not contain -over_m_i as for unmag.

					}
				}; // ita_par > 0.0

				// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);

				endpt0 = endpt1;
				prevpos = opppos;
				prev_v = opp_v;
				opppos = nextpos;
				opp_v = next_v;
			}; // next i
			memcpy(p_d_eps_by_d_beta_i_ + iVertex + BEGINNING_OF_CENTRAL, &d_eps_by_d_beta, sizeof(f64_vec3));
		} else {
			// NOT domain vertex: Do nothing			
		};
	};
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	// Ion , triangle:
	info = p_info_minor[iMinor];
	our_v = shared_vJ[threadIdx.x];
	d_eps_by_d_beta = our_v;

	//if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	{
		long izNeighMinor[6];
		char szPBC[6];

		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) && (shared_ita_par[threadIdx.x] > 0.0)) {

			f64 Factor = hsub / (p_n_minor[iMinor].n * p_AreaMinor[iMinor] * m_ion);

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				memcpy(&prev_v, &(shared_vJ[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&prev_v, &(shared_vJ_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					memcpy(&prev_v, &(p_Jacobi_ion[izNeighMinor[iprev]]), sizeof(f64_vec3));
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				RotateClockwise(prev_v);
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				RotateAnticlockwise(prev_v);
			}

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				memcpy(&opp_v, &(shared_vJ[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&opp_v, &(shared_vJ_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					memcpy(&opp_v, &(p_Jacobi_ion[izNeighMinor[i]]), sizeof(f64_vec3));
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				RotateClockwise(opp_v);
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				RotateAnticlockwise(opp_v);
			}

			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

			f64_vec3 omega_ci;
			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					memcpy(&next_v, &(shared_vJ[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						memcpy(&next_v, &(shared_vJ_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						memcpy(&next_v, &(p_Jacobi_ion[izNeighMinor[inext]]), sizeof(f64_vec3));
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					RotateClockwise(next_v);
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					RotateAnticlockwise(next_v);
				}

				//	nu = 1.0e10; // DEBUG
				bool bUsableSide = true;
				{
					f64_vec2 opp_B(0.0, 0.0);
					// newly uncommented:
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						opp_B = shared_B[izNeighMinor[i] - StartMinor];
						if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
						{
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						} else {
							ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
							nu = shared_nu[izNeighMinor[i] - StartMinor];
						};
						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							} else {
								ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							};
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							opp_B = p_B_minor[izNeighMinor[i]].xypart();
							f64 ita_par_opp = p_ita_parallel_ion_minor[izNeighMinor[i]];
							f64 nu_theirs = p_nu_ion_minor[izNeighMinor[i]];
							if (shared_ita_par[threadIdx.x] < ita_par_opp) {
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
							} else {
								ita_par = ita_par_opp;
								nu = nu_theirs;
							}
							if (ita_par_opp == 0.0) bUsableSide = false;
						}
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				if (bUsableSide) {
					// New definition of endpoint of minor edge:
					f64_vec2 gradvx, gradvy, gradviz;
					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);
					gradvx.x = 0.5*(
						(our_v.x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + our_v.x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(our_v.x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + our_v.x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(our_v.y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + our_v.y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(our_v.y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + our_v.y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradviz.x = 0.5*(
						(our_v.z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + our_v.z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradviz.y = -0.5*(
						(our_v.z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + our_v.z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;


					if ((VISCMAG == 0) || (omega_ci.dot(omega_ci) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradviz.x);
						Pi_zy = -ita_par*(gradviz.y);

						f64_vec2 edge_normal;
						edge_normal.x = endpt1.y - endpt0.y;
						edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

						d_eps_by_d_beta.x = Factor*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						d_eps_by_d_beta.y = Factor*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						d_eps_by_d_beta.z = Factor*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);

						// So we are saying if edge_normal.x > 0 and gradviz.x > 0
						// then Pi_zx < 0 then ownrates += a positive amount. That is correct.
					}
					else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

							f64 omegasq = omega_ci.dot(omega_ci);
							omegamod = sqrt(omegasq);
							unit_b = omega_ci / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
						{
							f64_vec3 intermed;

							// use: d vb / da = b transpose [ dvi/dxj ] a
							// Prototypical element: a.x b.y dvy/dx
							// b.x a.y dvx/dy

							intermed.x = unit_b.dotxy(gradvx);
							intermed.y = unit_b.dotxy(gradvy);
							intermed.z = unit_b.dotxy(gradviz);
							{
								f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

								dvb_by_db = unit_b.dot(intermed);
								dvperp_by_db = unit_perp.dot(intermed);
								dvHall_by_db = unit_Hall.dot(intermed);

								W_bb += 4.0*THIRD*dvb_by_db;
								W_bP += dvperp_by_db;
								W_bH += dvHall_by_db;
								W_PP -= 2.0*THIRD*dvb_by_db;
								W_HH -= 2.0*THIRD*dvb_by_db;
							}
							{
								f64 dvb_by_dperp, dvperp_by_dperp,
									dvHall_by_dperp;
								// Optimize by getting rid of different labels.

								intermed.x = unit_perp.dotxy(gradvx);
								intermed.y = unit_perp.dotxy(gradvy);
								intermed.z = unit_perp.dotxy(gradviz);

								dvb_by_dperp = unit_b.dot(intermed);
								dvperp_by_dperp = unit_perp.dot(intermed);
								dvHall_by_dperp = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvperp_by_dperp;
								W_PP += 4.0*THIRD*dvperp_by_dperp;
								W_HH -= 2.0*THIRD*dvperp_by_dperp;
								W_bP += dvb_by_dperp;
								W_PH += dvHall_by_dperp;
							}
							{
								f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

								intermed.x = unit_Hall.dotxy(gradvx);
								intermed.y = unit_Hall.dotxy(gradvy);
								intermed.z = unit_Hall.dotxy(gradviz);

								dvb_by_dHall = unit_b.dot(intermed);
								dvperp_by_dHall = unit_perp.dot(intermed);
								dvHall_by_dHall = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvHall_by_dHall;
								W_PP -= 2.0*THIRD*dvHall_by_dHall;
								W_HH += 4.0*THIRD*dvHall_by_dHall;
								W_bH += dvb_by_dHall;
								W_PH += dvperp_by_dHall;
							}
						}

						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							{
								f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

								Pi_b_b += -ita_par*W_bb;
								Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
								Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
								Pi_H_P += -ita_1*W_PH;
							}
							{
								f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_2*W_bP;
								Pi_H_b += -ita_2*W_bH;
							}
							{
								f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
								Pi_P_P -= ita_3*W_PH;
								Pi_H_H += ita_3*W_PH;
								Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
							}
							{
								f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_4*W_bH;
								Pi_H_b += ita_4*W_bP;
							}
						}

						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors
																 // Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;
							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}
						// visc_contrib.x = over_m_i*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						
						// Screen out looking out into insulator:
						// Not really needed since we did bUsableSide, but let's leave it in for now just to be delicate.						
						if (info.flag == CROSSING_INS) {
							char flag = p_info_minor[izNeighMinor[i]].flag;
							if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX)) {
								d_eps_by_d_beta.x -= Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
								d_eps_by_d_beta.y -= Factor*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
								d_eps_by_d_beta.z -= Factor*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
							} else {
								// DO NOTHING -- no additions
							}
						} else {
							d_eps_by_d_beta.x -= Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
							d_eps_by_d_beta.y -= Factor*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
							d_eps_by_d_beta.z -= Factor*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
						};

					}
				}; // bUsableSide

				endpt0 = endpt1;
				prevpos = opppos;
				prev_v = opp_v;
				opppos = nextpos;
				opp_v = next_v;
			};

			memcpy(&(p_d_eps_by_d_beta_i_[iMinor]), &(d_eps_by_d_beta), sizeof(f64_vec3));
		}
		else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		}
	} // scope

	__syncthreads();

	// Now do electron: overwrite ita and nu, copy-paste the above codes very carefully
	// OVERWRITE REGRESSOR

	shared_ita_par[threadIdx.x] = p_ita_parallel_elec_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_elec_minor[iMinor];
	shared_vJ[threadIdx.x] = p_Jacobi_elec[iMinor];

	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))  // keeping consistent with ion above where we did put OUTERMOST here
		{// but we set ita to 0 in the pre routine for outermost.
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_vJ_verts[threadIdx.x] = p_Jacobi_elec[iVertex + BEGINNING_OF_CENTRAL];
		} else {
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
			memset(&(shared_vJ_verts[threadIdx.x]), 0, sizeof(f64_vec3));
		};
	};

	__syncthreads();

	if (threadIdx.x < threadsPerTileMajor) {
		
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len; // ?!
		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		our_v = shared_vJ_verts[threadIdx.x]; // optimization: use replace or #define to get rid of storing this again.
		d_eps_by_d_beta = our_v;

		if ((info.flag == DOMAIN_VERTEX) && (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{

			f64 Factor = hsub / (p_n_minor[iVertex + BEGINNING_OF_CENTRAL].n * p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] * m_e);

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prev_v = shared_vJ[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			} else {
				prev_v = p_Jacobi_elec[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				RotateClockwise(prev_v);
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				RotateAnticlockwise(prev_v);
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opp_v = shared_vJ[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			} else {
				opp_v = p_Jacobi_elec[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				RotateClockwise(opp_v);
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				RotateAnticlockwise(opp_v);
			}

			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec3 omega_ce;
#pragma unroll 
			for (i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;
				
				{
					f64_vec2 opp_B;
					f64 opp_ita, opp_nu;
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_B = shared_B[izTri[i] - StartMinor];
						opp_ita = shared_ita_par[izTri[i] - StartMinor];
						opp_nu = shared_nu[izTri[i] - StartMinor];
						//ita_par = 0.5*(shared_ita_par_verts[threadIdx.x] + shared_ita_par[izTri[i] - StartMinor]);
						//nu = 0.5*(shared_nu_verts[threadIdx.x] + shared_nu[izTri[i] - StartMinor]);
					}
					else {
						opp_B = p_B_minor[izTri[i]].xypart();
						opp_ita = p_ita_parallel_elec_minor[izTri[i]];
						opp_nu = p_nu_elec_minor[izTri[i]];
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					if (shared_ita_par_verts[threadIdx.x] < opp_ita) {
						ita_par = shared_ita_par_verts[threadIdx.x];
						nu = shared_nu_verts[threadIdx.x];
					}
					else {
						ita_par = opp_ita;
						nu = opp_nu;
					}
					omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					next_v = shared_vJ[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					next_v = p_Jacobi_elec[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					RotateClockwise(next_v);
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					RotateAnticlockwise(next_v);
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);

				if (ita_par > 0.0) {
					// Order of calculations may help things to go out/into scope at the right times so careful with that.
					f64_vec2 gradvx, gradvy, gradvez;

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(our_v.x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + our_v.x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(our_v.x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + our_v.x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(our_v.y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + our_v.y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(our_v.y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + our_v.y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvez.x = 0.5*(
						(our_v.z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + our_v.z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvez.y = -0.5*(
						(our_v.z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + our_v.z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;


					if ((VISCMAG == 0) || (omega_ce.dot(omega_ce) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						// Let's suppose, Pi_yx means the rate of flow of y-momentum in the x direction.
						// Thus when we want to know how much y momentum is flowing through the wall we take
						// Pi_yx.edge_x + Pi_yy.edge_y -- reasonable.

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradvez.x);
						Pi_zy = -ita_par*(gradvez.y);

						f64_vec2 edge_normal;
						edge_normal.x = endpt1.y - endpt0.y;
						edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

						d_eps_by_d_beta.x += Factor*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						d_eps_by_d_beta.y += Factor*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						d_eps_by_d_beta.z += Factor*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);
					}
					else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

							f64 omegasq = omega_ce.dot(omega_ce);
							omegamod = sqrt(omegasq);
							unit_b = omega_ce / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
						{
							f64_vec3 intermed;
							// use: d vb / da = b transpose [ dvi/dxj ] a
							// Prototypical element: a.x b.y dvy/dx
							// b.x a.y dvx/dy
							intermed.x = unit_b.dotxy(gradvx);
							intermed.y = unit_b.dotxy(gradvy);
							intermed.z = unit_b.dotxy(gradvez);
							{
								f64 dvb_by_db, dvperp_by_db, dvHall_by_db;
								dvb_by_db = unit_b.dot(intermed);
								dvperp_by_db = unit_perp.dot(intermed);
								dvHall_by_db = unit_Hall.dot(intermed);

								W_bb += 4.0*THIRD*dvb_by_db;
								W_bP += dvperp_by_db;
								W_bH += dvHall_by_db;
								W_PP -= 2.0*THIRD*dvb_by_db;
								W_HH -= 2.0*THIRD*dvb_by_db;
							}
							{
								f64 dvb_by_dperp, dvperp_by_dperp,
									dvHall_by_dperp;
								// Optimize by getting rid of different labels.

								intermed.x = unit_perp.dotxy(gradvx);
								intermed.y = unit_perp.dotxy(gradvy);
								intermed.z = unit_perp.dotxy(gradvez);

								dvb_by_dperp = unit_b.dot(intermed);
								dvperp_by_dperp = unit_perp.dot(intermed);
								dvHall_by_dperp = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvperp_by_dperp;
								W_PP += 4.0*THIRD*dvperp_by_dperp;
								W_HH -= 2.0*THIRD*dvperp_by_dperp;
								W_bP += dvb_by_dperp;
								W_PH += dvHall_by_dperp;
							}
							{
								f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

								intermed.x = unit_Hall.dotxy(gradvx);
								intermed.y = unit_Hall.dotxy(gradvy);
								intermed.z = unit_Hall.dotxy(gradvez);

								dvb_by_dHall = unit_b.dot(intermed);
								dvperp_by_dHall = unit_perp.dot(intermed);
								dvHall_by_dHall = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvHall_by_dHall;
								W_PP -= 2.0*THIRD*dvHall_by_dHall;
								W_HH += 4.0*THIRD*dvHall_by_dHall;
								W_bH += dvb_by_dHall;
								W_PH += dvperp_by_dHall;
							}
						}

						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							{
								f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

								Pi_b_b += -ita_par*W_bb;
								Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
								Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
								Pi_H_P += -ita_1*W_PH;
							}
							{
								f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_2*W_bP;
								Pi_H_b += -ita_2*W_bH;
							}
							{
								f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
								Pi_P_P -= ita_3*W_PH;
								Pi_H_H += ita_3*W_PH;
								Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
							}
							{
								f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_4*W_bH;
								Pi_H_b += ita_4*W_bP;
							}
						}

						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;
							// Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;    // b component
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y; // P component
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y; // H component

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						d_eps_by_d_beta.x -= Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						d_eps_by_d_beta.y -= Factor*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
						d_eps_by_d_beta.z -= Factor*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
					};
				};

				endpt0 = endpt1;
				prevpos = opppos;
				prev_v = opp_v;
				opppos = nextpos;
				opp_v = next_v;
			}; // next i

			memcpy(p_d_eps_by_d_beta_e_ + iVertex + BEGINNING_OF_CENTRAL, &d_eps_by_d_beta, sizeof(f64_vec3));
		} else {
			// NOT domain vertex: Do nothing			
		};
	};
	
	// Electrons in tris:
	info = p_info_minor[iMinor];
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	}
	else {
		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) && (shared_ita_par[threadIdx.x] > 0.0)){

			our_v = shared_vJ[threadIdx.x];
			d_eps_by_d_beta = our_v;

			f64 Factor = hsub / (p_n_minor[iMinor].n * p_AreaMinor[iMinor] * m_e);

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				memcpy(&prev_v, &(shared_vJ[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&prev_v, &(shared_vJ_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					memcpy(&prev_v, &(p_Jacobi_elec[izNeighMinor[iprev]]), sizeof(f64_vec3));
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				RotateClockwise(prev_v);
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				RotateAnticlockwise(prev_v);
			}

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				memcpy(&opp_v, &(shared_vJ[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&opp_v, &(shared_vJ_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					memcpy(&opp_v, &(p_Jacobi_elec[izNeighMinor[i]]), sizeof(f64_vec3));
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				RotateClockwise(opp_v);
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				RotateAnticlockwise(opp_v);
			}

			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec3 omega_ce;
			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					memcpy(&next_v, &(shared_vJ[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						memcpy(&next_v, &(shared_vJ_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						memcpy(&next_v, &(p_Jacobi_elec[izNeighMinor[inext]]), sizeof(f64_vec3));
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					RotateClockwise(next_v);
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					RotateAnticlockwise(next_v);
				}

				bool bUsableSide = true;
				{
					f64_vec2 opp_B;
					f64 opp_ita, opp_nu;
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						opp_B = shared_B[izNeighMinor[i] - StartMinor];
						opp_ita = shared_ita_par[izNeighMinor[i] - StartMinor];
						opp_nu = shared_nu[izNeighMinor[i] - StartMinor];
						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							opp_ita = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							opp_nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							opp_B = p_B_minor[izNeighMinor[i]].xypart();
							opp_ita = p_ita_parallel_elec_minor[izNeighMinor[i]];
							opp_nu = p_nu_elec_minor[izNeighMinor[i]];
							if (opp_ita == 0.0) bUsableSide = false;
						}
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					if (shared_ita_par[threadIdx.x] < opp_ita) {
						ita_par = shared_ita_par[threadIdx.x];
						nu = shared_nu[threadIdx.x];
					}
					else {
						ita_par = opp_ita;
						nu = opp_nu;
					}
					omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);

				if (bUsableSide) {
					// New definition of endpoint of minor edge:
					f64_vec2 gradvez, gradvx, gradvy;

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(our_v.x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + our_v.x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(our_v.x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + our_v.x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(our_v.y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + our_v.y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(our_v.y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + our_v.y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvez.x = 0.5*(
						(our_v.z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + our_v.z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvez.y = -0.5*(
						(our_v.z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + our_v.z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;


					if ((VISCMAG == 0) || (omega_ce.dot(omega_ce) < 0.1*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradvez.x);
						Pi_zy = -ita_par*(gradvez.y);

						f64_vec2 edge_normal;
						edge_normal.x = endpt1.y - endpt0.y;
						edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

						if (info.flag == CROSSING_INS) {
							char flag = p_info_minor[izNeighMinor[i]].flag;
							if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX)) {

								d_eps_by_d_beta.x += Factor *(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
								d_eps_by_d_beta.y += Factor *(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
								d_eps_by_d_beta.z += Factor *(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);
							} else {
								// DO NOTHING
							}
						} else {
							d_eps_by_d_beta.x += Factor *(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
							d_eps_by_d_beta.y += Factor *(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
							d_eps_by_d_beta.z += Factor *(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);
						}
					} else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors

							f64 omegasq = omega_ce.dot(omega_ce);
							omegamod = sqrt(omegasq);
							unit_b = omega_ce / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
						{
							f64_vec3 intermed;

							// use: d vb / da = b transpose [ dvi/dxj ] a
							// Prototypical element: a.x b.y dvy/dx
							// b.x a.y dvx/dy

							intermed.x = unit_b.dotxy(gradvx);
							intermed.y = unit_b.dotxy(gradvy);
							intermed.z = unit_b.dotxy(gradvez);
							{
								f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

								dvb_by_db = unit_b.dot(intermed);
								dvperp_by_db = unit_perp.dot(intermed);
								dvHall_by_db = unit_Hall.dot(intermed);

								W_bb += 4.0*THIRD*dvb_by_db;
								W_bP += dvperp_by_db;
								W_bH += dvHall_by_db;
								W_PP -= 2.0*THIRD*dvb_by_db;
								W_HH -= 2.0*THIRD*dvb_by_db;
							}
							{
								f64 dvb_by_dperp, dvperp_by_dperp,
									dvHall_by_dperp;
								// Optimize by getting rid of different labels.

								intermed.x = unit_perp.dotxy(gradvx);
								intermed.y = unit_perp.dotxy(gradvy);
								intermed.z = unit_perp.dotxy(gradvez);

								dvb_by_dperp = unit_b.dot(intermed);
								dvperp_by_dperp = unit_perp.dot(intermed);
								dvHall_by_dperp = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvperp_by_dperp;
								W_PP += 4.0*THIRD*dvperp_by_dperp;
								W_HH -= 2.0*THIRD*dvperp_by_dperp;
								W_bP += dvb_by_dperp;
								W_PH += dvHall_by_dperp;
							}
							{
								f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

								intermed.x = unit_Hall.dotxy(gradvx);
								intermed.y = unit_Hall.dotxy(gradvy);
								intermed.z = unit_Hall.dotxy(gradvez);

								dvb_by_dHall = unit_b.dot(intermed);
								dvperp_by_dHall = unit_perp.dot(intermed);
								dvHall_by_dHall = unit_Hall.dot(intermed);

								W_bb -= 2.0*THIRD*dvHall_by_dHall;
								W_PP -= 2.0*THIRD*dvHall_by_dHall;
								W_HH += 4.0*THIRD*dvHall_by_dHall;
								W_bH += dvb_by_dHall;
								W_PH += dvperp_by_dHall;
							}
						}

						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							{
								f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

								Pi_b_b += -ita_par*W_bb;
								Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
								Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
								Pi_H_P += -ita_1*W_PH;
							}
							{
								f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_2*W_bP;
								Pi_H_b += -ita_2*W_bH;
							}
							{
								f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
								Pi_P_P -= ita_3*W_PH;
								Pi_H_H += ita_3*W_PH;
								Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
							}
							{
								f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
								Pi_P_b += -ita_4*W_bH;
								Pi_H_b += ita_4*W_bP;
							}
						}

						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
							f64_vec2 edge_normal;
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x; // need to define so as to create unit vectors
																 // Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						// unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall
						// is the flow of p_x dotted with the edge_normal
						// ownrates will be divided by N to give dv/dt
						// m N dvx/dt = integral div momflux_x
						// Therefore divide here just by m
						
						
						if (info.flag == CROSSING_INS) {
							char flag = p_info_minor[izNeighMinor[i]].flag;
							if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX)) {
								d_eps_by_d_beta.x -= Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
								d_eps_by_d_beta.y -= Factor*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
								d_eps_by_d_beta.z -= Factor*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
							} else {
								// DO NOTHING
							}
						} else {
							d_eps_by_d_beta.x -= Factor*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
							d_eps_by_d_beta.y -= Factor*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
							d_eps_by_d_beta.z -= Factor*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);
						}
					}
				}; // bUsableSide

				endpt0 = endpt1;
				prevpos = opppos;
				prev_v = opp_v;
				opppos = nextpos;
				opp_v = next_v;
			};
			memcpy(&(p_d_eps_by_d_beta_e_[iMinor]), &(d_eps_by_d_beta), sizeof(f64_vec3));
		}
		else {
			// Not domain, not crossing_ins, not a frill			
		} // non-domain tri
	}; // was it FRILL
}

__global__ void
// __launch_bounds__(128) -- manual says that if max is less than 1 block, kernel launch will fail. Too bad huh.
kernelCreate_viscous_contrib_to_MAR_and_NT(

	structural * __restrict__ p_info_minor,
	v4 * __restrict__ p_vie_minor,
	
	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_parallel_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_ita_parallel_elec_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_ion_minor,   // nT / nu ready to look up
	f64 * __restrict__ p_nu_elec_minor,   // nT / nu ready to look up
	f64_vec3 * __restrict__ p_B_minor,

	f64_vec3 * __restrict__ p_MAR_ion,
	f64_vec3 * __restrict__ p_MAR_elec,
	NTrates * __restrict__ p_NT_addition_rate,
	NTrates * __restrict__ p_NT_addition_tri)
{
	__shared__ v4 shared_vie[threadsPerTileMinor]; // sort of thing we want as input
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_B[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];
	
	__shared__ v4 shared_vie_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_B_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

	// 4+2+2+1+1 *1.5 = 15 per thread. That is possibly as slow as having 24 per thread. 
	// Thus putting some stuff in shared may speed up if there are spills.
	
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!
	f64_vec3 ownrates_visc;
	f64 visc_htg;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_vie[threadIdx.x] = p_vie_minor[iMinor];
	shared_B[threadIdx.x] = p_B_minor[iMinor].xypart();
	shared_ita_par[threadIdx.x] = p_ita_parallel_ion_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_ion_minor[iMinor];

	// Perhaps the real answer is this. Advection and therefore advective momflux
	// do not need to be recalculated very often at all. At 1e6 cm/s, we aim for 1 micron,
	// get 1e-10s to actually do the advection !!
	// So an outer cycle. Still limiting the number of total things in a minor tile. We might like 384 = 192*2.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_B_verts[threadIdx.x] = p_B_minor[iVertex + BEGINNING_OF_CENTRAL].xypart();
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) 
		{
			memcpy(&(shared_vie_verts[threadIdx.x]), &(p_vie_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(v4));
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_ion_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_vie_verts[threadIdx.x]), 0, sizeof(v4));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};
	
	__syncthreads();

	// How shall we arrange to do v_n, which is isotropic? Handle this first...
	// Is the v_n coefficient negligible? Check.

	// We actually have to think how to handle the x-y dimension. PopOhms will handle it.
	
	// We can re-use some shared data -- such as pos and B -- to do both ions and electrons
	// But they use different ita_par and different vez, viz. 
	// Often we don't need to do magnetised ion viscosity when we do magnetised electron.
	
	// IONS FIRST:

	if (threadIdx.x < threadsPerTileMajor) {
		memset(&ownrates_visc, 0, sizeof(f64_vec3));
		visc_htg = 0.0;

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!
		
		if ((info.flag == DOMAIN_VERTEX) && (shared_ita_par_verts[threadIdx.x] > 0.0) )
			//|| (info.flag == OUTERMOST)) 
		{
			// We are losing energy if there is viscosity into OUTERMOST.
			
			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));
						
			// DROP THIS ONE.

	//		f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			// short iend = tri_len;
			//f64_vec2 projendpt0;
			//if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
			//	iend = tri_len - 2;
			//	if (info.flag == OUTERMOST) {
			//		endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
			//	}
			//	else {
			//		endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
			//	}
			//	edge_normal.x = endpt0.y - projendpt0.y;
			//	edge_normal.y = projendpt0.x - endpt0.x;
			//	AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
			//};
						
#pragma unroll 
			for (short i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think

				f64_vec2 gradvx, gradvy, gradviz;
				f64_vec3 htg_diff;
				f64_vec2 edge_normal;

				// Order of calculations may help things to go out/into scope at the right times so careful with that.
				//f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				// we also want to get nu from somewhere. So precompute nu at the time we precompute ita_e = n Te / nu_e, ita_i = n Ti / nu_i. 

				f64_vec3 omega_ci;
				{
					f64_vec2 opp_B;
					f64 ita_theirs, nu_theirs;
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_B = shared_B[izTri[i] - StartMinor];
						ita_theirs = shared_ita_par[izTri[i] - StartMinor];
						nu_theirs = shared_nu[izTri[i] - StartMinor];
					} else {
						opp_B = p_B_minor[izTri[i]].xypart();
						ita_theirs = p_ita_parallel_ion_minor[izTri[i]];
						nu_theirs = p_nu_ion_minor[izTri[i]];						
					};

					if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
						ita_par = shared_ita_par_verts[threadIdx.x];
						nu = shared_nu_verts[threadIdx.x];
					} else {
						ita_par = ita_theirs;
						nu = nu_theirs;
					};

					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				} // Guaranteed DOMAIN_VERTEX never needs to skip an edge; we include CROSSING_INS in viscosity.

				if (ita_par > 0.0) 
				{
					v4 opp_v, prev_v, next_v;
					f64_vec2 opppos, prevpos, nextpos;
					// ideally we might want to leave position out of the loop so that we can avoid reloading it.

					short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
					short inext = i + 1; if (inext >= tri_len) inext = 0;

					if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
					{
						prev_v = shared_vie[izTri[iprev] - StartMinor];
						prevpos = shared_pos[izTri[iprev] - StartMinor];
					}
					else {
						prev_v = p_vie_minor[izTri[iprev]];
						prevpos = p_info_minor[izTri[iprev]].pos;
					}
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						prev_v.vxy = Clockwise_d*prev_v.vxy;
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						prev_v.vxy = Anticlockwise_d*prev_v.vxy;
					}

					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_v = shared_vie[izTri[i] - StartMinor];
						opppos = shared_pos[izTri[i] - StartMinor];
					}
					else {
						opp_v = p_vie_minor[izTri[i]];
						opppos = p_info_minor[izTri[i]].pos;
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						opp_v.vxy = Clockwise_d*opp_v.vxy;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						opp_v.vxy = Anticlockwise_d*opp_v.vxy;
					}

					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						next_v = shared_vie[izTri[inext] - StartMinor];
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						next_v = p_vie_minor[izTri[inext]];
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						next_v.vxy = Clockwise_d*next_v.vxy;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						next_v.vxy = Anticlockwise_d*next_v.vxy;
					}

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);

					gradvx.x = 0.5*(
						(shared_vie_verts[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.x + shared_vie_verts[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_vie_verts[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.x + shared_vie_verts[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_vie_verts[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.y + shared_vie_verts[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_vie_verts[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.y + shared_vie_verts[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					
					gradviz.x = 0.5*(
						(shared_vie_verts[threadIdx.x].viz + next_v.viz)*(info.pos.y - nextpos.y)
						+ (prev_v.viz + shared_vie_verts[threadIdx.x].viz)*(prevpos.y - info.pos.y)
						+ (opp_v.viz + prev_v.viz)*(opppos.y - prevpos.y)
						+ (next_v.viz + opp_v.viz)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradviz.y = -0.5*(
						(shared_vie_verts[threadIdx.x].viz + next_v.viz)*(info.pos.x - nextpos.x)
						+ (prev_v.viz + shared_vie_verts[threadIdx.x].viz)*(prevpos.x - info.pos.x)
						+ (opp_v.viz + prev_v.viz)*(opppos.x - prevpos.x)
						+ (next_v.viz + opp_v.viz)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					if (TESTIONVERTVISC) printf(
						"iVertex %d area_quad %1.8E \n"
						"our_v.x next prev opp %1.8E %1.8E %1.8E %1.8E gradvx %1.8E %1.8E\n"
						"our_v.y next prev opp %1.8E %1.8E %1.8E %1.8E gradvy %1.8E %1.8E\n"
						"our_v.z next prev opp %1.8E %1.8E %1.8E %1.8E gradvz %1.8E %1.8E\n"
						"info.pos %1.8E %1.8E opppos %1.8E %1.8E prev %1.8E %1.8E next %1.8E %1.8E\n",
						iVertex, area_quadrilateral,
						shared_vie_verts[threadIdx.x].vxy.x, next_v.vxy.x, prev_v.vxy.x, opp_v.vxy.x,
						gradvx.x, gradvx.y,
						shared_vie_verts[threadIdx.x].vxy.y, next_v.vxy.y, prev_v.vxy.y, opp_v.vxy.y,
						gradvy.x, gradvy.y,
						shared_vie_verts[threadIdx.x].viz, next_v.viz, prev_v.viz, opp_v.viz,
						gradviz.x, gradviz.y,
						info.pos.x, info.pos.y, opppos.x, opppos.y, prevpos.x, prevpos.y, nextpos.x, nextpos.y);

					htg_diff.x = shared_vie_verts[threadIdx.x].vxy.x - opp_v.vxy.x;
					htg_diff.y = shared_vie_verts[threadIdx.x].vxy.y - opp_v.vxy.y;
					htg_diff.z = shared_vie_verts[threadIdx.x].viz - opp_v.viz;
				}

				if (ita_par > 0.0) {
					if ((VISCMAG == 0) || (omega_ci.dot(omega_ci) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradviz.x);
						Pi_zy = -ita_par*(gradviz.y);

						f64_vec3 visc_contrib;
						visc_contrib.x = -over_m_i*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						visc_contrib.y = -over_m_i*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						visc_contrib.z = -over_m_i*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);

						//if (info.flag == OUTERMOST) {
						//	if (p_info_minor[izTri[i]].flag == DOMAIN_TRIANGLE)
						//	{
						//		ownrates_visc += visc_contrib;
						//		visc_htg += -THIRD*m_ion*(
						//			(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
						//			+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
						//			+ (our_v.viz - opp_v.viz)*visc_contrib.z);
						//		// do not look into frill
						//	}
						//	else {
						//		visc_contrib.x = 0.0; visc_contrib.y = 0.0; visc_contrib.z = 0.0;
						//	}
						//} else
						{
							ownrates_visc += visc_contrib;

							visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
							//	(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
							//	+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
							//	+ (our_v.viz - opp_v.viz)*visc_contrib.z);
						}

						//					
						//					if (TEST)
						//						printf("iVertex %d tri %d ION ita_par %1.9E \n"
						//							"gradvx %1.8E %1.8E gradvy %1.8E %1.8E gradvz %1.8E %1.8E\n"
						//							"edgenormal %1.8E %1.8E  opp_viz %1.10E our_viz %1.10E\n"
						//							"ourpos %1.8E %1.8E opp pos %1.8E %1.8E\n"
						//							"Pi_xx %1.8E xy %1.8E yy %1.8E zx %1.8E\n"
						//							"visc_contrib %1.9E %1.9E %1.9E visc_htg %1.10E\n"
						//							"===\n",
						//							iVertex, izTri[i], ita_par, gradvx.x, gradvx.y, gradvy.x, gradvy.y, 
						//							gradviz.x, gradviz.y,
						//							edge_normal.x, edge_normal.y, opp_v.viz, our_v.viz,
						//							info.pos.x,info.pos.y, opppos.x,opppos.y,
						//							Pi_xx, Pi_xy, Pi_yy, Pi_zx,
						//							visc_contrib.x, visc_contrib.y, visc_contrib.z, visc_htg
						//						);
						////
											// So we are saying if edge_normal.x > 0 and gradviz.x > 0
											// then Pi_zx < 0 then ownrates += a positive amount. That is correct.
					}
					else {

						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
							// but we can make do with 3x partials
							// 2. Now get partials in magnetic coordinates 

							f64 omegamod;
							{
								//f64_vec2 edge_normal;
								//edge_normal.x = THIRD * (nextpos.y - prevpos.y);
								//edge_normal.y = THIRD * (prevpos.x - nextpos.x);

								f64 omegasq = omega_ci.dot(omega_ci);
								omegamod = sqrt(omegasq);
								unit_b = omega_ci / omegamod;
								unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
								unit_perp = unit_perp / unit_perp.modulus();
								unit_Hall = unit_b.cross(unit_perp); // Note sign.

								// We picked edge_normal to be unit_perp.
								// Is that at all valid?

								// It seems like an arbitrary choice. Since B is in the plane, it's saying we picked perp in the plane, H = z.

																	 // store omegamod instead.
																	 //	ita_perp = FACTOR_PERP * ita_par * nu*nu / (omegasq + nu*nu);
																	 //	ita_cross = FACTOR_HALL * ita_par * nu*omegamod / (omegasq + nu*nu);
							}
							{
								f64_vec3 intermed;

								// use: d vb / da = b transpose [ dvi/dxj ] a
								// Prototypical element: a.x b.y dvy/dx
								// b.x a.y dvx/dy

								intermed.x = unit_b.dotxy(gradvx);
								intermed.y = unit_b.dotxy(gradvy);
								intermed.z = unit_b.dotxy(gradviz);
								{
									f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

									dvb_by_db = unit_b.dot(intermed);
									dvperp_by_db = unit_perp.dot(intermed);
									dvHall_by_db = unit_Hall.dot(intermed);

									W_bb += 4.0*THIRD*dvb_by_db;
									W_bP += dvperp_by_db;
									W_bH += dvHall_by_db;
									W_PP -= 2.0*THIRD*dvb_by_db;
									W_HH -= 2.0*THIRD*dvb_by_db;

									if (TESTIONVERTVISC)
										printf("dvperp_by_db %1.8E W_bP %1.8E \n",
											dvperp_by_db, W_bP);
									if (TESTIONVERTVISC)
										printf("dvHall_by_db %1.8E W_bH %1.8E \n",
											dvHall_by_db, W_bH);
								}
								{
									f64 dvb_by_dperp, dvperp_by_dperp,
										dvHall_by_dperp;
									// Optimize by getting rid of different labels.

									intermed.x = unit_perp.dotxy(gradvx);
									intermed.y = unit_perp.dotxy(gradvy);
									intermed.z = unit_perp.dotxy(gradviz);

									dvb_by_dperp = unit_b.dot(intermed);
									dvperp_by_dperp = unit_perp.dot(intermed);
									dvHall_by_dperp = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvperp_by_dperp;
									W_PP += 4.0*THIRD*dvperp_by_dperp;
									W_HH -= 2.0*THIRD*dvperp_by_dperp;
									W_bP += dvb_by_dperp;
									W_PH += dvHall_by_dperp;

									if (TESTIONVERTVISC)
										printf("dvb_by_dperp %1.8E W_bP %1.8E \n",
											dvb_by_dperp, W_bP);
								}
								{
									f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

									intermed.x = unit_Hall.dotxy(gradvx);
									intermed.y = unit_Hall.dotxy(gradvy);
									intermed.z = unit_Hall.dotxy(gradviz);

									dvb_by_dHall = unit_b.dot(intermed);
									dvperp_by_dHall = unit_perp.dot(intermed);
									dvHall_by_dHall = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvHall_by_dHall;
									W_PP -= 2.0*THIRD*dvHall_by_dHall;
									W_HH += 4.0*THIRD*dvHall_by_dHall;
									W_bH += dvb_by_dHall;
									W_PH += dvperp_by_dHall;
									if (TESTIONVERTVISC)
										printf("dvb_by_dHall %1.8E W_bH %1.8E \n",
											dvb_by_dHall, W_bH);
								}
							}

							{
								{
									f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

									Pi_b_b += -ita_par*W_bb;
									Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
									Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
									Pi_H_P += -ita_1*W_PH;
								}
								{
									f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_2*W_bP;
									Pi_H_b += -ita_2*W_bH;

									if (TESTIONVERTVISC) 
										printf(" -ita_2 %1.8E W_bP %1.8E contrib %1.8E Pi_P_b %1.8E \n",
											-ita_2, W_bP, -ita_2*W_bP, Pi_P_b);
								}
								{
									f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
									Pi_P_P -= ita_3*W_PH;
									Pi_H_H += ita_3*W_PH;
									Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
								}
								{
									f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_4*W_bH;

									if (TESTIONVERTVISC)
										printf(" -ita_4 %1.8E W_bH %1.8E contrib %1.8E Pi_P_b %1.8E nu %1.8E omega %1.8E \n",
											-ita_4, W_bH, -ita_4*W_bH, Pi_P_b, nu, omegamod);

									Pi_H_b += ita_4*W_bP;
								}
							}

						} // scope W

						// All we want left over at this point is Pi .. and unit_b

						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							// Most efficient way: compute mom flux in magnetic coords
							f64_vec3 mag_edge;
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						// unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall
						// is the flow of p_x dotted with the edge_normal
						// ownrates will be divided by N to give dv/dt
						// m N dvx/dt = integral div momflux_x
						// Therefore divide here just by m

						f64_vec3 visc_contrib;
						visc_contrib.x = over_m_i*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						visc_contrib.y = over_m_i*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
						visc_contrib.z = over_m_i*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);

						//if (info.flag == OUTERMOST) {
						//	if (p_info_minor[izTri[i]].flag == DOMAIN_TRIANGLE)	{
						//		ownrates_visc += visc_contrib;

						//		visc_htg += -TWOTHIRDS*m_ion*(
						//			(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
						//			+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
						//			+ (our_v.viz - opp_v.viz)*visc_contrib.z);  // Claim all visc htg for this vertcell
						//	}
						//} else 
						{
							ownrates_visc += visc_contrib;

							visc_htg += -TWOTHIRDS*m_ion*(htg_diff.dot(visc_contrib));
							//							(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
								//						+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
									//					+ (our_v.viz - opp_v.viz)*visc_contrib.z);  // Claim all visc htg for this vertcell
						}


						if (TESTIONVERTVISC) {
							printf("iVertex %d tri %d ION ita_par %1.9E omega %1.9E %1.9E %1.9E nu %1.9E ourpos %1.8E %1.8E \n"
								"unit_b %1.8E %1.8E %1.8E unit_perp %1.8E %1.8E %1.8E unit_Hall %1.8E %1.8E %1.8E\n"
								"Pi_b_b %1.8E Pi_P_b %1.8E Pi_P_P %1.8E Pi_H_b %1.8E Pi_H_P %1.8E Pi_H_H %1.8E\n"
								"momflux b %1.8E perp %1.8E cross %1.8E visc_contrib %1.9E %1.9E %1.9E \n",
								iVertex, izTri[i], ita_par, omega_ci.x, omega_ci.y, omega_ci.z, nu,
								info.pos.x, info.pos.y,
								unit_b.x, unit_b.y, unit_b.z, unit_perp.x, unit_perp.y, unit_perp.z, unit_Hall.x, unit_Hall.y, unit_Hall.z,
								Pi_b_b, Pi_P_b, Pi_P_P, Pi_H_b, Pi_H_P, Pi_H_H,
								momflux_b, momflux_perp, momflux_Hall,
								visc_contrib.x, visc_contrib.y, visc_contrib.z
							); 
							printf(
								"htgdiff %1.10E %1.10E %1.10E htg %1.10E \n===========================\n",
								htg_diff.x,
								htg_diff.y,
								htg_diff.z,
								-TWOTHIRDS*m_ion*(htg_diff.dot(visc_contrib))
							);
						}
						//	

					}
				}; // was ita_par == 0

				// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);

				// Just leaving these but they won't do anything :
				//prevpos = opppos;
				//prev_v = opp_v;
				//opppos = nextpos;
				//opp_v = next_v;				
			}; // next i
			
			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_ion[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
		//	if (TEST) 
		//		printf("%d ion ownrates %1.8E %1.8E %1.8E ownrates_visc %1.8E %1.8E %1.8E our_v %1.8E %1.8E %1.8E\n",
		//		iVertex, ownrates.x, ownrates.y, ownrates.z, ownrates_visc.x, ownrates_visc.y, ownrates_visc.z, our_v.vxy.x, our_v.vxy.y, our_v.viz);
			ownrates += ownrates_visc;
			memcpy(p_MAR_ion + iVertex + BEGINNING_OF_CENTRAL, &ownrates, sizeof(f64_vec3));
			
			p_NT_addition_rate[iVertex].NiTi += visc_htg;
			
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iVertex %d NaN ownrates.x\n",iVertex);
			if (ownrates.y != ownrates.y)
				printf("iVertex %d NaN ownrates.y\n", iVertex);
			if (ownrates.z != ownrates.z)
				printf("iVertex %d NaN ownrates.z\n", iVertex);			
			if (visc_htg != visc_htg) printf("iVertex %d NAN VISC HTG\n", iVertex);
#endif

		} else {
			// NOT domain vertex: Do nothing			
		};
	}; 
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...
	
	info = p_info_minor[iMinor];
	memset(&ownrates_visc, 0, sizeof(f64_vec3));
	visc_htg = 0.0;
	{	
		long izNeighMinor[6];
		char szPBC[6];

		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) 
			&& (shared_ita_par[threadIdx.x] > 0.0)){

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
									
			f64_vec3 omega_ci;
			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (short i = 0; i < 6; i++)
			{	
				if (TESTIONVISC) printf("start loop %d: ownrates.x %1.9E", i, ownrates_visc.x);

				bool bUsableSide = true;
				{
					f64_vec2 opp_B(0.0, 0.0);
					// newly uncommented:
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						opp_B = shared_B[izNeighMinor[i] - StartMinor];
						if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
						{
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
							nu = shared_nu[izNeighMinor[i] - StartMinor];
						};

						// USEFUL:
						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							};

							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							opp_B = p_B_minor[izNeighMinor[i]].xypart();
							f64 ita_par_opp = p_ita_parallel_ion_minor[izNeighMinor[i]];
							f64 nu_theirs = p_nu_ion_minor[izNeighMinor[i]];
							if (shared_ita_par[threadIdx.x] < ita_par_opp) {
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
							}
							else {
								ita_par = ita_par_opp;
								nu = nu_theirs;
							}

							if (ita_par_opp == 0.0) bUsableSide = false;
						}
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					omega_ci = 0.5*qoverMc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}

				f64_vec2 gradvx, gradvy, gradviz;
				f64_vec2 edge_normal;
				f64_vec3 htg_diff;

				if (bUsableSide)
				{
					short inext = i + 1; if (inext > 5) inext = 0;
					short iprev = i - 1; if (iprev < 0) iprev = 5;
					v4 prev_v, opp_v, next_v;
					f64_vec2 prevpos, nextpos, opppos;

					if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
					{
						memcpy(&prev_v, &(shared_vie[izNeighMinor[iprev] - StartMinor]), sizeof(v4));
						prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
					}
					else {
						if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&prev_v, &(shared_vie_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							prevpos = p_info_minor[izNeighMinor[iprev]].pos;
							memcpy(&prev_v, &(p_vie_minor[izNeighMinor[iprev]]), sizeof(v4));
						};
					};
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						prev_v.vxy = Clockwise_d*prev_v.vxy;
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						prev_v.vxy = Anticlockwise_d*prev_v.vxy;
					}

					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						memcpy(&opp_v, &(shared_vie[izNeighMinor[i] - StartMinor]), sizeof(v4));
						opppos = shared_pos[izNeighMinor[i] - StartMinor];
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&opp_v, &(shared_vie_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							opppos = p_info_minor[izNeighMinor[i]].pos;
							memcpy(&opp_v, &(p_vie_minor[izNeighMinor[i]]), sizeof(v4));
						};
					};
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						opp_v.vxy = Clockwise_d*opp_v.vxy;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						opp_v.vxy = Anticlockwise_d*opp_v.vxy;
					}

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						memcpy(&next_v, &(shared_vie[izNeighMinor[inext] - StartMinor]), sizeof(v4));
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&next_v, &(shared_vie_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
							memcpy(&next_v, &(p_vie_minor[izNeighMinor[inext]]), sizeof(v4));
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						next_v.vxy = Clockwise_d*next_v.vxy;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						next_v.vxy = Anticlockwise_d*next_v.vxy;
					}

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradviz.x = 0.5*(
						(shared_vie[threadIdx.x].viz + next_v.viz)*(info.pos.y - nextpos.y)
						+ (prev_v.viz + shared_vie[threadIdx.x].viz)*(prevpos.y - info.pos.y)
						+ (opp_v.viz + prev_v.viz)*(opppos.y - prevpos.y)
						+ (next_v.viz + opp_v.viz)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradviz.y = -0.5*(
						(shared_vie[threadIdx.x].viz + next_v.viz)*(info.pos.x - nextpos.x)
						+ (prev_v.viz + shared_vie[threadIdx.x].viz)*(prevpos.x - info.pos.x)
						+ (opp_v.viz + prev_v.viz)*(opppos.x - prevpos.x)
						+ (next_v.viz + opp_v.viz)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if (flag == CROSSING_INS) {
							if (prev_v.vxy.x == 0.0) // prev is in the insulator.
							{
								// do like the above but it goes (ours, next, opp) somehow?

								f64 area_triangle = 0.5*(
									(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
									+ (opppos.x + info.pos.x)*(opppos.y - info.pos.y)
									+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y));

								gradvx.x = 0.5*(
									(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
									+ (opp_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(opppos.y - info.pos.y)
									+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvx.y = -0.5*(
									(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
									+ (opp_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(opppos.x - info.pos.x)
									+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x)
									) / area_triangle;
								
								gradvy.x = 0.5*(
									(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
									+ (opp_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(opppos.y - info.pos.y)
									+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvy.y = -0.5*(
									(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
									+ (opp_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(opppos.x - info.pos.x)
									+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x)
									) / area_triangle;
								
								gradviz.x = 0.5*(
									(shared_vie[threadIdx.x].viz + next_v.viz)*(info.pos.y - nextpos.y)
									+ (opp_v.viz + shared_vie[threadIdx.x].viz)*(opppos.y - info.pos.y)
									+ (next_v.viz + opp_v.viz)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradviz.y = -0.5*(
									(shared_vie[threadIdx.x].viz + next_v.viz)*(info.pos.x - nextpos.x)
									+ (opp_v.viz + shared_vie[threadIdx.x].viz)*(opppos.x - info.pos.x)
									+ (next_v.viz + opp_v.viz)*(nextpos.x - opppos.x)
									) / area_triangle;
								
							} else {
								if (next_v.vxy.x == 0.0) // next is in the insulator
								{
									f64 area_triangle = 0.5*(
										  (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
										+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
										+ (info.pos.x + opppos.x)*(info.pos.y - opppos.y)
										);

									gradvx.x = 0.5*(
										(prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
										+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].vxy.x + opp_v.vxy.x)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvx.y = -0.5*(
										(prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
											+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
											+ (shared_vie[threadIdx.x].vxy.x + opp_v.vxy.x)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;

									gradvy.x = 0.5*(
										(prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
										+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].vxy.y + opp_v.vxy.y)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvy.y = -0.5*(
										(prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
											+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
											+ (shared_vie[threadIdx.x].vxy.y + opp_v.vxy.y)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;

									gradviz.x = 0.5*(
										(prev_v.viz + shared_vie[threadIdx.x].viz)*(prevpos.y - info.pos.y)
										+ (opp_v.viz + prev_v.viz)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].viz + opp_v.viz)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradviz.y = -0.5*(
										  (prev_v.viz + shared_vie[threadIdx.x].viz)*(prevpos.x - info.pos.x)
										+ (opp_v.viz + prev_v.viz)*(opppos.x - prevpos.x)
										+ (shared_vie[threadIdx.x].viz + opp_v.viz)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
										) / area_triangle;
									
								} else {
									printf("\n\n\nDid not make sense! Alert RING-TAILED LEMUR. iMinor %d iNiegh %d \n\n\n\a", iMinor,
										izNeighMinor[i]);
								};
							};
						};
					};

					if (TESTIONVISC) printf("--------------\n%d %d our v: %1.8E %1.8E %1.8E  "
						"opp v: %1.8E %1.8E %1.8E \n",
						iMinor, i, shared_vie[threadIdx.x].vxy.x, shared_vie[threadIdx.x].vxy.y, shared_vie[threadIdx.x].viz,
						opp_v.vxy.x, opp_v.vxy.y, opp_v.viz);

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors

					htg_diff.x = shared_vie[threadIdx.x].vxy.x - opp_v.vxy.x;
					htg_diff.y = shared_vie[threadIdx.x].vxy.y - opp_v.vxy.y;
					htg_diff.z = shared_vie[threadIdx.x].viz - opp_v.viz;
				} else {
					if (TESTIONVISC) printf("side not usable: %d", i);
				};

				if (bUsableSide) {
					if ((VISCMAG == 0) || (omega_ci.dot(omega_ci) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradviz.x);
						Pi_zy = -ita_par*(gradviz.y);
						
						f64_vec3 visc_contrib;
						visc_contrib.x = -over_m_i*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						visc_contrib.y = -over_m_i*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						visc_contrib.z = -over_m_i*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);

////						if (info.flag == CROSSING_INS) {
////							char flag = p_info_minor[izNeighMinor[i]].flag;
////							if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX)) {
////								ownrates_visc += visc_contrib;
////								
////								if (TESTIONVISC) printf("UNMAGNETIZED visc_contrib.x %1.9E ownrates %1.9E\n",
////									visc_contrib.x, ownrates_visc.x);
////
////								if (i % 2 == 0) {
////									// vertex : heat collected by vertex
////								}
////								else {
////									visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
//////										(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
////	//									+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
////		//								+ (our_v.viz - opp_v.viz)*visc_contrib.z);
////									// And we are going to give it to what? Just spread it out after.
////
////								}
////							}
////							else {
////								// DO NOTHING
////							}
////						} else {
						ownrates_visc += visc_contrib;

						if (i % 2 == 0) {
							// vertex : heat collected by vertex
						}
						else {
							visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
//									(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
//								+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
	//							+ (our_v.viz - opp_v.viz)*visc_contrib.z);
						}

							//if (TESTTRI) {
							//	printf("iMinor %d %d "
							//		" ita_par %1.11E nu %1.11E omega %1.9E %1.9E %1.9E \n"
							//		"gradvx %1.9E %1.9E our vx %1.9E theirs %1.9E\n"
							//		"gradvy %1.9E %1.9E our vy %1.9E theirs %1.9E\n"
							//		"gradvz %1.9E %1.9E our vz %1.9E theirs %1.9E\n"
							//		"visc contrib %1.10E %1.10E %1.10E\n"
							//		"visc htg %1.10E %1.10E %1.10E | running %1.10E \n"
							//		" *************************************** \n",
							//		iMinor, izNeighMinor[i],
							//		ita_par, // Think nu is what breaks it
							//		nu, omega_ci.x, omega_ci.y, omega_ci.z,
							//		gradvx.x, gradvx.y, our_v.vxy.x, opp_v.vxy.x,
							//		gradvy.x, gradvy.y, our_v.vxy.y, opp_v.vxy.y,
							//		gradviz.x, gradviz.y, our_v.viz, opp_v.viz,
							//		visc_contrib.x, visc_contrib.y, visc_contrib.z,
							//		-THIRD*m_ion*(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x,
							//		-THIRD*m_ion*(our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y,
							//		-THIRD*m_ion*(our_v.viz - opp_v.viz)*visc_contrib.z,
							//		visc_htg
							//		);

							//	printf("iMinor %d visc_contrib.z %1.10E our-opp %1.10E z htg %1.10E | running %1.10E \n"
							//		" *************************************** \n",
							//		iMinor, visc_contrib.z, our_v.viz - opp_v.viz,
							//		-(our_v.viz - opp_v.viz)*THIRD*m_ion*visc_contrib.z,
							//		visc_htg);
							// }
						

						// So we are saying if edge_normal.x > 0 and gradviz.x > 0
						// then Pi_zx < 0 then ownrates += a positive amount. That is correct.
					} else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
							f64 omegasq = omega_ci.dot(omega_ci);
							omegamod = sqrt(omegasq);
							unit_b = omega_ci / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
							{
								f64_vec3 intermed;

								// use: d vb / da = b transpose [ dvi/dxj ] a
								// Prototypical element: a.x b.y dvy/dx
								// b.x a.y dvx/dy

								intermed.x = unit_b.dotxy(gradvx);
								intermed.y = unit_b.dotxy(gradvy);
								intermed.z = unit_b.dotxy(gradviz);
								{
									f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

									dvb_by_db = unit_b.dot(intermed);
									dvperp_by_db = unit_perp.dot(intermed);
									dvHall_by_db = unit_Hall.dot(intermed);

									W_bb += 4.0*THIRD*dvb_by_db;
									W_bP += dvperp_by_db;
									W_bH += dvHall_by_db;
									W_PP -= 2.0*THIRD*dvb_by_db;
									W_HH -= 2.0*THIRD*dvb_by_db;
								}
								{
									f64 dvb_by_dperp, dvperp_by_dperp,
										dvHall_by_dperp;
									// Optimize by getting rid of different labels.

									intermed.x = unit_perp.dotxy(gradvx);
									intermed.y = unit_perp.dotxy(gradvy);
									intermed.z = unit_perp.dotxy(gradviz);

									dvb_by_dperp = unit_b.dot(intermed);
									dvperp_by_dperp = unit_perp.dot(intermed);
									dvHall_by_dperp = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvperp_by_dperp;
									W_PP += 4.0*THIRD*dvperp_by_dperp;
									W_HH -= 2.0*THIRD*dvperp_by_dperp;
									W_bP += dvb_by_dperp;
									W_PH += dvHall_by_dperp;
								}
								{
									f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

									intermed.x = unit_Hall.dotxy(gradvx);
									intermed.y = unit_Hall.dotxy(gradvy);
									intermed.z = unit_Hall.dotxy(gradviz);

									dvb_by_dHall = unit_b.dot(intermed);
									dvperp_by_dHall = unit_perp.dot(intermed);
									dvHall_by_dHall = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvHall_by_dHall;
									W_PP -= 2.0*THIRD*dvHall_by_dHall;
									W_HH += 4.0*THIRD*dvHall_by_dHall;
									W_bH += dvb_by_dHall;
									W_PH += dvperp_by_dHall;
								}
							}

							{
								{
									f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

									Pi_b_b += -ita_par*W_bb;
									Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
									Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
									Pi_H_P += -ita_1*W_PH;
								}
								{
									f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_2*W_bP;
									Pi_H_b += -ita_2*W_bH;
								}
								{
									f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
									Pi_P_P -= ita_3*W_PH;
									Pi_H_H += ita_3*W_PH;
									Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
								}
								{
									f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_4*W_bH;
									Pi_H_b += ita_4*W_bP;
								}
							}
						}
						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
						
							// Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);

							//if (TESTTRI)
							//	printf("iMinor %d %d edge_normal %1.10E %1.10E mag_edge (b,P,H) %1.10E %1.10E %1.10E\n"
							//		"Pi_b_b %1.10E Pi_b_P %1.10E Pi_b_H %1.10E \n"
							//		"Pi_P_b %1.10E Pi_P_P %1.10E Pi_P_H %1.10E \n"
							//		"Pi_H_b %1.10E Pi_H_P %1.10E Pi_H_H %1.10E \n",
							//		iMinor, izNeighMinor[i], edge_normal.x, edge_normal.y, mag_edge.x, mag_edge.y, mag_edge.z,// b,P,H
							//		Pi_b_b, Pi_P_b, Pi_H_b,
							//		Pi_P_b, Pi_P_P, Pi_H_P,
							//		Pi_H_b, Pi_H_P, Pi_H_H);
						}

						// Time to double-check carefully the signs.
						// Pi was defined with - on dv/dx and we then dot that with the edge_normal, so giving + if we are higher than outside.
						
						// unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall
						// is the flow of p_x dotted with the edge_normal
						// ownrates will be divided by N to give dv/dt
						// m N dvx/dt = integral div momflux_x
						// Therefore divide here just by m
						f64_vec3 visc_contrib;
						visc_contrib.x = over_m_i*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						visc_contrib.y = over_m_i*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
						visc_contrib.z = over_m_i*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);

						if (TESTIONVISC)
							printf("%d %d over_m_i %1.9E "
						//		"unit_b %1.9E %1.9E %1.9E \n"
						//		"unit_perp %1.9E %1.9E %1.9E \n"
						//		"unit_Hall %1.9E %1.9E %1.9E \n"
						//		"momflux_b %1.9E momflux_perp %1.9E momflux_Hall %1.9E\n"
								"ita_par %1.10E visc_contrib.x %1.10E \n",
								
								iMinor, izNeighMinor[i], over_m_i, 
						//		unit_b.x, unit_b.y, unit_b.z,
						//		unit_perp.x, unit_perp.y, unit_perp.z,
						//		unit_Hall.x, unit_Hall.y, unit_Hall.z,
						//		momflux_b, momflux_perp, momflux_Hall,
								ita_par, visc_contrib.x );

						ownrates_visc += visc_contrib;
						if (i % 2 != 0) // not vertex
						{
							visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
							if (TESTIONVISC)
								printf("%d %d visc_htg %1.10E\n", iMinor,i, -THIRD*m_ion*(htg_diff.dot(visc_contrib)));

						}
							
					}
				}; // bUsableSide

//				endpt0 = endpt1;
			//	prevpos = opppos;
			//	prev_v = opp_v;
			//	opppos = nextpos;
			//	opp_v = next_v;
			};

			f64_vec3 ownrates;
			memcpy(&ownrates,&(p_MAR_ion[iMinor]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(&(p_MAR_ion[iMinor]), &(ownrates), sizeof(f64_vec3));

			p_NT_addition_tri[iMinor].NiTi += visc_htg;


			// Barking mad --- we never made special allowance yet for if a prev point is in insulator.
			//___________________________________________________________________________________________



			// We will have to round this up into the vertex heat afterwards.


#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iMinor %d NaN ownrates.x\n", iMinor);
			if (ownrates.y != ownrates.y)
				printf("iMinor %d NaN ownrates.y\n", iMinor);
			if (ownrates.z != ownrates.z)
				printf("iMinor %d NaN ownrates.z\n", iMinor);

			if (visc_htg != visc_htg) printf("iMinor %d NAN VISC HTG\n", iMinor);
#endif
		
			// We do best by taking each boundary, considering how
			// much heat to add for each one.

		} else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		} 
	} // scope

	__syncthreads();
	
	// Now do electron: overwrite ita and nu, copy-paste the above codes very carefully
	shared_ita_par[threadIdx.x] = p_ita_parallel_elec_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_elec_minor[iMinor];

	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];

		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))  // keeping consistent with ion above where we did put OUTERMOST here
		{// but we set ita to 0 in the pre routine for outermost.
			shared_ita_par_verts[threadIdx.x] = p_ita_parallel_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_elec_minor[iVertex + BEGINNING_OF_CENTRAL];
		}
		else {
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	if (threadIdx.x < threadsPerTileMajor) {
		memset(&ownrates_visc, 0, sizeof(f64_vec3));
		visc_htg = 0.0;

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len; // ?!
		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		if ((info.flag == DOMAIN_VERTEX) && (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{

			
		//	f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

#pragma unroll 
			for (int i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				f64_vec3 omega_ce;
				{
					f64_vec2 opp_B;
					f64 opp_ita, opp_nu;
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_B = shared_B[izTri[i] - StartMinor];
						opp_ita = shared_ita_par[izTri[i] - StartMinor];
						opp_nu = shared_nu[izTri[i] - StartMinor];
						//ita_par = 0.5*(shared_ita_par_verts[threadIdx.x] + shared_ita_par[izTri[i] - StartMinor]);
						//nu = 0.5*(shared_nu_verts[threadIdx.x] + shared_nu[izTri[i] - StartMinor]);
					}
					else {
						opp_B = p_B_minor[izTri[i]].xypart();
						opp_ita = p_ita_parallel_elec_minor[izTri[i]];
						opp_nu = p_nu_elec_minor[izTri[i]];
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					if (shared_ita_par_verts[threadIdx.x] < opp_ita) {
						ita_par = shared_ita_par_verts[threadIdx.x];
						nu = shared_nu_verts[threadIdx.x];
					}
					else {
						ita_par = opp_ita;
						nu = opp_nu;
					}
					omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B_verts[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}

				f64_vec2 gradvx, gradvy, gradvez;
				f64_vec2 edge_normal;
				f64_vec3 htg_diff;

				if (ita_par > 0.0)
				{
					v4 prev_v, next_v, opp_v;
					f64_vec2 prevpos, nextpos, opppos;

					short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
					short inext = i + 1;  if (inext >= tri_len) inext = 0;
					if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
					{
						prev_v = shared_vie[izTri[iprev] - StartMinor];
						prevpos = shared_pos[izTri[iprev] - StartMinor];
					}
					else {
						prev_v = p_vie_minor[izTri[iprev]];
						prevpos = p_info_minor[izTri[iprev]].pos;
					}
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						prev_v.vxy = Clockwise_d*prev_v.vxy;
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						prev_v.vxy = Anticlockwise_d*prev_v.vxy;
					}

					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_v = shared_vie[izTri[i] - StartMinor];
						opppos = shared_pos[izTri[i] - StartMinor];
					}
					else {
						opp_v = p_vie_minor[izTri[i]];
						opppos = p_info_minor[izTri[i]].pos;
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						opp_v.vxy = Clockwise_d*opp_v.vxy;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						opp_v.vxy = Anticlockwise_d*opp_v.vxy;
					}

					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						next_v = shared_vie[izTri[inext] - StartMinor];
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						next_v = p_vie_minor[izTri[inext]];
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						next_v.vxy = Clockwise_d*next_v.vxy;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						next_v.vxy = Anticlockwise_d*next_v.vxy;
					}
					// All same as ion here:

					// Order of calculations may help things to go out/into scope at the right times so careful with that.

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(shared_vie_verts[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.x + shared_vie_verts[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_vie_verts[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.x + shared_vie_verts[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_vie_verts[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.y + shared_vie_verts[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_vie_verts[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.y + shared_vie_verts[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvez.x = 0.5*(
						(shared_vie_verts[threadIdx.x].vez + next_v.vez)*(info.pos.y - nextpos.y)
						+ (prev_v.vez + shared_vie_verts[threadIdx.x].vez)*(prevpos.y - info.pos.y)
						+ (opp_v.vez + prev_v.vez)*(opppos.y - prevpos.y)
						+ (next_v.vez + opp_v.vez)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvez.y = -0.5*(
						(shared_vie_verts[threadIdx.x].vez + next_v.vez)*(info.pos.x - nextpos.x)
						+ (prev_v.vez + shared_vie_verts[threadIdx.x].vez)*(prevpos.x - info.pos.x)
						+ (opp_v.vez + prev_v.vez)*(opppos.x - prevpos.x)
						+ (next_v.vez + opp_v.vez)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					
						
					if (TESTVISC) printf("%d our v %1.8E %1.8E %1.8E oppv %1.8E %1.8E %1.8E \n",
						izTri[i],
						shared_vie_verts[threadIdx.x].vxy.x, shared_vie_verts[threadIdx.x].vxy.y,
						shared_vie_verts[threadIdx.x].vez, opp_v.vxy.x, opp_v.vxy.y, opp_v.vez);

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x); // need to define so as to create unit vectors

					htg_diff.x = shared_vie_verts[threadIdx.x].vxy.x - opp_v.vxy.x;
					htg_diff.y = shared_vie_verts[threadIdx.x].vxy.y - opp_v.vxy.y;
					htg_diff.z = shared_vie_verts[threadIdx.x].vez - opp_v.vez;
				}
				

		//		f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				if (ita_par > 0.0) {
					if ((VISCMAG == 0) || (omega_ce.dot(omega_ce) < 0.01*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						// Let's suppose, Pi_yx means the rate of flow of y-momentum in the x direction.
						// Thus when we want to know how much y momentum is flowing through the wall we take
						// Pi_yx.edge_x + Pi_yy.edge_y -- reasonable.

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradvez.x);
						Pi_zy = -ita_par*(gradvez.y);

						f64_vec3 visc_contrib;
						visc_contrib.x = -over_m_e*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						visc_contrib.y = -over_m_e*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						visc_contrib.z = -over_m_e*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);

						//					if (info.flag == OUTERMOST) {
						//						if (p_info_minor[izTri[i]].flag == DOMAIN_TRIANGLE) {
						//							ownrates_visc += visc_contrib;
						//
						//							visc_htg += -TWOTHIRDS*m_e*(
						//								(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
						//								+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
						//								+ (our_v.vez - opp_v.vez)*visc_contrib.z);
						//						}
						//						else {
						//							visc_contrib.x = 0.0; visc_contrib.y = 0.0; visc_contrib.z = 0.0;
						//						}
						//					} else
						{
							ownrates_visc += visc_contrib;

							visc_htg += -TWOTHIRDS*m_e*(htg_diff.dot(visc_contrib));
							//							(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
								//						+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
									//					+ (our_v.vez - opp_v.vez)*visc_contrib.z);
						};
						// The alternative, that may or may not run faster, is to test for ita == 0 before we do all the calcs
						// and then set ita == 0 in all the places not to look, including OUTERMOST, and do not do traffic to or from it.
	//
						if (0) // (TESTVISC) 
							printf("iVertex %d tri %d ELEC ita_par %1.9E own ita %1.9E\n"
							"gradvx %1.8E %1.8E gradvy %1.8E %1.8E gradvez %1.8E %1.8E\n"
							"edgenormal %1.8E %1.8E\n"
							"Pi_xx %1.8E xy %1.8E yy %1.8E zx %1.8E\n"
							"visc_contrib %1.9E %1.9E %1.9E \n"
							"htg cum %1.9E  heating %1.9E \n"
							"===\n",
							iVertex, izTri[i], ita_par, shared_ita_par_verts[threadIdx.x],
							gradvx.x, gradvx.y, gradvy.x, gradvy.y, gradvez.x, gradvez.y,
							edge_normal.x, edge_normal.y,
							Pi_xx, Pi_xy, Pi_yy, Pi_zx,
							visc_contrib.x, visc_contrib.y, visc_contrib.z,
							visc_htg,
							-TWOTHIRDS*m_e*(htg_diff.dot(visc_contrib))
							);

						// -= !!!
						// So we are saying if edge_normal.x > 0 and gradviz.x > 0
						// then Pi_zx < 0 then ownrates += a positive amount. That is correct.
					}
					else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
							f64 omegasq = omega_ce.dot(omega_ce);
							omegamod = sqrt(omegasq);
							unit_b = omega_ce / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
							{
								f64_vec3 intermed;

								// use: d vb / da = b transpose [ dvi/dxj ] a
								// Prototypical element: a.x b.y dvy/dx
								// b.x a.y dvx/dy

								intermed.x = unit_b.dotxy(gradvx);
								intermed.y = unit_b.dotxy(gradvy);
								intermed.z = unit_b.dotxy(gradvez);
								{
									f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

									dvb_by_db = unit_b.dot(intermed);
									dvperp_by_db = unit_perp.dot(intermed);
									dvHall_by_db = unit_Hall.dot(intermed);

									W_bb += 4.0*THIRD*dvb_by_db;
									W_bP += dvperp_by_db;
									W_bH += dvHall_by_db;
									W_PP -= 2.0*THIRD*dvb_by_db;
									W_HH -= 2.0*THIRD*dvb_by_db;
								}
								{
									f64 dvb_by_dperp, dvperp_by_dperp,
										dvHall_by_dperp;
									// Optimize by getting rid of different labels.

									intermed.x = unit_perp.dotxy(gradvx);
									intermed.y = unit_perp.dotxy(gradvy);
									intermed.z = unit_perp.dotxy(gradvez);

									dvb_by_dperp = unit_b.dot(intermed);
									dvperp_by_dperp = unit_perp.dot(intermed);
									dvHall_by_dperp = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvperp_by_dperp;
									W_PP += 4.0*THIRD*dvperp_by_dperp;
									W_HH -= 2.0*THIRD*dvperp_by_dperp;
									W_bP += dvb_by_dperp;
									W_PH += dvHall_by_dperp;
								}
								{
									f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

									intermed.x = unit_Hall.dotxy(gradvx);
									intermed.y = unit_Hall.dotxy(gradvy);
									intermed.z = unit_Hall.dotxy(gradvez);

									dvb_by_dHall = unit_b.dot(intermed);
									dvperp_by_dHall = unit_perp.dot(intermed);
									dvHall_by_dHall = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvHall_by_dHall;
									W_PP -= 2.0*THIRD*dvHall_by_dHall;
									W_HH += 4.0*THIRD*dvHall_by_dHall;
									W_bH += dvb_by_dHall;
									W_PH += dvperp_by_dHall;
								}
							}

							{
								{
									f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

									Pi_b_b += -ita_par*W_bb;
									Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
									Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
									Pi_H_P += -ita_1*W_PH;
								}
								{
									f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_2*W_bP;
									Pi_H_b += -ita_2*W_bH;
								}
								{
									f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
									Pi_P_P -= ita_3*W_PH;
									Pi_H_H += ita_3*W_PH;
									Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
								}
								{
									f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_4*W_bH;
									Pi_H_b += ita_4*W_bP;
								}
							}
						}
						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
							// Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;    // b component
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y; // P component
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y; // H component

							// verify for chosen edge that we obtained a 3-vector of the same length as the original edge!
							// Tick

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						f64_vec3 visc_contrib;
						visc_contrib.x = over_m_e*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						visc_contrib.y = over_m_e*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
						visc_contrib.z = over_m_e*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);

						//if (info.flag == OUTERMOST) {
						//	if (p_info_minor[izTri[i]].flag == DOMAIN_TRIANGLE) {
						//		ownrates_visc += visc_contrib;

						//		visc_htg += -TWOTHIRDS*m_e*(
						//			(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
						//			+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
						//			+ (our_v.vez - opp_v.vez)*visc_contrib.z);
						//	}
						//}else
						{
							ownrates_visc += visc_contrib;

							visc_htg += -TWOTHIRDS*m_e*(htg_diff.dot(visc_contrib));
							//	(our_v.vxy.x - opp_v.vxy.x)*visc_contrib.x
							//	+ (our_v.vxy.y - opp_v.vxy.y)*visc_contrib.y
							//	+ (our_v.vez - opp_v.vez)*visc_contrib.z);
						};
						if (TESTVISC) {

							// Most efficient way: compute mom flux in magnetic coords

							printf("iVertex %d MAGNETIZED elec: visc contrib %1.8E %1.8E %1.8E\n"
								"htg cum %1.9E  visc htg %1.9E ita_par %1.9E \n"
								"gradvx %1.8E %1.8E gradvy %1.8E %1.8E gradvez %1.8E %1.8E\n"

								"unit_b %1.8E %1.8E %1.8E unit_perp %1.8E %1.8E %1.8E unit_H %1.8E %1.8E %1.8E\n"
								"omega_ce %1.8E %1.8E %1.8E mod %1.8E nu %1.8E \n"
								"===\n",
								iVertex, visc_contrib.x, visc_contrib.y, visc_contrib.z,
								visc_htg, -TWOTHIRDS*m_e*(htg_diff.dot(visc_contrib)),
								ita_par,
								gradvx.x, gradvx.y, gradvy.x, gradvy.y, gradvez.x, gradvez.y,

								unit_b.x, unit_b.y, unit_b.z, unit_perp.x, unit_perp.y, unit_perp.z, unit_Hall.x, unit_Hall.y, unit_Hall.z,
								omega_ce.x, omega_ce.y, omega_ce.z, omega_ce.modulus(), nu);
						}
						//
						// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
						// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);
					}
				}; // ita_par > 0.0

		//		endpt0 = endpt1;
				
			}; // next i

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_elec[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
//
			if (TESTVISC)
				printf("iVertex %d ownrates %1.8E %1.8E %1.8E ownrates_visc %1.8E %1.8E %1.8E htg %1.8E \n",
				iVertex, ownrates.x, ownrates.y, ownrates.z, ownrates_visc.x, ownrates_visc.y, ownrates_visc.z, visc_htg);

			ownrates += ownrates_visc;
			memcpy(p_MAR_elec + iVertex + BEGINNING_OF_CENTRAL, &ownrates, sizeof(f64_vec3));

			if (TESTVISC) printf("iVertex %d NeTe recorded %1.10E  \n", iVertex, p_NT_addition_rate[iVertex].NeTe);
			p_NT_addition_rate[iVertex].NeTe += visc_htg;
			if (TESTVISC) printf("iVertex %d NeTe recorded %1.10E  \n", iVertex, p_NT_addition_rate[iVertex].NeTe);
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iVertex e %d NaN ownrates.x\n", iVertex);
			if (ownrates.y != ownrates.y)
				printf("iVertex e %d NaN ownrates.y\n", iVertex);
			if (ownrates.z != ownrates.z)
				printf("iVertex e %d NaN ownrates.z\n", iVertex);

			if (visc_htg != visc_htg) printf("iVertex e %d NAN VISC HTG\n", iVertex);
#endif
		} else {
			 // NOT domain vertex: Do nothing			
		};
	};

	// Electrons in tris:
	info = p_info_minor[iMinor];
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
	memset(&ownrates_visc, 0, sizeof(f64_vec3));
	visc_htg = 0.0;


	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	}
	else {
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			short inext, iprev = 5, i = 0;
			
		//	f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

			f64_vec3 omega_ce;
			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				bool bUsableSide = true;
				{
					f64_vec2 opp_B;
					f64 opp_ita, opp_nu;
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						opp_B = shared_B[izNeighMinor[i] - StartMinor];
						opp_ita = shared_ita_par[izNeighMinor[i] - StartMinor];
						opp_nu = shared_nu[izNeighMinor[i] - StartMinor];
						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							opp_B = shared_B_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							opp_ita = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							opp_nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							opp_B = p_B_minor[izNeighMinor[i]].xypart();
							opp_ita = p_ita_parallel_elec_minor[izNeighMinor[i]];
							opp_nu = p_nu_elec_minor[izNeighMinor[i]];
							if (opp_ita == 0.0) bUsableSide = false;
							
						}
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opp_B = Clockwise_d*opp_B;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opp_B = Anticlockwise_d*opp_B;
					}
					if (shared_ita_par[threadIdx.x] < opp_ita) {
						ita_par = shared_ita_par[threadIdx.x];
						nu = shared_nu[threadIdx.x];
					}
					else {
						ita_par = opp_ita;
						nu = opp_nu;
					}
					omega_ce = 0.5*qovermc*(Make3(opp_B + shared_B[threadIdx.x], BZ_CONSTANT)); // NOTE BENE qoverMc
				}
				f64_vec2 gradvez, gradvx, gradvy;
				f64_vec2 edge_normal; // a reason why storing position > loading.
				f64_vec3 htg_diff;

				if (bUsableSide)
				{
					short inext = i + 1; if (inext > 5) inext = 0;
					short iprev = i - 1; if (iprev < 0) iprev = 5;

					v4 opp_v, next_v, prev_v;
					f64_vec2 opppos, nextpos, prevpos;
					
					if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
					{
						memcpy(&prev_v, &(shared_vie[izNeighMinor[iprev] - StartMinor]), sizeof(v4));
						prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
					}
					else {
						if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&prev_v, &(shared_vie_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							prevpos = p_info_minor[izNeighMinor[iprev]].pos;
							memcpy(&prev_v, &(p_vie_minor[izNeighMinor[iprev]]), sizeof(v4));
						};
					};
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						prev_v.vxy = Clockwise_d*prev_v.vxy;
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						prev_v.vxy = Anticlockwise_d*prev_v.vxy;
					}

					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						memcpy(&opp_v, &(shared_vie[izNeighMinor[i] - StartMinor]), sizeof(v4));
						opppos = shared_pos[izNeighMinor[i] - StartMinor];
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&opp_v, &(shared_vie_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							opppos = p_info_minor[izNeighMinor[i]].pos;
							memcpy(&opp_v, &(p_vie_minor[izNeighMinor[i]]), sizeof(v4));
						};
					};
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						opp_v.vxy = Clockwise_d*opp_v.vxy;
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						opp_v.vxy = Anticlockwise_d*opp_v.vxy;
					}

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						memcpy(&next_v, &(shared_vie[izNeighMinor[inext] - StartMinor]), sizeof(v4));
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&next_v, &(shared_vie_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(v4));
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
							memcpy(&next_v, &(p_vie_minor[izNeighMinor[inext]]), sizeof(v4));
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						next_v.vxy = Clockwise_d*next_v.vxy;
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						next_v.vxy = Anticlockwise_d*next_v.vxy;
					}

					// New definition of endpoint of minor edge:

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
						+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
						+ (prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
						+ (prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
						+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
						+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvez.x = 0.5*(
						(shared_vie[threadIdx.x].vez + next_v.vez)*(info.pos.y - nextpos.y)
						+ (prev_v.vez + shared_vie[threadIdx.x].vez)*(prevpos.y - info.pos.y)
						+ (opp_v.vez + prev_v.vez)*(opppos.y - prevpos.y)
						+ (next_v.vez + opp_v.vez)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvez.y = -0.5*(
						(shared_vie[threadIdx.x].vez + next_v.vez)*(info.pos.x - nextpos.x)
						+ (prev_v.vez + shared_vie[threadIdx.x].vez)*(prevpos.x - info.pos.x)
						+ (opp_v.vez + prev_v.vez)*(opppos.x - prevpos.x)
						+ (next_v.vez + opp_v.vez)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if (flag == CROSSING_INS) {
							if (prev_v.vxy.x == 0.0) // prev is in the insulator.
							{
								// do like the above but it goes (ours, next, opp) somehow?

								f64 area_triangle = 0.5*(
									(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
									+ (opppos.x + info.pos.x)*(opppos.y - info.pos.y)
									+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y));

								gradvx.x = 0.5*(
									(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.y - nextpos.y)
									+ (opp_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(opppos.y - info.pos.y)
									+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvx.y = -0.5*(
									(shared_vie[threadIdx.x].vxy.x + next_v.vxy.x)*(info.pos.x - nextpos.x)
									+ (opp_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(opppos.x - info.pos.x)
									+ (next_v.vxy.x + opp_v.vxy.x)*(nextpos.x - opppos.x)
									) / area_triangle;

								gradvy.x = 0.5*(
									(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.y - nextpos.y)
									+ (opp_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(opppos.y - info.pos.y)
									+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvy.y = -0.5*(
									(shared_vie[threadIdx.x].vxy.y + next_v.vxy.y)*(info.pos.x - nextpos.x)
									+ (opp_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(opppos.x - info.pos.x)
									+ (next_v.vxy.y + opp_v.vxy.y)*(nextpos.x - opppos.x)
									) / area_triangle;

								gradvez.x = 0.5*(
									(shared_vie[threadIdx.x].vez + next_v.vez)*(info.pos.y - nextpos.y)
									+ (opp_v.vez + shared_vie[threadIdx.x].vez)*(opppos.y - info.pos.y)
									+ (next_v.vez + opp_v.vez)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvez.y = -0.5*(
									(shared_vie[threadIdx.x].vez + next_v.vez)*(info.pos.x - nextpos.x)
									+ (opp_v.vez + shared_vie[threadIdx.x].vez)*(opppos.x - info.pos.x)
									+ (next_v.vez + opp_v.vez)*(nextpos.x - opppos.x)
									) / area_triangle;

							}
							else {
								if (next_v.vxy.x == 0.0) // next is in the insulator
								{
									f64 area_triangle = 0.5*(
										(prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
										+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
										+ (info.pos.x + opppos.x)*(info.pos.y - opppos.y)
										);

									gradvx.x = 0.5*(
										(prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.y - info.pos.y)
										+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].vxy.x + opp_v.vxy.x)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvx.y = -0.5*(
										(prev_v.vxy.x + shared_vie[threadIdx.x].vxy.x)*(prevpos.x - info.pos.x)
											+ (opp_v.vxy.x + prev_v.vxy.x)*(opppos.x - prevpos.x)
											+ (shared_vie[threadIdx.x].vxy.x + opp_v.vxy.x)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;

									gradvy.x = 0.5*(
										(prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.y - info.pos.y)
										+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].vxy.y + opp_v.vxy.y)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvy.y = -0.5*(
										(prev_v.vxy.y + shared_vie[threadIdx.x].vxy.y)*(prevpos.x - info.pos.x)
											+ (opp_v.vxy.y + prev_v.vxy.y)*(opppos.x - prevpos.x)
											+ (shared_vie[threadIdx.x].vxy.y + opp_v.vxy.y)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;

									gradvez.x = 0.5*(
										(prev_v.vez + shared_vie[threadIdx.x].vez)*(prevpos.y - info.pos.y)
										+ (opp_v.vez + prev_v.vez)*(opppos.y - prevpos.y)
										+ (shared_vie[threadIdx.x].vez + opp_v.vez)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvez.y = -0.5*(
											  (prev_v.vez + shared_vie[threadIdx.x].vez)*(prevpos.x - info.pos.x)
											+ (opp_v.vez + prev_v.vez)*(opppos.x - prevpos.x)
											+ (shared_vie[threadIdx.x].vez + opp_v.vez)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
											) / area_triangle;

								} else {
									printf("\n\n\nDid not make sense! Alert RING-TAILED LEMUR. iMinor %d iNeigh %d \n\n\n\a", iMinor,
										izNeighMinor[i]);
								};
							};
						};
					};

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors
					
					if (TEST_ELEC_VISC_TRI) printf("%d prev_v %1.14E opp_v %1.14E next_v %1.14E our_v %1.14E omega %1.8E %1.8E\n",
						iMinor, prev_v.vez, opp_v.vez, next_v.vez, shared_vie[threadIdx.x].vez,
						omega_ce.x, omega_ce.y);


					htg_diff.x = shared_vie[threadIdx.x].vxy.x - opp_v.vxy.x;
					htg_diff.y = shared_vie[threadIdx.x].vxy.y - opp_v.vxy.y;
					htg_diff.z = shared_vie[threadIdx.x].vez - opp_v.vez;
				}
				// Wouldn't it be nice if we could now drop all our prev_v variables and pick them up again on the next
				// go around?
				// That's really what I think we need.

				if (bUsableSide)
				{
					if ((VISCMAG == 0) || (omega_ce.dot(omega_ce) < 0.1*0.1*nu*nu))
					{
						// run unmagnetised case
						f64 Pi_xx, Pi_xy, Pi_yx, Pi_yy, Pi_zx, Pi_zy;

						Pi_xx = -ita_par*THIRD*(4.0*gradvx.x - 2.0*gradvy.y);
						Pi_xy = -ita_par*(gradvx.y + gradvy.x);
						Pi_yx = Pi_xy;
						Pi_yy = -ita_par*THIRD*(4.0*gradvy.y - 2.0*gradvx.x);
						Pi_zx = -ita_par*(gradvez.x);
						Pi_zy = -ita_par*(gradvez.y);

						f64_vec3 visc_contrib;
						visc_contrib.x = -over_m_e*(Pi_xx*edge_normal.x + Pi_xy*edge_normal.y);
						visc_contrib.y = -over_m_e*(Pi_yx*edge_normal.x + Pi_yy*edge_normal.y);
						visc_contrib.z = -over_m_e*(Pi_zx*edge_normal.x + Pi_zy*edge_normal.y);


						ownrates_visc += visc_contrib;
						if (i % 2 != 0)
							visc_htg += -THIRD*m_e*(htg_diff.dot(visc_contrib));
						

						if (TESTVISC)
							printf("\n%d : %d : ita %1.8E gradvz %1.9E %1.9E ourpos %1.9E %1.9E visc_contrib.z %1.10E visc_htg %1.10E\n", 
								iMinor, izNeighMinor[i], ita_par, 
								gradvez.x,gradvez.y, info.pos.x,info.pos.y,
								visc_contrib.z, visc_htg);

						// 42939: Find out why it makes too much heat. Probably a compound error.
				//		if (iMinor == 42939) printf("42939\nour_v %1.8E %1.8E %1.8E \n"
				//			"opp_v %1.8E %1.8E %1.8E \n"
				//			"visc_contrib %1.8E %1.8E %1.8E \n",
				//			our_v.vxy.x, our_v.vxy.y, our_v.vez,
				//			opp_v.vxy.x, opp_v.vxy.y, opp_v.vez,
				//			visc_contrib.x, visc_contrib.y, visc_contrib.z);
				//		
						
					} else {
						f64_vec3 unit_b, unit_perp, unit_Hall;
						f64 omegamod;
						{
						//	f64_vec2 edge_normal;
						//	edge_normal.x = THIRD * (nextpos.y - prevpos.y);
						//	edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors

							f64 omegasq = omega_ce.dot(omega_ce);
							omegamod = sqrt(omegasq);
							unit_b = omega_ce / omegamod;
							unit_perp = Make3(edge_normal, 0.0) - unit_b*(unit_b.dotxy(edge_normal));
							unit_perp = unit_perp / unit_perp.modulus();
							unit_Hall = unit_b.cross(unit_perp); // Note sign.
																 // store omegamod instead.
						}
						f64 Pi_b_b = 0.0, Pi_P_b = 0.0, Pi_P_P = 0.0, Pi_H_b = 0.0, Pi_H_P = 0.0, Pi_H_H = 0.0;
						{
							f64 W_bb = 0.0, W_bP = 0.0, W_bH = 0.0, W_PP = 0.0, W_PH = 0.0, W_HH = 0.0; // these have to be alive at same time as 9 x partials
							{
								f64_vec3 intermed;

								// use: d vb / da = b transpose [ dvi/dxj ] a
								// Prototypical element: a.x b.y dvy/dx
								// b.x a.y dvx/dy

								intermed.x = unit_b.dotxy(gradvx);
								intermed.y = unit_b.dotxy(gradvy);
								intermed.z = unit_b.dotxy(gradvez);
								{
									f64 dvb_by_db, dvperp_by_db, dvHall_by_db;

									dvb_by_db = unit_b.dot(intermed);
									dvperp_by_db = unit_perp.dot(intermed);
									dvHall_by_db = unit_Hall.dot(intermed);

									W_bb += 4.0*THIRD*dvb_by_db;
									W_bP += dvperp_by_db;
									W_bH += dvHall_by_db;
									W_PP -= 2.0*THIRD*dvb_by_db;
									W_HH -= 2.0*THIRD*dvb_by_db;
								}
								{
									f64 dvb_by_dperp, dvperp_by_dperp,
										dvHall_by_dperp;
									// Optimize by getting rid of different labels.

									intermed.x = unit_perp.dotxy(gradvx);
									intermed.y = unit_perp.dotxy(gradvy);
									intermed.z = unit_perp.dotxy(gradvez);

									dvb_by_dperp = unit_b.dot(intermed);
									dvperp_by_dperp = unit_perp.dot(intermed);
									dvHall_by_dperp = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvperp_by_dperp;
									W_PP += 4.0*THIRD*dvperp_by_dperp;
									W_HH -= 2.0*THIRD*dvperp_by_dperp;
									W_bP += dvb_by_dperp;
									W_PH += dvHall_by_dperp;
								}
								{
									f64 dvb_by_dHall, dvperp_by_dHall, dvHall_by_dHall;

									intermed.x = unit_Hall.dotxy(gradvx);
									intermed.y = unit_Hall.dotxy(gradvy);
									intermed.z = unit_Hall.dotxy(gradvez);

									dvb_by_dHall = unit_b.dot(intermed);
									dvperp_by_dHall = unit_perp.dot(intermed);
									dvHall_by_dHall = unit_Hall.dot(intermed);

									W_bb -= 2.0*THIRD*dvHall_by_dHall;
									W_PP -= 2.0*THIRD*dvHall_by_dHall;
									W_HH += 4.0*THIRD*dvHall_by_dHall;
									W_bH += dvb_by_dHall;
									W_PH += dvperp_by_dHall;
								}
							}

							{
								{
									f64 ita_1 = ita_par*(nu*nu / (nu*nu + omegamod*omegamod));

									Pi_b_b += -ita_par*W_bb;
									Pi_P_P += -0.5*(ita_par + ita_1)*W_PP - 0.5*(ita_par - ita_1)*W_HH;
									Pi_H_H += -0.5*(ita_par + ita_1)*W_HH - 0.5*(ita_par - ita_1)*W_PP;
									Pi_H_P += -ita_1*W_PH;
								}
								{
									f64 ita_2 = ita_par*(nu*nu / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_2*W_bP;
									Pi_H_b += -ita_2*W_bH;
								}
								{
									f64 ita_3 = ita_par*(nu*omegamod / (nu*nu + omegamod*omegamod));
									Pi_P_P -= ita_3*W_PH;
									Pi_H_H += ita_3*W_PH;
									Pi_H_P += 0.5*ita_3*(W_PP - W_HH);
								}
								{
									f64 ita_4 = 0.5*ita_par*(nu*omegamod / (nu*nu + 0.25*omegamod*omegamod));
									Pi_P_b += -ita_4*W_bH;
									Pi_H_b += ita_4*W_bP;
								}
							}
						}
						f64 momflux_b, momflux_perp, momflux_Hall;
						{
							f64_vec3 mag_edge;
						//	f64_vec2 edge_normal;
						//	edge_normal.x = THIRD * (nextpos.y - prevpos.y);
						//	edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors
							// Most efficient way: compute mom flux in magnetic coords
							mag_edge.x = unit_b.x*edge_normal.x + unit_b.y*edge_normal.y;
							mag_edge.y = unit_perp.x*edge_normal.x + unit_perp.y*edge_normal.y;
							mag_edge.z = unit_Hall.x*edge_normal.x + unit_Hall.y*edge_normal.y;

							momflux_b = -(Pi_b_b*mag_edge.x + Pi_P_b*mag_edge.y + Pi_H_b*mag_edge.z);
							momflux_perp = -(Pi_P_b*mag_edge.x + Pi_P_P*mag_edge.y + Pi_H_P*mag_edge.z);
							momflux_Hall = -(Pi_H_b*mag_edge.x + Pi_H_P*mag_edge.y + Pi_H_H*mag_edge.z);
						}

						// unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall
						// is the flow of p_x dotted with the edge_normal
						// ownrates will be divided by N to give dv/dt
						// m N dvx/dt = integral div momflux_x
						// Therefore divide here just by m
						f64_vec3 visc_contrib;
						visc_contrib.x = over_m_e*(unit_b.x*momflux_b + unit_perp.x*momflux_perp + unit_Hall.x*momflux_Hall);
						visc_contrib.y = over_m_e*(unit_b.y*momflux_b + unit_perp.y*momflux_perp + unit_Hall.y*momflux_Hall);
						visc_contrib.z = over_m_e*(unit_b.z*momflux_b + unit_perp.z*momflux_perp + unit_Hall.z*momflux_Hall);


						ownrates_visc += visc_contrib;


						if (TEST_ELEC_VISC_TRI) printf(
							"%d ownrates_visc.z %1.14E visc_contrib.z %1.14E 1/m_e %1.14E\n"
							"unit_b.z %1.14E unit_perp.z %1.14E unit_Hall.z %1.14E\n"
							"momflux b perp Hall %1.14E %1.14E %1.14E  gradvez %1.14E %1.14E\n",
							iMinor, ownrates_visc.z, visc_contrib.z, over_m_e, unit_b.z,
							unit_perp.z, unit_Hall.z, momflux_b, momflux_perp, momflux_Hall,
							gradvez.x, gradvez.y);

						if (i % 2 != 0)
							visc_htg += -THIRD*m_e*(htg_diff.dot(visc_contrib));
						
					}
				}; // bUsableSide

		//		endpt0 = endpt1;
			};
			f64_vec3 ownrates;
			memcpy(&(ownrates), &(p_MAR_elec[iMinor]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(&(p_MAR_elec[iMinor]), &(ownrates), sizeof(f64_vec3));

			p_NT_addition_tri[iMinor].NeTe += visc_htg;

#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iMinor e %d NaN ownrates.x\n", iMinor);
			if (ownrates.y != ownrates.y)
				printf("iMinor e %d NaN ownrates.y\n", iMinor);
			if (ownrates.z != ownrates.z)
				printf("iMinor e %d NaN ownrates.z\n", iMinor);

			if (visc_htg != visc_htg) printf("iMinor e %d NAN VISC HTG\n", iMinor);
#endif
			if (TESTVISC) {

				if (ownrates.x != ownrates.x)
					printf("iMinor e %d NaN ownrates.x\n", iMinor);
				if (ownrates.y != ownrates.y)
					printf("iMinor e %d NaN ownrates.y\n", iMinor);
				if (ownrates.z != ownrates.z)
					printf("iMinor e %d NaN ownrates.z\n", iMinor);

				if (visc_htg != visc_htg) printf("iMinor e %d NAN VISC HTG\n", iMinor);
			}

		} else {
			// Not domain, not crossing_ins, not a frill			
		} // non-domain tri
	}; // was it FRILL
}

// Neutral routine:
__global__ void kernelCreate_neutral_viscous_contrib_to_MAR_and_NT(
	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_v_n_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_neut_minor,   //
	f64 * __restrict__ p_nu_neut_minor,   // 

	f64_vec3 * __restrict__ p_MAR_neut,
	NTrates * __restrict__ p_NT_addition_rate,
	NTrates * __restrict__ p_NT_addition_tri)
{
	__shared__ f64_vec3 shared_v_n[threadsPerTileMinor]; // sort of thing we want as input
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];

	__shared__ f64_vec3 shared_v_n_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

														 // There is room for some more double in shared per thread.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!
	f64_vec3 ownrates_visc;
	f64 visc_htg;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_v_n[threadIdx.x] = p_v_n_minor[iMinor];
	shared_ita_par[threadIdx.x] = p_ita_neut_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_neut_minor[iMinor];

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
		{
			memcpy(&(shared_v_n_verts[threadIdx.x]), &(p_v_n_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = p_ita_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		}
		else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.			
			memset(&(shared_v_n_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	// How shall we arrange to do v_n, which is isotropic? Handle this first...
	// Is the v_n coefficient negligible? Check.

	// We actually have to think how to handle the x-y dimension. PopOhms will handle it.

	// We can re-use some shared data -- such as pos and B -- to do both ions and electrons
	// But they use different ita_par and different vez, viz. 
	// Often we don't need to do magnetised ion viscosity when we do magnetised electron.


	if (threadIdx.x < threadsPerTileMajor) {
		memset(&ownrates_visc, 0, sizeof(f64_vec3));
		visc_htg = 0.0;

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!

										// JUST TO GET IT TO RUN: LIMIT OURSELVES TO RADIUS 4.5 : 
		if ((info.flag == DOMAIN_VERTEX) && (info.pos.modulus() < 4.5)
			&& (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{
			// We are losing energy if there is viscosity into OUTERMOST.

			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

#pragma unroll 
			for (short i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think

				{
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						if (shared_ita_par_verts[threadIdx.x] < shared_ita_par[izTri[i] - StartMinor])
						{
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izTri[i] - StartMinor];
							nu = shared_nu[izTri[i] - StartMinor];
						};
					}
					else {
						f64 ita_theirs = p_ita_neut_minor[izTri[i]];
						f64 nu_theirs = p_nu_neut_minor[izTri[i]];
						if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = ita_theirs;
							nu = nu_theirs;
						};

						// I understand why we are still doing minimum ita at the wall but we would ideally like to stop.

					};
				} // Guaranteed DOMAIN_VERTEX never needs to skip an edge; we include CROSSING_INS in viscosity.

				f64_vec2 gradvx, gradvy, gradvz;
				f64_vec3 htg_diff;
				f64_vec2 edge_normal;

				if (ita_par > 0.0) // note it was the minimum taken.
				{
					f64_vec3 opp_v, prev_v, next_v;
					f64_vec2 opppos, prevpos, nextpos;
					// ideally we might want to leave position out of the loop so that we can avoid reloading it.

					short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
					short inext = i + 1; if (inext >= tri_len) inext = 0;

					if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
					{
						prev_v = shared_v_n[izTri[iprev] - StartMinor];
						prevpos = shared_pos[izTri[iprev] - StartMinor];
					}
					else {
						prev_v = p_v_n_minor[izTri[iprev]];
						prevpos = p_info_minor[izTri[iprev]].pos;
					}
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						RotateClockwise(prev_v);
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						RotateAnticlockwise(prev_v);
					}

					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_v = shared_v_n[izTri[i] - StartMinor];
						opppos = shared_pos[izTri[i] - StartMinor];

						//		if (iVertex == VERTCHOSEN) printf("opp_v %1.9E izTri[i] %d \n", opp_v.x, izTri[i]);

					}
					else {
						opp_v = p_v_n_minor[izTri[i]];
						opppos = p_info_minor[izTri[i]].pos;

						//	if (iVertex == VERTCHOSEN) printf("opp_v %1.9E v_n_minor izTri[i] %d \n", opp_v.x, izTri[i]);
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						RotateAnticlockwise(opp_v);
					}

					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						next_v = shared_v_n[izTri[inext] - StartMinor];
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						next_v = p_v_n_minor[izTri[inext]];
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						RotateClockwise(next_v);
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						RotateAnticlockwise(next_v);
					}

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);


					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);



					gradvx.x = 0.5*(
						(shared_v_n_verts[threadIdx.x].x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + shared_v_n_verts[threadIdx.x].x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_v_n_verts[threadIdx.x].x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + shared_v_n_verts[threadIdx.x].x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_v_n_verts[threadIdx.x].y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + shared_v_n_verts[threadIdx.x].y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_v_n_verts[threadIdx.x].y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + shared_v_n_verts[threadIdx.x].y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					//
					//				if (TEST) printf(
					//					"iVertex %d our_v.y next prev opp %1.8E %1.8E %1.8E %1.8E\n"
					//					"area_quad %1.8E \n"
					//					"info.pos %1.8E %1.8E opppos %1.8E %1.8E prev %1.8E %1.8E next %1.8E %1.8E\n",
					//					iVertex, our_v.vxy.y, next_v.vxy.y, prev_v.vxy.y, opp_v.vxy.y,
					//					area_quadrilateral,
					//					info.pos.x, info.pos.y, opppos.x, opppos.y, prevpos.x, prevpos.y, nextpos.x, nextpos.y);
					//
					gradvz.x = 0.5*(
						(shared_v_n_verts[threadIdx.x].z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + shared_v_n_verts[threadIdx.x].z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvz.y = -0.5*(
						(shared_v_n_verts[threadIdx.x].z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + shared_v_n_verts[threadIdx.x].z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					htg_diff.x = shared_v_n_verts[threadIdx.x].x - opp_v.x;
					htg_diff.y = shared_v_n_verts[threadIdx.x].y - opp_v.y;
					htg_diff.z = shared_v_n_verts[threadIdx.x].z - opp_v.z;

					if (TESTNEUTVISC)
						printf("============================\nNeutral viscosity %d tri %d ita_par %1.10E\n"
							"v %1.9E %1.9E %1.9E  opp_v %1.9E %1.9E %1.9E\n"
							"gradvx %1.9E %1.9E gradvy %1.9E %1.9E gradvz %1.9E %1.9E \n"
							"ourpos %1.8E %1.8E prevpos %1.8E %1.8E opppos %1.8E %1.8E nextpos %1.8E %1.8E edge_nor %1.9E %1.9E\n"
							,
							iVertex, izTri[i], ita_par,
							shared_v_n_verts[threadIdx.x].x, shared_v_n_verts[threadIdx.x].y,
							shared_v_n_verts[threadIdx.x].z, opp_v.x, opp_v.y, opp_v.z,
							gradvx.x, gradvx.y, gradvy.x, gradvy.y, gradvz.x, gradvz.y,
							info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
							edge_normal.x, edge_normal.y);
				}

				// Order of calculations may help things to go out/into scope at the right times so careful with that.

				// we also want to get nu from somewhere. So precompute nu at the time we precompute ita_e = n Te / nu_e, ita_i = n Ti / nu_i. 

				if (ita_par > 0.0)
				{
					// For neutral fluid viscosity does not involve dimensional transfers.

					f64_vec3 visc_contrib;
					visc_contrib.x = over_m_n*(ita_par*gradvx.dot(edge_normal)); // if we are looking at higher vz looking out, go up.
					visc_contrib.y = over_m_n*(ita_par*gradvy.dot(edge_normal));
					visc_contrib.z = over_m_n*(ita_par*gradvz.dot(edge_normal));

					//		if (iVertex == VERTCHOSEN) {
					//			printf("visc_contrib %1.9E %1.9E %1.9E  ita %1.10E \n",
					//				visc_contrib.x, visc_contrib.y, visc_contrib.z, ita_par);
					//		}

					ownrates_visc += visc_contrib;
					visc_htg += -THIRD*m_n*(htg_diff.dot(visc_contrib));

					if (TESTNEUTVISC)
						printf("htg_diff %1.9E %1.9E %1.9E visc_contrib %1.9E %1.9E %1.9E visc_htg %1.10E\n"
							,
							htg_diff.x, htg_diff.y, htg_diff.z, visc_contrib.x, visc_contrib.y, visc_contrib.z,
							visc_htg
						);

				}

				// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);

			}; // next i

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &ownrates, sizeof(f64_vec3));

			p_NT_addition_rate[iVertex].NnTn += visc_htg;
			if (TESTNEUTVISC) {
				printf("%d : cumulative d/dt NnTn %1.10E \n", iVertex, p_NT_addition_rate[iVertex].NnTn);
			};
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iVertex %d NaN ownrates.x\n", iVertex);
			if (ownrates.y != ownrates.y)
				printf("iVertex %d NaN ownrates.y\n", iVertex);
			if (ownrates.z != ownrates.z)
				printf("iVertex %d NaN ownrates.z\n", iVertex);
			if (visc_htg != visc_htg) printf("iVertex %d NAN VISC HTG\n", iVertex);
#endif
		}
		else {
			// NOT domain vertex: Do nothing			
		};
	};
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	info = p_info_minor[iMinor];

	// memcpy(&(ownrates), &(p_MAR_ion[iMinor]), sizeof(f64_vec3));
	memset(&ownrates_visc, 0, sizeof(f64_vec3));
	visc_htg = 0.0;

	{
		long izNeighMinor[6];
		char szPBC[6];

		if (TESTNEUTVISC2) printf("%d info.flag %d ita_ours %1.8E \n", iMinor, info.flag, shared_ita_par[threadIdx.x]);

		// JUST TO GET IT TO RUN:
		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) &&
			(info.pos.modulus() < 4.9) && (shared_ita_par[threadIdx.x] > 0.0)) {

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (short i = 0; i < 6; i++)
			{
				bool bUsableSide = true;
				{
					// newly uncommented:
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
						{
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
							nu = shared_nu[izNeighMinor[i] - StartMinor];
						};

						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							};
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							f64 ita_par_opp = p_ita_neut_minor[izNeighMinor[i]];
							f64 nu_theirs = p_nu_neut_minor[izNeighMinor[i]];
							if (shared_ita_par[threadIdx.x] < ita_par_opp) {
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
							}
							else {
								ita_par = ita_par_opp;
								nu = nu_theirs;
							}
							if (ita_par_opp == 0.0) bUsableSide = false;
						};
					}
				}
				// basically bUsableSide here just depends on whether min(ita, ita_opp) == 0.


				f64_vec2 gradvx, gradvy, gradvz;
				f64_vec2 edge_normal;
				f64_vec3 htg_diff;

				if (bUsableSide)
				{
					short inext = i + 1; if (inext > 5) inext = 0;
					short iprev = i - 1; if (iprev < 0) iprev = 5;
					f64_vec3 prev_v, opp_v, next_v;
					f64_vec2 prevpos, nextpos, opppos;

					if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
					{
						memcpy(&prev_v, &(shared_v_n[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
						prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
					}
					else {
						if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&prev_v, &(shared_v_n_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							prevpos = p_info_minor[izNeighMinor[iprev]].pos;
							memcpy(&prev_v, &(p_v_n_minor[izNeighMinor[iprev]]), sizeof(f64_vec3));

						};
					};
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						RotateClockwise(prev_v);
					};
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						RotateAnticlockwise(prev_v);
					};

					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						memcpy(&opp_v, &(shared_v_n[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
						opppos = shared_pos[izNeighMinor[i] - StartMinor];
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&opp_v, &(shared_v_n_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							opppos = p_info_minor[izNeighMinor[i]].pos;
							memcpy(&opp_v, &(p_v_n_minor[izNeighMinor[i]]), sizeof(f64_vec3));
						};
					};
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						RotateAnticlockwise(opp_v);
					}

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						memcpy(&next_v, &(shared_v_n[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&next_v, &(shared_v_n_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
							memcpy(&next_v, &(p_v_n_minor[izNeighMinor[inext]]), sizeof(f64_vec3));
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						RotateClockwise(next_v);
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						RotateAnticlockwise(next_v);
					}

					f64 area_quadrilateral = 0.5*(
						(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
						+ (prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
						+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
						+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y)
						);

					gradvx.x = 0.5*(
						(shared_v_n[threadIdx.x].x + next_v.x)*(info.pos.y - nextpos.y)
						+ (prev_v.x + shared_v_n[threadIdx.x].x)*(prevpos.y - info.pos.y)
						+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
						+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvx.y = -0.5*(
						(shared_v_n[threadIdx.x].x + next_v.x)*(info.pos.x - nextpos.x)
						+ (prev_v.x + shared_v_n[threadIdx.x].x)*(prevpos.x - info.pos.x)
						+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
						+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvy.x = 0.5*(
						(shared_v_n[threadIdx.x].y + next_v.y)*(info.pos.y - nextpos.y)
						+ (prev_v.y + shared_v_n[threadIdx.x].y)*(prevpos.y - info.pos.y)
						+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
						+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvy.y = -0.5*(
						(shared_v_n[threadIdx.x].y + next_v.y)*(info.pos.x - nextpos.x)
						+ (prev_v.y + shared_v_n[threadIdx.x].y)*(prevpos.x - info.pos.x)
						+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
						+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					gradvz.x = 0.5*(
						(shared_v_n[threadIdx.x].z + next_v.z)*(info.pos.y - nextpos.y)
						+ (prev_v.z + shared_v_n[threadIdx.x].z)*(prevpos.y - info.pos.y)
						+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
						+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
						) / area_quadrilateral;
					gradvz.y = -0.5*(
						(shared_v_n[threadIdx.x].z + next_v.z)*(info.pos.x - nextpos.x)
						+ (prev_v.z + shared_v_n[threadIdx.x].z)*(prevpos.x - info.pos.x)
						+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
						+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x) // nextpos = pos_anti, assumed
						) / area_quadrilateral;

					if (TESTNEUTVISC2) {
						printf("%d i %d prev_v %1.10E our_v %1.10E opp_v %1.10E next_v %1.10E\n",
							iMinor, i, prev_v.y, shared_v_n[threadIdx.x].y, opp_v.y, next_v.y);
					};

					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if (flag == CROSSING_INS) {

							gradvx.x = 0.0;
							gradvx.y = 0.0;
							gradvy.x = 0.0;
							gradvy.y = 0.0;
							gradvz.x = 0.0;
							gradvz.y = 0.0;
							bUsableSide = 0;
							/*

							if (prev_v.x == 0.0) // prev is in the insulator. ---- this seems like a dodgy way of trying to know this.
							{
								// do like the above but it goes (ours, next, opp) somehow?

								f64 area_triangle = 0.5*(
									(info.pos.x + nextpos.x)*(info.pos.y - nextpos.y)
									+ (opppos.x + info.pos.x)*(opppos.y - info.pos.y)
									+ (nextpos.x + opppos.x)*(nextpos.y - opppos.y));

								gradvx.x = 0.5*(
									(shared_v_n[threadIdx.x].x + next_v.x)*(info.pos.y - nextpos.y)
									+ (opp_v.x + shared_v_n[threadIdx.x].x)*(opppos.y - info.pos.y)
									+ (next_v.x + opp_v.x)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvx.y = -0.5*(
									(shared_v_n[threadIdx.x].x + next_v.x)*(info.pos.x - nextpos.x)
									+ (opp_v.x + shared_v_n[threadIdx.x].x)*(opppos.x - info.pos.x)
									+ (next_v.x + opp_v.x)*(nextpos.x - opppos.x)
									) / area_triangle;

								gradvy.x = 0.5*(
									(shared_v_n[threadIdx.x].y + next_v.y)*(info.pos.y - nextpos.y)
									+ (opp_v.y + shared_v_n[threadIdx.x].y)*(opppos.y - info.pos.y)
									+ (next_v.y + opp_v.y)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvy.y = -0.5*(
									(shared_v_n[threadIdx.x].y + next_v.y)*(info.pos.x - nextpos.x)
									+ (opp_v.y + shared_v_n[threadIdx.x].y)*(opppos.x - info.pos.x)
									+ (next_v.y + opp_v.y)*(nextpos.x - opppos.x)
									) / area_triangle;

								gradvz.x = 0.5*(
									(shared_v_n[threadIdx.x].z + next_v.z)*(info.pos.y - nextpos.y)
									+ (opp_v.z + shared_v_n[threadIdx.x].z)*(opppos.y - info.pos.y)
									+ (next_v.z + opp_v.z)*(nextpos.y - opppos.y) // nextpos = pos_anti, assumed
									) / area_triangle;
								gradvz.y = -0.5*(
									(shared_v_n[threadIdx.x].z + next_v.z)*(info.pos.x - nextpos.x)
									+ (opp_v.z + shared_v_n[threadIdx.x].z)*(opppos.x - info.pos.x)
									+ (next_v.z + opp_v.z)*(nextpos.x - opppos.x)
									) / area_triangle;

								if (TESTNEUTVISC2) {
									printf("%d i %d PREVV=0 our_v %1.10E opp_v %1.10E next_v %1.10E\n",
										iMinor, i, shared_v_n[threadIdx.x].y, opp_v.y, next_v.y);
								};
							}
							else {
								if (next_v.x == 0.0) // next is in the insulator
								{
									f64 area_triangle = 0.5*(
										(prevpos.x + info.pos.x)*(prevpos.y - info.pos.y)
										+ (opppos.x + prevpos.x)*(opppos.y - prevpos.y)
										+ (info.pos.x + opppos.x)*(info.pos.y - opppos.y)
										);

									gradvx.x = 0.5*(
										(prev_v.x + shared_v_n[threadIdx.x].x)*(prevpos.y - info.pos.y)
										+ (opp_v.x + prev_v.x)*(opppos.y - prevpos.y)
										+ (shared_v_n[threadIdx.x].x + opp_v.x)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvx.y = -0.5*(
										(prev_v.x + shared_v_n[threadIdx.x].x)*(prevpos.x - info.pos.x)
										+ (opp_v.x + prev_v.x)*(opppos.x - prevpos.x)
										+ (shared_v_n[threadIdx.x].x + opp_v.x)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
										) / area_triangle;

									gradvy.x = 0.5*(
										(prev_v.y + shared_v_n[threadIdx.x].y)*(prevpos.y - info.pos.y)
										+ (opp_v.y + prev_v.y)*(opppos.y - prevpos.y)
										+ (shared_v_n[threadIdx.x].y + opp_v.y)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvy.y = -0.5*(
										(prev_v.y + shared_v_n[threadIdx.x].y)*(prevpos.x - info.pos.x)
										+ (opp_v.y + prev_v.y)*(opppos.x - prevpos.x)
										+ (shared_v_n[threadIdx.x].y + opp_v.y)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
										) / area_triangle;

									if (TESTNEUTVISC2) {
										printf("%d i %d NEXTV=0 our_v %1.10E opp_v %1.10E prev_v %1.10E\n",
											iMinor, i, shared_v_n[threadIdx.x].y, opp_v.y, prev_v.y);
									};

									gradvz.x = 0.5*(
										(prev_v.z + shared_v_n[threadIdx.x].z)*(prevpos.y - info.pos.y)
										+ (opp_v.z + prev_v.z)*(opppos.y - prevpos.y)
										+ (shared_v_n[threadIdx.x].z + opp_v.z)*(info.pos.y - opppos.y) // nextpos = pos_anti, assumed
										) / area_triangle;
									gradvz.y = -0.5*(
										(prev_v.z + shared_v_n[threadIdx.x].z)*(prevpos.x - info.pos.x)
										+ (opp_v.z + prev_v.z)*(opppos.x - prevpos.x)
										+ (shared_v_n[threadIdx.x].z + opp_v.z)*(info.pos.x - opppos.x) // nextpos = pos_anti, assumed
										) / area_triangle;

								}
								else {
									printf("\n\n\nDid not make sense! Alert RING-TAILED LEMUR. iMinor %d iNiegh %d \n"
										"izNeighMinor[inext] %d izNeighMinor[iprev] %d flag %d %d \n"
										"prev_v.x %1.8E next_v.x %1.8E \n"
										"\n\n\a", iMinor,
										izNeighMinor[i],
										izNeighMinor[inext], izNeighMinor[iprev], p_info_minor[izNeighMinor[inext]].flag,
										p_info_minor[izNeighMinor[iprev]].flag, prev_v.x, next_v.x);
								};
							};
							*/
						};
					};

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors

					htg_diff.x = shared_v_n[threadIdx.x].x - opp_v.x;
					htg_diff.y = shared_v_n[threadIdx.x].y - opp_v.y;
					htg_diff.z = shared_v_n[threadIdx.x].z - opp_v.z;
					
					//					if (iMinor == CHOSEN) printf("============================\nNeutral viscosity %d %d\n"
					//							"v.x %1.9E  opp_v.x %1.9E prev_v.x %1.9E next_v.x %1.9E\n"
					//							"ourpos %1.9E %1.9E \n"
					//							"prevpos %1.9E %1.9E \n"
					//							"opppos %1.9E %1.9E \n"
					//							"nextpos %1.9E %1.9E \n"
					//							"gradvx %1.9E %1.9E gradvy %1.9E %1.9E edge_nor %1.9E %1.9E\n",
					//							iMinor, izNeighMinor[i],
					//							shared_v_n[threadIdx.x].x, opp_v.x, prev_v.x, next_v.x,
					//							info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
					//							gradvx.x, gradvx.y, gradvy.x, gradvy.y, edge_normal.x, edge_normal.y);
					//
				};
				if (bUsableSide) {

					f64_vec3 visc_contrib;
					visc_contrib.x = over_m_n*ita_par*gradvx.dot(edge_normal);
					visc_contrib.y = over_m_n*ita_par*gradvy.dot(edge_normal);
					visc_contrib.z = over_m_n*ita_par*gradvz.dot(edge_normal);


					// Set to 0 any that are pushing momentum uphill. For neutral this is unphysical.
					//	if (visc_contrib.x*htg_diff.x > 0.0) visc_contrib.x = 0.0;
					// Can't do it because it'll ruin backward solve.

					ownrates_visc += visc_contrib;

					if (TESTNEUTVISC2) {
						printf("%d i %d contrib.y %1.10E gradvy %1.10E %1.10E edge_nml %1.9E %1.9E ita %1.8E /m_n %1.8E cumu %1.9E\n",
							iMinor, i, visc_contrib.y, gradvy.x, gradvy.y, edge_normal.x, edge_normal.y, ita_par, over_m_n, ownrates_visc.y);
					};

					if (i % 2 == 0) {
						// vertex : heat collected by vertex
					}
					else {
						visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
					};


				}; // bUsableSide
			};

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iMinor]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(&(p_MAR_neut[iMinor]), &(ownrates), sizeof(f64_vec3));

			p_NT_addition_tri[iMinor].NnTn += visc_htg;

			// We will have to round this up into the vertex heat afterwards.

#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iMinor %d NaN ownrates.x\n", iMinor);
			if (ownrates.y != ownrates.y)
				printf("iMinor %d NaN ownrates.y\n", iMinor);
			if (ownrates.z != ownrates.z)
				printf("iMinor %d NaN ownrates.z\n", iMinor);

			if (visc_htg != visc_htg) printf("iMinor %d NAN VISC HTG\n", iMinor);
#endif

			// We do best by taking each boundary, considering how
			// much heat to add for each one.

		}
		else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		}
	} // scope

}


__global__ void kernelExpandSelectFlagIta(
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izTri,
	long * __restrict__ p_izNeighMinor,
	int * __restrict__ p_iSelectFlag,
	int * __restrict__ p_iSelectflagNeut,
	int const number
) {

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {

		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		long izTri[MAXNEIGH_d];
		short tri_len = info.neigh_len;
		memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));

		if (p_iSelectFlag[iVertex + BEGINNING_OF_CENTRAL] == 0) {

			bool found = false;
			for (short i = 0; ((i < tri_len) && (found == false)); i++)
			{
				if (p_iSelectFlag[izTri[i]] == number) {
					found = true;
				};
			};
			if (found) p_iSelectFlag[iVertex + BEGINNING_OF_CENTRAL] = number + 1;
		};

		if (p_iSelectflagNeut[iVertex + BEGINNING_OF_CENTRAL] == 0) {

			bool found = false;
			for (short i = 0; ((i < tri_len) && (found == false)); i++)
			{
				if (p_iSelectflagNeut[izTri[i]] == number) {
					found = true;
				};
			};
			if (found) p_iSelectflagNeut[iVertex + BEGINNING_OF_CENTRAL] = number + 1;
		};
	};

	info = p_info_minor[iMinor];
	long izNeighMinor[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);

	if (p_iSelectFlag[iMinor] == 0) {
		bool found = false;
		for (short i = 0; ((i < 6) && (found == false)); i++)
		{
			if (p_iSelectFlag[izNeighMinor[i]] == number) {
					found = true;
				};			
		};
		if (found) p_iSelectFlag[iMinor] = number + 1;	
	};

	if (p_iSelectflagNeut[iMinor] == 0) {
		bool found = false;
		for (short i = 0; ((i < 6) && (found == false)); i++)
		{
			if (p_iSelectflagNeut[izNeighMinor[i]] == number) {
				found = true;
			};
		};
		if (found) p_iSelectflagNeut[iMinor] = number + 1;
	};
}


// Neutral routine:
__global__ void kernelCreate_neutral_viscous_contrib_to_MAR_and_NT_Geometric(
	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_v_n_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_neut_minor,   //
	f64 * __restrict__ p_nu_neut_minor,   // 

	f64_vec3 * __restrict__ p_MAR_neut,
	NTrates * __restrict__ p_NT_addition_rate,
	NTrates * __restrict__ p_NT_addition_tri)
{

	// ************************************************************************
	// ***********   WATCH OUT ************************************************
	// ************************************************************************
	// A copy of this routine with fixed flows only is in heatflux.cu

	__shared__ f64_vec3 shared_v_n[threadsPerTileMinor]; // sort of thing we want as input
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];

	__shared__ f64_vec3 shared_v_n_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

														 // There is room for some more double in shared per thread.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!
	f64_vec3 ownrates_visc;
	f64 visc_htg;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_v_n[threadIdx.x] = p_v_n_minor[iMinor];
	shared_ita_par[threadIdx.x] = p_ita_neut_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_neut_minor[iMinor];

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
		{
			memcpy(&(shared_v_n_verts[threadIdx.x]), &(p_v_n_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = p_ita_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		}
		else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.			
			memset(&(shared_v_n_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	// How shall we arrange to do v_n, which is isotropic? Handle this first...
	// Is the v_n coefficient negligible? Check.

	// We actually have to think how to handle the x-y dimension. PopOhms will handle it.

	// We can re-use some shared data -- such as pos and B -- to do both ions and electrons
	// But they use different ita_par and different vez, viz. 
	// Often we don't need to do magnetised ion viscosity when we do magnetised electron.


	if (threadIdx.x < threadsPerTileMajor) {
		memset(&ownrates_visc, 0, sizeof(f64_vec3));
		visc_htg = 0.0;

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!

				// JUST TO GET IT TO RUN: LIMIT OURSELVES TO RADIUS 4.9 : 
		// !
		if ((info.flag == DOMAIN_VERTEX) 
			// && (info.pos.modulus() < 4.9) -- if we have this then need in d/dbeta also.
			&& (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{
			// We are losing energy if there is viscosity into OUTERMOST.

			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

#pragma unroll 
			for (short i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think

				{
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						if (shared_ita_par_verts[threadIdx.x] < shared_ita_par[izTri[i] - StartMinor])
						{
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izTri[i] - StartMinor];
							nu = shared_nu[izTri[i] - StartMinor];
						};
					}
					else {
						f64 ita_theirs = p_ita_neut_minor[izTri[i]];
						f64 nu_theirs = p_nu_neut_minor[izTri[i]];
						if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
							ita_par = shared_ita_par_verts[threadIdx.x];
							nu = shared_nu_verts[threadIdx.x];
						}
						else {
							ita_par = ita_theirs;
							nu = nu_theirs;
						};

						// I understand why we are still doing minimum ita at the wall but we would ideally like to stop.

					};
				} // Guaranteed DOMAIN_VERTEX never needs to skip an edge; we include CROSSING_INS in viscosity.

				f64_vec2 gradvx, gradvy, gradvz;
				f64_vec3 htg_diff;
				f64_vec2 edge_normal;

				if (ita_par > 0.0) // note it was the minimum taken.
				{
					f64_vec3 opp_v, prev_v, next_v;
					f64_vec2 opppos, prevpos, nextpos;
					// ideally we might want to leave position out of the loop so that we can avoid reloading it.

					short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
					short inext = i + 1; if (inext >= tri_len) inext = 0;

					if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
					{
						prev_v = shared_v_n[izTri[iprev] - StartMinor];
						prevpos = shared_pos[izTri[iprev] - StartMinor];
					}
					else {
						prev_v = p_v_n_minor[izTri[iprev]];
						prevpos = p_info_minor[izTri[iprev]].pos;
					}
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						RotateClockwise(prev_v);
					}
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						RotateAnticlockwise(prev_v);
					}

					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_v = shared_v_n[izTri[i] - StartMinor];
						opppos = shared_pos[izTri[i] - StartMinor];

						//		if (iVertex == VERTCHOSEN) printf("opp_v %1.9E izTri[i] %d \n", opp_v.x, izTri[i]);

					}
					else {
						opp_v = p_v_n_minor[izTri[i]];
						opppos = p_info_minor[izTri[i]].pos;

						//	if (iVertex == VERTCHOSEN) printf("opp_v %1.9E v_n_minor izTri[i] %d \n", opp_v.x, izTri[i]);
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						RotateAnticlockwise(opp_v);
					}

					if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
					{
						next_v = shared_v_n[izTri[inext] - StartMinor];
						nextpos = shared_pos[izTri[inext] - StartMinor];
					}
					else {
						next_v = p_v_n_minor[izTri[inext]];
						nextpos = p_info_minor[izTri[inext]].pos;
					}
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						RotateClockwise(next_v);
					}
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						RotateAnticlockwise(next_v);
					}

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);

					if ((TestDomainPos(prevpos) == false) || (TestDomainPos(nextpos) == false))
						edge_normal = ReconstructEdgeNormal(prevpos, info.pos, nextpos, opppos);

#ifdef INS_INS_3POINT
					if (TestDomainPos(prevpos) == false) {

						gradvx = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n_verts[threadIdx.x].x, next_v.x, opp_v.x
						);
						gradvy = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n_verts[threadIdx.x].y, next_v.y, opp_v.y
						);
						gradvz = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n_verts[threadIdx.x].z, next_v.z, opp_v.z
						);

					} else {
						if (TestDomainPos(nextpos) == false) {

							gradvx = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.x, shared_v_n_verts[threadIdx.x].x, opp_v.x
							);
							gradvy = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.y, shared_v_n_verts[threadIdx.x].y, opp_v.y
							);
							gradvz = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.z, shared_v_n_verts[threadIdx.x].z, opp_v.z
							);

						} else {
							gradvx = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.x, shared_v_n_verts[threadIdx.x].x, next_v.x, opp_v.x
							);
							gradvy = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.y, shared_v_n_verts[threadIdx.x].y, next_v.y, opp_v.y
							);
							gradvz = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.z, shared_v_n_verts[threadIdx.x].z, next_v.z, opp_v.z
							);
						};
					};

#else
					if ((TestDomainPos(prevpos) == false) || (TestDomainPos(nextpos) == false))
					{
						// One of the sides is dipped under the insulator -- set transverse deriv to 0.
						// Bear in mind we are looking from a vertex into a tri, it can be ins tri.

						gradvx = (opp_v.x - shared_v_n_verts[threadIdx.x].x)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);
						gradvy = (opp_v.y - shared_v_n_verts[threadIdx.x].y)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);
						gradvz = (opp_v.z - shared_v_n_verts[threadIdx.x].z)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);

					} else {
						gradvx = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.x, shared_v_n_verts[threadIdx.x].x, next_v.x, opp_v.x
						);
						gradvy = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.y, shared_v_n_verts[threadIdx.x].y, next_v.y, opp_v.y
						);
						gradvz = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.z, shared_v_n_verts[threadIdx.x].z, next_v.z, opp_v.z
						);

						// Could switch to the 3 in one function that handles all 3. in one.
					};
					// Simplify:
#endif
					htg_diff.x = shared_v_n_verts[threadIdx.x].x - opp_v.x;
					htg_diff.y = shared_v_n_verts[threadIdx.x].y - opp_v.y;
					htg_diff.z = shared_v_n_verts[threadIdx.x].z - opp_v.z;

					if (TESTNEUTVISC)
						printf("============================\nNeutral viscosity %d tri %d ita_par %1.10E\n"
							"v %1.9E %1.9E %1.9E  opp_v %1.9E %1.9E %1.9E\n"
							"gradvx %1.9E %1.9E gradvy %1.9E %1.9E gradvz %1.9E %1.9E \n"
							"ourpos %1.8E %1.8E prevpos %1.8E %1.8E opppos %1.8E %1.8E nextpos %1.8E %1.8E edge_nor %1.9E %1.9E\n"
							,
							iVertex, izTri[i], ita_par,
							shared_v_n_verts[threadIdx.x].x, shared_v_n_verts[threadIdx.x].y,
							shared_v_n_verts[threadIdx.x].z, opp_v.x, opp_v.y, opp_v.z,
							gradvx.x, gradvx.y, gradvy.x, gradvy.y, gradvz.x, gradvz.y,
							info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
							edge_normal.x, edge_normal.y);
				}

				// Order of calculations may help things to go out/into scope at the right times so careful with that.

				// we also want to get nu from somewhere. So precompute nu at the time we precompute ita_e = n Te / nu_e, ita_i = n Ti / nu_i. 

				if (ita_par > 0.0)
				{
					// For neutral fluid viscosity does not involve dimensional transfers.

					f64_vec3 visc_contrib;
					visc_contrib.x = over_m_n*(ita_par*gradvx.dot(edge_normal)); // if we are looking at higher vz looking out, go up.
					visc_contrib.y = over_m_n*(ita_par*gradvy.dot(edge_normal));
					visc_contrib.z = over_m_n*(ita_par*gradvz.dot(edge_normal));

					//		if (iVertex == VERTCHOSEN) {
					//			printf("visc_contrib %1.9E %1.9E %1.9E  ita %1.10E \n",
					//				visc_contrib.x, visc_contrib.y, visc_contrib.z, ita_par);
					//		}

					ownrates_visc += visc_contrib;
					visc_htg += -THIRD*m_n*(htg_diff.dot(visc_contrib));

					if (TESTNEUTVISC)
						printf("htg_diff %1.9E %1.9E %1.9E visc_contrib %1.9E %1.9E %1.9E visc_htg %1.10E\n"
							,
							htg_diff.x, htg_diff.y, htg_diff.z, visc_contrib.x, visc_contrib.y, visc_contrib.z,
							visc_htg
						);

				}

				// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);

			}; // next i

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &ownrates, sizeof(f64_vec3));

			p_NT_addition_rate[iVertex].NnTn += visc_htg;
			if (TESTNEUTVISC) {
				printf("%d : cumulative d/dt NnTn %1.10E \n", iVertex, p_NT_addition_rate[iVertex].NnTn);
			};
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iVertex %d NaN ownrates.x\n", iVertex);
			if (ownrates.y != ownrates.y)
				printf("iVertex %d NaN ownrates.y\n", iVertex);
			if (ownrates.z != ownrates.z)
				printf("iVertex %d NaN ownrates.z\n", iVertex);
			if (visc_htg != visc_htg) printf("iVertex %d NAN VISC HTG\n", iVertex);
#endif
		}
		else {
			// NOT domain vertex: Do nothing			
		};
	};
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	info = p_info_minor[iMinor];

	// memcpy(&(ownrates), &(p_MAR_ion[iMinor]), sizeof(f64_vec3));
	memset(&ownrates_visc, 0, sizeof(f64_vec3));

#ifdef COLLECT_VISC_HTG_IN_TRIANGLES
	visc_htg = 0.0;
#else
	f64 visc_htg0, visc_htg1, visc_htg2;
	visc_htg0 = 0.0;
	visc_htg1 = 0.0;
	visc_htg2 = 0.0;
#endif

	{
		long izNeighMinor[6];
		char szPBC[6];

		if (TESTNEUTVISC2) printf("%d info.flag %d ita_ours %1.8E \n", iMinor, info.flag, shared_ita_par[threadIdx.x]);

		// JUST TO GET IT TO RUN:
		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) 
			// && (info.pos.modulus() < 4.9) // if we have this then we have to have it in d/dbeta routine also.
			&& (shared_ita_par[threadIdx.x] > 0.0)) {

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (short i = 0; i < 6; i++)
			{
				bool bUsableSide = true;
				{
					// newly uncommented:
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
						{
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
							nu = shared_nu[izNeighMinor[i] - StartMinor];
						};

						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							};
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							f64 ita_par_opp = p_ita_neut_minor[izNeighMinor[i]];
							f64 nu_theirs = p_nu_neut_minor[izNeighMinor[i]];
							if (shared_ita_par[threadIdx.x] < ita_par_opp) {
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
							}
							else {
								ita_par = ita_par_opp;
								nu = nu_theirs;
							}
							if (ita_par_opp == 0.0) bUsableSide = false;
						};
					}
				}
				// basically bUsableSide here just depends on whether min(ita, ita_opp) == 0.

				bool bLongi = false;
#ifdef INS_INS_NONE
				// Get rid of ins-ins triangle traffic:
				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if (flag == CROSSING_INS)
						bUsableSide = 0;
				}
		//		if ((TestDomainPos(prevpos) == false) || (TestDomainPos(nextpos) == false))
		//			bLongi = true;
				// have to put it below
#else
				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if (flag == CROSSING_INS)
						bLongi = true;
				}
#endif


				f64_vec2 gradvx, gradvy, gradvz;
				f64_vec2 edge_normal;
				f64_vec3 htg_diff;

				if (bUsableSide)
				{
					short inext = i + 1; if (inext > 5) inext = 0;
					short iprev = i - 1; if (iprev < 0) iprev = 5;
					f64_vec3 prev_v, opp_v, next_v;
					f64_vec2 prevpos, nextpos, opppos;

					if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
					{
						memcpy(&prev_v, &(shared_v_n[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
						prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
					}
					else {
						if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&prev_v, &(shared_v_n_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							prevpos = p_info_minor[izNeighMinor[iprev]].pos;
							memcpy(&prev_v, &(p_v_n_minor[izNeighMinor[iprev]]), sizeof(f64_vec3));

						};
					};
					if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
						prevpos = Clockwise_d*prevpos;
						RotateClockwise(prev_v);
					};
					if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
						prevpos = Anticlockwise_d*prevpos;
						RotateAnticlockwise(prev_v);
					};

					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						memcpy(&opp_v, &(shared_v_n[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
						opppos = shared_pos[izNeighMinor[i] - StartMinor];
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&opp_v, &(shared_v_n_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							opppos = p_info_minor[izNeighMinor[i]].pos;
							memcpy(&opp_v, &(p_v_n_minor[izNeighMinor[i]]), sizeof(f64_vec3));
						};
					};
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						opppos = Clockwise_d*opppos;
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						opppos = Anticlockwise_d*opppos;
						RotateAnticlockwise(opp_v);
					}

					if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
					{
						memcpy(&next_v, &(shared_v_n[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
						nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					}
					else {
						if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&next_v, &(shared_v_n_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
							nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						}
						else {
							nextpos = p_info_minor[izNeighMinor[inext]].pos;
							memcpy(&next_v, &(p_v_n_minor[izNeighMinor[inext]]), sizeof(f64_vec3));
						};
					};
					if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
						nextpos = Clockwise_d*nextpos;
						RotateClockwise(next_v);
					};
					if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
						nextpos = Anticlockwise_d*nextpos;
						RotateAnticlockwise(next_v);
					};

					if ((TestDomainPos(prevpos) == false) || (TestDomainPos(nextpos) == false))
						bLongi = true;
#ifdef INS_INS_3POINT
					if (TestDomainPos(prevpos) == false) {

						gradvx = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n[threadIdx.x].x, next_v.x, opp_v.x
						);
						gradvy = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n[threadIdx.x].y, next_v.y, opp_v.y
						);
						gradvz = GetGradient_3Point(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							shared_v_n[threadIdx.x].z, next_v.z, opp_v.z
						);

					} else {
						if (TestDomainPos(nextpos) == false) {

							gradvx = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.x, shared_v_n[threadIdx.x].x, opp_v.x
							);
							gradvy = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.y, shared_v_n[threadIdx.x].y, opp_v.y
							);
							gradvz = GetGradient_3Point(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.z, shared_v_n[threadIdx.x].z, opp_v.z
							);

						} else {

							gradvx = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.x, shared_v_n[threadIdx.x].x, next_v.x, opp_v.x
							);
							gradvy = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.y, shared_v_n[threadIdx.x].y, next_v.y, opp_v.y
							);
							gradvz = GetGradient(
								//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
								prevpos, info.pos, nextpos, opppos,
								//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
								prev_v.z, shared_v_n[threadIdx.x].z, next_v.z, opp_v.z
							);

						};
					};
#else
					if ((TestDomainPos(prevpos) == false) || (TestDomainPos(nextpos) == false))
					{
						// One of the sides is dipped under the insulator -- set transverse deriv to 0.
						// Bear in mind we are looking from a vertex into a tri, it can be ins tri.

						gradvx = (opp_v.x - shared_v_n[threadIdx.x].x)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);
						gradvy = (opp_v.y - shared_v_n[threadIdx.x].y)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);
						gradvz = (opp_v.z - shared_v_n[threadIdx.x].z)*(opppos - info.pos) /
							(opppos - info.pos).dot(opppos - info.pos);

					}
					else {
						gradvx = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.x, shared_v_n[threadIdx.x].x, next_v.x, opp_v.x
						);
						gradvy = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.y, shared_v_n[threadIdx.x].y, next_v.y, opp_v.y
						);
						gradvz = GetGradient(
							//f64_vec2 prevpos, f64_vec2 ourpos, f64_vec2 nextpos, f64_vec2 opppos,
							prevpos, info.pos, nextpos, opppos,
							//f64 prev_v, f64 our_v, f64 next_v, f64 opp_v
							prev_v.z, shared_v_n[threadIdx.x].z, next_v.z, opp_v.z
						);
					}
#endif
#ifdef INS_INS_NONE
					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if (flag == CROSSING_INS) {
							// just set it to 0.
							bUsableSide = false;
							gradvz.x = 0.0;
							gradvz.y = 0.0;
							gradvx.x = 0.0;
							gradvx.y = 0.0;
							gradvy.x = 0.0;
							gradvy.y = 0.0;
						};
					};
#endif

					htg_diff = shared_v_n[threadIdx.x] - opp_v;

					if (TESTNEUTVISC2) {
						printf("%d i %d prev_v %1.10E our_v %1.10E opp_v %1.10E next_v %1.10E\n",
							iMinor, i, prev_v.y, shared_v_n[threadIdx.x].y, opp_v.y, next_v.y);
					};

					edge_normal.x = THIRD * (nextpos.y - prevpos.y);
					edge_normal.y = THIRD * (prevpos.x - nextpos.x);  // need to define so as to create unit vectors


					//					if (iMinor == CHOSEN) printf("============================\nNeutral viscosity %d %d\n"
					//							"v.x %1.9E  opp_v.x %1.9E prev_v.x %1.9E next_v.x %1.9E\n"
					//							"ourpos %1.9E %1.9E \n"
					//							"prevpos %1.9E %1.9E \n"
					//							"opppos %1.9E %1.9E \n"
					//							"nextpos %1.9E %1.9E \n"
					//							"gradvx %1.9E %1.9E gradvy %1.9E %1.9E edge_nor %1.9E %1.9E\n",
					//							iMinor, izNeighMinor[i],
					//							shared_v_n[threadIdx.x].x, opp_v.x, prev_v.x, next_v.x,
					//							info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
					//							gradvx.x, gradvx.y, gradvy.x, gradvy.y, edge_normal.x, edge_normal.y);
					//

					if (bLongi) {
						// move any edge_normal endpoints that are below the insulator,
						// until they are above the insulator.
						edge_normal = ReconstructEdgeNormal(
							prevpos, info.pos, nextpos, opppos
						);
					};

				};
				if (bUsableSide) {

					f64_vec3 visc_contrib;
					visc_contrib.x = over_m_n*ita_par*gradvx.dot(edge_normal);
					visc_contrib.y = over_m_n*ita_par*gradvy.dot(edge_normal);
					visc_contrib.z = over_m_n*ita_par*gradvz.dot(edge_normal);

					// Set to 0 any that are pushing momentum uphill. For neutral this is unphysical.
					//	if (visc_contrib.x*htg_diff.x > 0.0) visc_contrib.x = 0.0;
					// Can't do it because it'll ruin backward solve.

					ownrates_visc += visc_contrib;

					if (TESTNEUTVISC2) {
						printf("%d i %d contrib.y %1.10E gradvy %1.10E %1.10E edge_nml %1.9E %1.9E ita %1.8E /m_n %1.8E cumu %1.9E\n",
							iMinor, i, visc_contrib.y, gradvy.x, gradvy.y, edge_normal.x, edge_normal.y, ita_par, over_m_n, ownrates_visc.y);
					};

					if (i % 2 == 0) {
						// vertex : heat collected by vertex
					}
					else {
#ifdef COLLECT_VISC_HTG_IN_TRIANGLES
						visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
#else
						f64 heat_addn = -THIRD*m_ion*(htg_diff.dot(visc_contrib));
						if (i == 1) {
							visc_htg0 += 0.5*heat_addn;
							visc_htg1 += 0.5*heat_addn;
						} else {
							if (i == 3) {
								visc_htg1 += 0.5*heat_addn;
								visc_htg2 += 0.5*heat_addn;
							} else {
								visc_htg0 += 0.5*heat_addn;
								visc_htg2 += 0.5*heat_addn;
							};
						};
#endif
					};
					
				}; // bUsableSide
			};

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iMinor]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(&(p_MAR_neut[iMinor]), &(ownrates), sizeof(f64_vec3));

#ifdef COLLECT_VISC_HTG_IN_TRIANGLES
			p_NT_addition_tri[iMinor].NnTn += visc_htg;
#else

			p_NT_addition_tri[iMinor * 3 + 0].NnTn += visc_htg0;
			p_NT_addition_tri[iMinor * 3 + 1].NnTn += visc_htg1;
			p_NT_addition_tri[iMinor * 3 + 2].NnTn += visc_htg2;

#endif

			// We will have to round this up into the vertex heat afterwards.

#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iMinor %d NaN ownrates.x\n", iMinor);
			if (ownrates.y != ownrates.y)
				printf("iMinor %d NaN ownrates.y\n", iMinor);
			if (ownrates.z != ownrates.z)
				printf("iMinor %d NaN ownrates.z\n", iMinor);

			if (visc_htg != visc_htg) printf("iMinor %d NAN VISC HTG\n", iMinor);
#endif

			// We do best by taking each boundary, considering how
			// much heat to add for each one.

		}
		else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		}
	} // scope

}


__global__ void kernelCreate_neutral_viscous_contrib_to_MAR_and_NT_SYMM(
	structural * __restrict__ p_info_minor,
	f64_vec3 * __restrict__ p_v_n_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,

	f64 * __restrict__ p_ita_neut_minor,   //
	f64 * __restrict__ p_nu_neut_minor,   // 

	f64_vec3 * __restrict__ p_MAR_neut,
	NTrates * __restrict__ p_NT_addition_rate,
	NTrates * __restrict__ p_NT_addition_tri)
{
	__shared__ f64_vec3 shared_v_n[threadsPerTileMinor]; // sort of thing we want as input
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64 shared_ita_par[threadsPerTileMinor]; // reuse for i,e ; or make 2 vars to combine the routines.
	__shared__ f64 shared_nu[threadsPerTileMinor];

	__shared__ f64_vec3 shared_v_n_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_ita_par_verts[threadsPerTileMajor];
	__shared__ f64 shared_nu_verts[threadsPerTileMajor]; // used for creating ita_perp, ita_cross

	// There is room for some more double in shared per thread.
	
	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	f64 nu, ita_par;  // optimization: we always each loop want to get rid of omega, nu once we have calc'd these, if possible!!
	f64_vec3 ownrates_visc;
	f64 visc_htg;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	shared_v_n[threadIdx.x] = p_v_n_minor[iMinor];
	shared_ita_par[threadIdx.x] = p_ita_neut_minor[iMinor];
	shared_nu[threadIdx.x] = p_nu_neut_minor[iMinor];

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST))
		{
			memcpy(&(shared_v_n_verts[threadIdx.x]), &(p_v_n_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = p_ita_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_nu_verts[threadIdx.x] = p_nu_neut_minor[iVertex + BEGINNING_OF_CENTRAL];
			// But now I am going to set ita == 0 in OUTERMOST and agree never to look there because that's fairer than one-way traffic and I don't wanna handle OUTERMOST?
			// I mean, I could handle it, and do flows only if the flag does not come up OUTER_FRILL.
			// OK just do that.
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.			
			memset(&(shared_v_n_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			shared_ita_par_verts[threadIdx.x] = 0.0;
			shared_nu_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	// How shall we arrange to do v_n, which is isotropic? Handle this first...
	// Is the v_n coefficient negligible? Check.

	// We actually have to think how to handle the x-y dimension. PopOhms will handle it.

	// We can re-use some shared data -- such as pos and B -- to do both ions and electrons
	// But they use different ita_par and different vez, viz. 
	// Often we don't need to do magnetised ion viscosity when we do magnetised electron.

	f64_vec2 cc0, cc1;

	if (threadIdx.x < threadsPerTileMajor) {
		memset(&ownrates_visc, 0, sizeof(f64_vec3));
		visc_htg = 0.0;

		long izTri[MAXNEIGH_d];
		char szPBC[MAXNEIGH_d];
		short tri_len = info.neigh_len; // ?!
		
		// JUST TO GET IT TO RUN: LIMIT OURSELVES TO RADIUS 4.5 : 
		if ((info.flag == DOMAIN_VERTEX) && (info.pos.modulus() < 4.5)
			&& (shared_ita_par_verts[threadIdx.x] > 0.0))
			//|| (info.flag == OUTERMOST)) 
		{
			// We are losing energy if there is viscosity into OUTERMOST.

			memcpy(izTri, p_izTri + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH_d, MAXNEIGH_d * sizeof(char));

			short i = 0; 
			f64_vec3 opp_v;// , prev_v, next_v; // never used

			f64_vec2 opppos, prevpos, nextpos;
			short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
			short inext = i + 1; if (inext >= tri_len) inext = 0;

			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
			}
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opp_v = shared_v_n[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];

				//		if (iVertex == VERTCHOSEN) printf("opp_v %1.9E izTri[i] %d \n", opp_v.x, izTri[i]);

			}
			else {
				opp_v = p_v_n_minor[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;

				//	if (iVertex == VERTCHOSEN) printf("opp_v %1.9E v_n_minor izTri[i] %d \n", opp_v.x, izTri[i]);
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				RotateClockwise(opp_v);
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				RotateAnticlockwise(opp_v);
			}

			CalculateCircumcenter(&cc0, info.pos, opppos, prevpos);

#pragma unroll 
			for (i = 0; i < tri_len; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think

				short iprev = i - 1; if (iprev < 0) iprev = tri_len - 1;
				short inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
				{
					if (shared_ita_par_verts[threadIdx.x] < shared_ita_par[izTri[i] - StartMinor])
					{
						ita_par = shared_ita_par_verts[threadIdx.x];
						nu = shared_nu_verts[threadIdx.x];
					}
					else {
						ita_par = shared_ita_par[izTri[i] - StartMinor];
						nu = shared_nu[izTri[i] - StartMinor];
					};
				} else {
					f64 ita_theirs = p_ita_neut_minor[izTri[i]];
					f64 nu_theirs = p_nu_neut_minor[izTri[i]];
					if (shared_ita_par_verts[threadIdx.x] < ita_theirs) {
						ita_par = shared_ita_par_verts[threadIdx.x];
						nu = shared_nu_verts[threadIdx.x];
					} else {
						ita_par = ita_theirs;
						nu = nu_theirs;
					};

					// I understand why we are still doing minimum ita at the wall but we would ideally like to stop.

				};
				// Guaranteed DOMAIN_VERTEX never needs to skip an edge; we include CROSSING_INS in viscosity.

			//	f64_vec2 gradvx, gradvy, gradvz;
				f64_vec3 htg_diff;
			//	f64_vec2 edge_normal;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
				}

				CalculateCircumcenter(&cc1, opppos, info.pos, nextpos);

				if (ita_par > 0.0) // note it was the minimum taken.
				{
					
					// ideally we might want to leave position out of the loop so that we can avoid reloading it.
					
					if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
					{
						opp_v = shared_v_n[izTri[i] - StartMinor];
				
				//		if (iVertex == VERTCHOSEN) printf("opp_v %1.9E izTri[i] %d \n", opp_v.x, izTri[i]);

					} else {
						opp_v = p_v_n_minor[izTri[i]];
					
					//	if (iVertex == VERTCHOSEN) printf("opp_v %1.9E v_n_minor izTri[i] %d \n", opp_v.x, izTri[i]);
					}
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						RotateAnticlockwise(opp_v);
					}

					// BEWARE OF WHEN EDGE ISN'T CLOCKWISE ORDERED -- WE TAKE NEGATIVE AREA as well?

					f64_vec3 deriv = (opp_v - shared_v_n_verts[threadIdx.x]) / (opppos - info.pos).modulus();

					f64_vec3 visc_contrib = over_m_n*ita_par*deriv*(cc1 - cc0).modulus();

					// there is an unnecessary sqrt but remember, sqrt is still cheaper than a divide.
					
					htg_diff.x = shared_v_n_verts[threadIdx.x].x - opp_v.x;
					htg_diff.y = shared_v_n_verts[threadIdx.x].y - opp_v.y;
					htg_diff.z = shared_v_n_verts[threadIdx.x].z - opp_v.z;
					
					if (TESTNEUTVISC) 
						printf("============================\nNeutral viscosity %d tri %d ita_par %1.10E\n"
							"v %1.9E %1.9E %1.9E  opp_v %1.9E %1.9E %1.9E\n"
							"ourpos %1.8E %1.8E prevpos %1.8E %1.8E opppos %1.8E %1.8E nextpos %1.8E %1.8E \n"
							,
							iVertex, izTri[i], ita_par,
							shared_v_n_verts[threadIdx.x].x, shared_v_n_verts[threadIdx.x].y,
							shared_v_n_verts[threadIdx.x].z, opp_v.x, opp_v.y, opp_v.z,							
							info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y);
				
					// For neutral fluid viscosity does not involve dimensional transfers.

					//f64_vec3 visc_contrib;
					//visc_contrib.x = over_m_n*(ita_par*gradvx.dot(edge_normal)); // if we are looking at higher vz looking out, go up.
					//visc_contrib.y = over_m_n*(ita_par*gradvy.dot(edge_normal));
					//visc_contrib.z = over_m_n*(ita_par*gradvz.dot(edge_normal));
				
					ownrates_visc += visc_contrib;
					visc_htg += -THIRD*m_n*(htg_diff.dot(visc_contrib));					

					if (TESTNEUTVISC)
						printf("htg_diff %1.9E %1.9E %1.9E visc_contrib %1.9E %1.9E %1.9E visc_htg %1.10E\n"
							,							
							htg_diff.x, htg_diff.y, htg_diff.z, visc_contrib.x, visc_contrib.y, visc_contrib.z,
							visc_htg
							);

				}
				
				// MAR_elec -= Make3(0.5*(n0 * T0.Te + n1 * T1.Te)*over_m_e*edge_normal, 0.0);
				// v0.vez = vie_k.vez + h_use * MAR.z / (n_use.n*AreaMinor);
							
				cc0 = cc1;
				prevpos = opppos;
				opppos = nextpos;
			}; // next i

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &ownrates, sizeof(f64_vec3));

			p_NT_addition_rate[iVertex].NnTn += visc_htg;
			if (TESTNEUTVISC) {
				printf("%d : cumulative d/dt NnTn %1.10E \n", iVertex, p_NT_addition_rate[iVertex].NnTn);
			};
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iVertex %d NaN ownrates.x\n", iVertex);
			if (ownrates.y != ownrates.y)
				printf("iVertex %d NaN ownrates.y\n", iVertex);
			if (ownrates.z != ownrates.z)
				printf("iVertex %d NaN ownrates.z\n", iVertex);
			if (visc_htg != visc_htg) printf("iVertex %d NAN VISC HTG\n", iVertex);
#endif
		} else {
			// NOT domain vertex: Do nothing			
		};
	};
	// __syncthreads(); // end of first vertex part
	// Do we need syncthreads? Not overwriting any shared data here...

	info = p_info_minor[iMinor];

	// memcpy(&(ownrates), &(p_MAR_ion[iMinor]), sizeof(f64_vec3));
	memset(&ownrates_visc, 0, sizeof(f64_vec3));
	visc_htg = 0.0;

	f64_vec3 opp_v;
	f64_vec2 prevpos, nextpos, opppos;
	{
		long izNeighMinor[6];
		char szPBC[6];

		if (TESTNEUTVISC2) printf("%d info.flag %d ita_ours %1.8E \n", iMinor, info.flag, shared_ita_par[threadIdx.x]);

		// JUST TO GET IT TO RUN:
		if (((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) &&
					(info.pos.modulus() < 4.9) && (shared_ita_par[threadIdx.x] > 0.0)){

			memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
			memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

			short i = 0;
			short inext = i + 1; if (inext > 5) inext = 0;
			short iprev = i - 1; if (iprev < 0) iprev = 5;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			} else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
			};
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
			};

			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
			}

			CalculateCircumcenter(&cc0, info.pos, opppos, prevpos);

			// Let's make life easier and load up an array of 6 n's beforehand.
#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;
				iprev = i - 1; if (iprev < 0) iprev = 5;
				bool bUsableSide = true;
				{
					// newly uncommented:
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						if (shared_ita_par[threadIdx.x] < shared_ita_par[izNeighMinor[i] - StartMinor])
						{
							ita_par = shared_ita_par[threadIdx.x];
							nu = shared_nu[threadIdx.x];
						}
						else {
							ita_par = shared_ita_par[izNeighMinor[i] - StartMinor];
							nu = shared_nu[izNeighMinor[i] - StartMinor];
						};

						if (shared_ita_par[izNeighMinor[i] - StartMinor] == 0.0) bUsableSide = false;
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							if (shared_ita_par[threadIdx.x] < shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL])
							{
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x];
							}
							else {
								ita_par = shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
								nu = shared_nu_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL];
							};
							if (shared_ita_par_verts[izNeighMinor[i] - StartMajor - BEGINNING_OF_CENTRAL] == 0.0) bUsableSide = false;
						}
						else {
							f64 ita_par_opp = p_ita_neut_minor[izNeighMinor[i]];
							f64 nu_theirs = p_nu_neut_minor[izNeighMinor[i]];
							if (shared_ita_par[threadIdx.x] < ita_par_opp) {
								ita_par = shared_ita_par[threadIdx.x];
								nu = shared_nu[threadIdx.x]; // why do I deliberately use the corresponding nu? nvm
							}
							else {
								ita_par = ita_par_opp;
								nu = nu_theirs;
							}
							if (ita_par_opp == 0.0) bUsableSide = false;
						};
					}
				};

				// basically bUsableSide here just depends on whether min(ita, ita_opp) == 0.
				f64_vec3 htg_diff;
				
				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
				}
					
				CalculateCircumcenter(&cc1, info.pos, nextpos, opppos);
				if (bUsableSide) {
					if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
					{
						memcpy(&opp_v, &(shared_v_n[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
					}
					else {
						if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
							(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
						{
							memcpy(&opp_v, &(shared_v_n_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
						}
						else {
							memcpy(&opp_v, &(p_v_n_minor[izNeighMinor[i]]), sizeof(f64_vec3));
						};
					};
					if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
						RotateClockwise(opp_v);
					}
					if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
						RotateAnticlockwise(opp_v);
					}
					htg_diff.x = shared_v_n[threadIdx.x].x - opp_v.x;
					htg_diff.y = shared_v_n[threadIdx.x].y - opp_v.y;
					htg_diff.z = shared_v_n[threadIdx.x].z - opp_v.z;

					f64_vec3 deriv = (opp_v - shared_v_n[threadIdx.x]) / (opppos - info.pos).modulus();
					f64_vec3 visc_contrib = over_m_n*ita_par*(cc1 - cc0).modulus()*deriv;
					// Set to 0 any that are pushing momentum uphill. For neutral this is unphysical.
				//	if (visc_contrib.x*htg_diff.x > 0.0) visc_contrib.x = 0.0;
					// Can't do it because it'll ruin backward solve.

					ownrates_visc += visc_contrib;

					if (0)//iMinor == CHOSEN) 
						printf("============================\nNeutral viscosity %d %d \n"
						"v.z %1.9E  opp_v.z %1.9E ita_par %1.9E edgelen %1.9E dist_out %1.9E \n"
						"ourpos %1.9E %1.9E prevpos %1.9E %1.9E opppos %1.9E %1.9E nextpos %1.9E %1.9E \n"
						"deriv.z %1.9E visc_contrib.z %1.9E ownrates.z %1.9E cc0 %1.8E %1.8E cc1 %1.8E %1.8E\n",
						
						iMinor, izNeighMinor[i], 
						shared_v_n[threadIdx.x].z, opp_v.z, ita_par,
						(cc1 - cc0).modulus(), (opppos - info.pos).modulus(),
						info.pos.x, info.pos.y, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y,
						deriv.z, visc_contrib.z, ownrates_visc.z, cc0.x,cc0.y, cc1.x, cc1.y
						);

					if (i % 2 == 0) {
							// vertex : heat collected by vertex
					} else {
						visc_htg += -THIRD*m_ion*(htg_diff.dot(visc_contrib));
					};
					
					
				}; // bUsableSide

				cc0 = cc1;
				prevpos = opppos;
				opppos = nextpos;

			}; // next i

			f64_vec3 ownrates;
			memcpy(&ownrates, &(p_MAR_neut[iMinor]), sizeof(f64_vec3));
			ownrates += ownrates_visc;
			memcpy(&(p_MAR_neut[iMinor]), &(ownrates), sizeof(f64_vec3));

			p_NT_addition_tri[iMinor].NnTn += visc_htg;

			// We will have to round this up into the vertex heat afterwards.
			
#ifdef DEBUGNANS
			if (ownrates.x != ownrates.x)
				printf("iMinor %d NaN ownrates.x\n", iMinor);
			if (ownrates.y != ownrates.y)
				printf("iMinor %d NaN ownrates.y\n", iMinor);
			if (ownrates.z != ownrates.z)
				printf("iMinor %d NaN ownrates.z\n", iMinor);

			if (visc_htg != visc_htg) printf("iMinor %d NAN VISC HTG\n", iMinor);
#endif

			// We do best by taking each boundary, considering how
			// much heat to add for each one.

		}
		else {
			// Not domain tri or crossing_ins
			// Did we fairly model the insulator as a reflection of v?
		}
	} // scope

}
/*
__global__ void kernelNeutral_pressure_and_momflux(
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighTriMinor,
	char * __restrict__ p_szPBCtriminor,
	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	T3 * __restrict__ p_T_minor,
	f64_vec3 * __restrict__ p_v_n_minor,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor, // Just to handle insulator

	f64_vec2 * __restrict__ p_v_overall_minor,
	f64_vec3 * __restrict__ p_MAR_neut
)
{
this routine is missing the changes to handle insulator tris, and shuold not have a setting vr=0

	__shared__ f64_vec3 shared_v_n[threadsPerTileMinor];
	__shared__ f64_vec2 shared_v_overall[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64 shared_Tn[threadsPerTileMinor]; // 3+2+2+1=8 per thread

	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];

	__shared__ f64_vec3 shared_v_n_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_v_overall_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Tn_verts[threadsPerTileMajor];  // 1/2( 13+3+2+2+1 = 21) = 10.5 => total 18.5 per minor thread.
	// shame we couldn't get down to 16 per minor thread, and if we could then that might be better even if we load on-the-fly something.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos; // QUESTION: DOES THIS LOAD CONTIGUOUSLY?
	shared_v_n[threadIdx.x] = p_v_n_minor[iMinor];
	shared_v_overall[threadIdx.x] = p_v_overall_minor[iMinor];
	shared_Tn[threadIdx.x] = p_T_minor[iMinor].Tn;		// QUESTION: DOES THIS LOAD CONTIGUOUSLY?

	// Advection should be an outer cycle at 1e-10 s.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if (info.flag == DOMAIN_VERTEX) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
			memcpy(&(shared_v_n_verts[threadIdx.x]), &(p_v_n_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_v_overall_verts[threadIdx.x] = p_v_overall_minor[iVertex + BEGINNING_OF_CENTRAL];
			shared_Tn_verts[threadIdx.x] = p_T_minor[iVertex + BEGINNING_OF_CENTRAL].Tn;
		}
		else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
			memset(&(shared_v_n_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			memset(&(shared_v_overall_verts[threadIdx.x]), 0, sizeof(f64_vec2));
			shared_Tn_verts[threadIdx.x] = 0.0;
		};
	};

	__syncthreads();

	f64_vec3 our_v, opp_v, prev_v, next_v;
	f64 oppT, prevT, nextT, ourT;
	f64_vec2 our_v_overall, prev_v_overall, next_v_overall, opp_v_overall;
	f64_vec2 opppos, prevpos, nextpos;
	f64 AreaMinor;

	if (threadIdx.x < threadsPerTileMajor) {
		AreaMinor = 0.0;
		f64_vec3 MAR_neut;
		memcpy(&(MAR_neut), &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
		
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		our_v = shared_v_n_verts[threadIdx.x];
		our_v_overall = shared_v_overall_verts[threadIdx.x];
		ourT = shared_Tn_verts[threadIdx.x];

		if (info.flag == DOMAIN_VERTEX) {

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevT = shared_Tn[izTri[iprev] - StartMinor];
				prev_v = shared_v_n[izTri[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			} else {
				T3 prev_T = p_T_minor[izTri[iprev]];
				prevT = prev_T.Tn;
				prev_v = p_v_n_minor[izTri[iprev]];
				prev_v_overall = p_v_overall_minor[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v = Clockwise_rotate3(prev_v);
				prev_v_overall = Clockwise_d*prev_v_overall;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v = Anticlock_rotate3(prev_v);
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			}
			
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppT = shared_Tn[izTri[i] - StartMinor];
				opp_v = shared_v_n[izTri[i] - StartMinor];
				opp_v_overall = shared_v_overall[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			} else {
				T3 opp_T = p_T_minor[izTri[i]];
				oppT = opp_T.Tn;
				opp_v = p_v_n_minor[izTri[i]];
				opp_v_overall = p_v_overall_minor[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v = Clockwise_rotate3(opp_v);
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v = Anticlock_rotate3(opp_v);
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			// Think carefully: DOMAIN vertex cases for n,T ...
			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

			f64_vec2 endpt1, edge_normal;

			short iend = tri_len;
			f64_vec2 projendpt0;
			if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) {
				iend = tri_len - 2;
				if (info.flag == OUTERMOST) {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_OUTER_RADIUS_d); // back of cell for Lap purposes
				} else {
					endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
				}
				edge_normal.x = endpt0.y - projendpt0.y;
				edge_normal.y = projendpt0.x - endpt0.x;
				AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
			};

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextT = shared_Tn[izTri[inext] - StartMinor];
					next_v = shared_v_n[izTri[inext] - StartMinor];
					next_v_overall = shared_v_overall[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					T3 next_T = p_T_minor[izTri[inext]];
					nextT = next_T.Tn;
					next_v = p_v_n_minor[izTri[inext]];
					next_v_overall = p_v_overall_minor[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v = Clockwise_rotate3(next_v);
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v = Anticlock_rotate3(next_v);
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				f64 n1;
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);
			
				f64 T0, T1;
				T0 = THIRD*(prevT + ourT + oppT);
				T1 = THIRD*(nextT + ourT + oppT);
				f64_vec3 v0 = THIRD*(our_v + prev_v + opp_v);
				f64_vec3 v1 = THIRD*(our_v + opp_v + next_v);

				f64 relvnormal = 0.5*((v0 + v1).xypart()
					- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
					- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
					).dot(edge_normal);
				// CHANGES 20th August 2019
				// OLD, unstable:
				// MAR_neut -= 0.5*relvnormal* (n0 *(v0-our_v) + n1 * (v1 - our_v));

				if (relvnormal < 0.0)
					MAR_neut -= 0.5*relvnormal* (n0 + n1) *(opp_v - our_v);
				// Note: minus a minus so correct sign

				// And we did what? We took n at centre of a triangle WITHIN this major cell 
				// But did not take upwind n ---- is that consistent for all advection?

				MAR_neut -= Make3(0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal, 0.0);

				// ______________________________________________________
				//// whether the v that is leaving is greater than our v ..
				//// Formula:
				//// dv/dt = (d(Nv)/dt - dN/dt v) / N
				//// We include the divide by N when we enter the accel routine.

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				n0 = n1;
				prevpos = opppos;
				prevT = oppT;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;

				opppos = nextpos;
				opp_v = next_v;
				oppT = nextT;
				opp_v_overall = next_v_overall;
			}; // next i

			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &(MAR_neut), sizeof(f64_vec3));
		} else {
			// NOT domain vertex: Do nothing
		};
	}; // was it domain vertex or Az-only
	   // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	   // __syncthreads(); // end of first vertex part
	   // Do we need syncthreads? Not overwriting any shared data here...

	   // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	our_v = shared_v_n[threadIdx.x];
	ourT = shared_Tn[threadIdx.x];
	our_v_overall = shared_v_overall[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighTriMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	f64_vec3 MAR_neut;
	memcpy(&(MAR_neut), &(p_MAR_neut[iMinor]), sizeof(f64_vec3));
	
	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		// Do nothing? Who cares what it is.
	} else {
		AreaMinor = 0.0;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				memcpy(&prev_v, &(shared_v_n[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
				prevT = shared_Tn[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izNeighMinor[iprev] - StartMinor];
			} else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&prev_v, &(shared_v_n_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					prev_v_overall = shared_v_overall_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevT = shared_Tn_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]; 
				} else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					memcpy(&prev_v, &(p_v_n_minor[izNeighMinor[iprev]]), sizeof(f64_vec3));
					prev_v_overall = p_v_overall_minor[izNeighMinor[iprev]];
					prevT = p_T_minor[izNeighMinor[iprev]].Tn;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v = Clockwise_rotate3(prev_v);
				prev_v_overall = Clockwise_d*prev_v_overall;
			};
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v = Anticlock_rotate3(prev_v);
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			};
			
			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				memcpy(&opp_v, &(shared_v_n[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
				opp_v_overall = shared_v_overall[izNeighMinor[i] - StartMinor];
				oppT = shared_Tn[izNeighMinor[i] - StartMinor];
			} else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&opp_v, &(shared_v_n_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					opp_v_overall = shared_v_overall_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppT = shared_Tn_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					memcpy(&opp_v, &(p_v_n_minor[izNeighMinor[i]]), sizeof(f64_vec3));
					opp_v_overall = p_v_overall_minor[izNeighMinor[i]];
					T3 opp_T = p_T_minor[izNeighMinor[i]];
					oppT = opp_T.Tn;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v = Clockwise_rotate3(opp_v);
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v = Anticlock_rotate3(opp_v);
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);
			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);

			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));
					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent); 
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				} else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					} else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;
				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					memcpy(&next_v, &(shared_v_n[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					next_v_overall = shared_v_overall[izNeighMinor[inext] - StartMinor];
					nextT = shared_Tn[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						memcpy(&next_v, &(shared_v_n_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
						next_v_overall = shared_v_overall_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextT = shared_Tn_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						memcpy(&next_v, &(p_v_n_minor[izNeighMinor[inext]]), sizeof(f64_vec3));
						next_v_overall = p_v_overall_minor[izNeighMinor[inext]];
						nextT = p_T_minor[izNeighMinor[inext]].Tn;						
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v = Clockwise_rotate3(next_v);
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v = Anticlock_rotate3(next_v);
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1, edge_normal;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				n0 = n_array[i];
				n1 = n_array[inext]; // 0,1 are either side of corner 0. What is seq of MinorNeigh ?

				// Assume neighs 0,1 are relevant to border with tri 0 minor.

				f64_vec3 v0 = THIRD*(our_v + prev_v + opp_v);
				f64_vec3 v1 = THIRD*(our_v + next_v + opp_v);

				//if (((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
				{	// Decided not to add test
					f64 relvnormal = 0.5*((v0 + v1).xypart()
						- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
						- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
						).dot(edge_normal);

					// CHANGES 20th August 2019:
					// OLD, unstable:
					// MAR_neut -= 0.5*relvnormal* (n0 *(v0-our_v) + n1 * (v1 - our_v));
					if (relvnormal < 0.0)
						MAR_neut -= 0.5*relvnormal* (n0 + n1) *(opp_v - our_v);

					f64 T0 = THIRD*(ourT + prevT + oppT);
					f64 T1 = THIRD*(ourT + nextT + oppT);
					
					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
						{
							// do nothing
						}
						else {
							// Looking into the insulator we see a reflection of nT. Here we look into an out-of-domain tri or vert below ins.
							// Or allowed a below-ins value to affect something anyway.
							// Just for sanity for now, let's just set our own n,T for the edge:
							n0 = p_n_minor[iMinor].n_n;
							n1 = p_n_minor[iMinor].n_n;
							T0 = ourT;
							T1 = ourT;
						}
					}

					MAR_neut -= Make3(0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal, 0.0);
				}

				endpt0 = endpt1;			
				prevT = oppT;
				prevpos = opppos;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;
				oppT = nextT;
				opppos = nextpos;
				opp_v = next_v;
				opp_v_overall = next_v_overall;
			};

			if (info.flag == CROSSING_INS) {
				// In this case set v_r = 0 and set a_TP_r = 0 and dv/dt _r = 0 in general
				//f64_vec2 rhat = info.pos / info.pos.modulus();
				MAR_neut -= Make3(
					(MAR_neut.dotxy(info.pos) /
					(info.pos.x*info.pos.x + info.pos.y*info.pos.y))*info.pos, 0.0);
				no
				// Hmm

			};
			
			memcpy(&(p_MAR_neut[iMinor]), &(MAR_neut), sizeof(f64_vec3));			
		}
		else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================
		} // non-domain tri
	}; // was it FRILL
}*/

__global__ void kernelNeutral_pressure(
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighTriMinor,
	char * __restrict__ p_szPBCtriminor,
	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	T3 * __restrict__ p_T_minor,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor, // Just to handle insulator

	bool * __restrict__ bz_pressureflag,

	f64_vec3 * __restrict__ p_MAR_neut
)
{
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64 shared_Tn[threadsPerTileMinor]; // 3+2+2+1=8 per thread

	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ f64 shared_Tn_verts[threadsPerTileMajor];  // 1/2( 13+3+2+2+1 = 21) = 10.5 => total 18.5 per minor thread.
														  // shame we couldn't get down to 16 per minor thread, and if we could then that might be better even if we load on-the-fly something.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos; // QUESTION: DOES THIS LOAD CONTIGUOUSLY?
	shared_Tn[threadIdx.x] = p_T_minor[iMinor].Tn;		// QUESTION: DOES THIS LOAD CONTIGUOUSLY?
														// Advection should be an outer cycle at 1e-10 s.
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_Tn_verts[threadIdx.x] = p_T_minor[iVertex + BEGINNING_OF_CENTRAL].Tn;
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
		};
	};

	__syncthreads();

	f64 oppT, prevT, nextT, ourT;
	f64_vec2 opppos, prevpos, nextpos;
	f64 AreaMinor;

	if (threadIdx.x < threadsPerTileMajor) {
		AreaMinor = 0.0;
		f64_vec3 MAR_neut;
		memcpy(&(MAR_neut), &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		ourT = shared_Tn_verts[threadIdx.x];

		bool bPressure = bz_pressureflag[iVertex];

		if (bPressure) {

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevT = shared_Tn[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				T3 prev_T = p_T_minor[izTri[iprev]];
				prevT = prev_T.Tn;
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppT = shared_Tn[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				T3 opp_T = p_T_minor[izTri[i]];
				oppT = opp_T.Tn;
				opppos = p_info_minor[izTri[i]].pos;
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
			}

			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec2 endpt1, edge_normal;
			short iend = tri_len;
			
			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextT = shared_Tn[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					T3 next_T = p_T_minor[izTri[inext]];
					nextT = next_T.Tn;
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				f64 n1;
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);

				f64 T0, T1;
				T0 = THIRD*(prevT + ourT + oppT);
				T1 = THIRD*(nextT + ourT + oppT);
	
				// And we did what? We took n at centre of a triangle WITHIN this major cell 
				// But did not take upwind n ---- is that consistent for all advection?

				MAR_neut -= Make3(0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal, 0.0);

		//		AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				if (TESTVNXVERT) {
					printf("iVertex %d %d iTri %d : contrib.x %1.8E n01 %1.8E %1.8E T01 %1.8E %1.8E oppT %1.8E cumu %1.8E\n",
						iVertex, i, izTri[i], 0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal.x,
						n0, n1, T0, T1, oppT, MAR_neut.x);
				}
				if (TESTVNYVERT) {
					printf("iVertex %d %d iTri %d : contrib.y %1.8E n01 %1.8E %1.8E T01 %1.8E %1.8E oppT %1.8E cumu %1.8E\n",
						iVertex, i, izTri[i], 0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal.y,
						n0, n1, T0, T1, oppT, MAR_neut.y);
				}

				endpt0 = endpt1;
				n0 = n1;
				prevpos = opppos;
				prevT = oppT;
				
				opppos = nextpos;
				oppT = nextT;
				
			}; // next i

			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &(MAR_neut), sizeof(f64_vec3));
		}
		else {
			// NOT domain vertex: Do nothing
		};
	}; // was it domain vertex or Az-only
	   // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	   // __syncthreads(); // end of first vertex part
	   // Do we need syncthreads? Not overwriting any shared data here...

	   // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	ourT = shared_Tn[threadIdx.x];
	
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighTriMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	f64_vec3 MAR_neut;
	memcpy(&(MAR_neut), &(p_MAR_neut[iMinor]), sizeof(f64_vec3));

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		// Do nothing? Who cares what it is.
	} else {
		AreaMinor = 0.0;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevT = shared_Tn[izNeighMinor[iprev] - StartMinor];
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			} else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevT = shared_Tn_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					prevT = p_T_minor[izNeighMinor[iprev]].Tn;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
			};
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
			};

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
				oppT = shared_Tn[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					oppT = shared_Tn_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					T3 opp_T = p_T_minor[izNeighMinor[i]];
					oppT = opp_T.Tn;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
			}

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);
			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
			} else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));
					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);
				} else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					} else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;
				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					nextT = shared_Tn[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextT = shared_Tn_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						nextT = p_T_minor[izNeighMinor[inext]].Tn;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
				}

				f64_vec2 endpt0, endpt1, edge_normal;
				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);
				if (TESTVNY2) printf("i %d prevpos %1.9E %1.9E opppos %1.9E %1.9E nextpos %1.9E %1.9E\n",
					i, prevpos.x, prevpos.y, opppos.x, opppos.y, nextpos.x, nextpos.y);
				
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				n0 = n_array[i];
				n1 = n_array[inext]; // 0,1 are either side of corner 0. What is seq of MinorNeigh ?

				// Assume neighs 0,1 are relevant to border with tri 0 minor.

				//if (((izNeighMinor[i] >= NumInnerFrills_d) && (izNeighMinor[i] < FirstOuterFrill_d)))
				{	// Decided not to add test

					f64 T0 = THIRD*(ourT + prevT + oppT);
					f64 T1 = THIRD*(ourT + nextT + oppT);

					// CROSSING_INS pressure isn't working .. somehow we are not getting edge_normal.y to sum to zero
					// when we have n and T the same all the way round.
					// However there's not much sense in having it anyway since nT can't possibly be different from top to bottom.
					// We could inherit the x-direction pressure if we wanted though...
					// Ideally it would be good to ask why this is happening.
					if (info.flag == CROSSING_INS) {
						char flag = p_info_minor[izNeighMinor[i]].flag;
						if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
						{

							MAR_neut -= Make3(0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal, 0.0);

							if (TESTVNY2) printf("CROSSING INS! THERMAL PRESSURE %d MAR_neut %1.10E contrib %1.10E n01 %1.8E %1.8E\n"
								"T %1.10E %1.10E edge_normal %1.9E %1.9E \n",
								iMinor, MAR_neut.y, 0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal.y,
								n0, n1, T0, T1, edge_normal.x, edge_normal.y);

						} else {
							// Looking into the insulator we see a reflection of nT. Here we look into an out-of-domain tri or vert below ins.
							// Or allowed a below-ins value to affect something anyway.
							// Just for sanity for now, let's just set our own n,T for the edge:
							n0 = p_n_minor[iMinor].n_n;
							n1 = p_n_minor[iMinor].n_n;
							T0 = ourT;
							T1 = ourT;


							if (flag == CROSSING_INS) {
								// make the edge go from the upper point, down to the insulator.
								// Basically radius changes almost linearly as we move from endpt1 to endpt0.
								f64 r1 = endpt0.modulus();
								f64 r2 = endpt1.modulus();
								if (r1 > r2) {
									// 0 is higher								
									f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(endpt0 - endpt1);
									endpt1 = point; // use this value again later for AreaMinor if nothing else
								}
								else {
									f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
									endpt0 = point;
								};
								edge_normal.x = endpt1.y - endpt0.y;
								edge_normal.y = endpt0.x - endpt1.x;

								// set nT on the edge: try just the average of the two nT, weighted by distance to own centre.
								// Recall periodic when we look at distance to own centre.

								f64 nT_edge = 0.5*(p_n_minor[iMinor].n_n*ourT + p_n_minor[izNeighMinor[i]].n_n*oppT);
								
								MAR_neut -= Make3(nT_edge*over_m_n*edge_normal, 0.0);
								if ((TESTVNY2)) printf("crossing-crossing: contrib %1.8E nT_edge %1.8E edge_normal %1.8E ourT %1.8E oppT %1.8E endpt0 %1.8E %1.8E edpt1 %1.8E %1.8E\n", -nT_edge*over_m_n*edge_normal.y,
									nT_edge, edge_normal.y, ourT, oppT,endpt0.x, endpt0.y, endpt1.x, endpt1.y);

							}
							else {
								// looking out the bottom of the insulator triangle at a within-insulator vertex or triangle.
								// so we want to project the point up to the insulator.

								// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
								char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
								char nextflag = p_info_minor[izNeighMinor[inext]].flag;

								// Let's distinguish several cases:
								if (prevflag == CROSSING_INS)
								{
									// endpt0 is THIRD * (prevpos + info.pos + opppos) 

									// move towards the position that is 2 previous --- ie the vertex above.
									// (Don't forget PBC.)
									int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
									f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
									if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
									if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;

									if (TESTVNY2) printf("prevprevpos %1.9E %1.9E \n", prevprevpos.x, prevprevpos.y);
									f64 r1 = prevprevpos.modulus();
									f64 r2 = endpt0.modulus();
									f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
									endpt0 = point;

								}
								else {
									if (TESTVNY2) printf("%%%%%%%%%%%%%%%%%%%%%%%%%% \nprevflag %d iprev %d izNeighMinor[iprev] %d\n%%%%%%%%%%%%%%%%%%%%%%%% /n",
										prevflag, iprev, izNeighMinor[iprev]);

									// prevflag will say it is below ins. 
									// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
									info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);

								};

								if (nextflag == CROSSING_INS)
								{
									// We still want to move towards vertex above. But now it's 2 next
									// Don't forget PBC
									int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
									f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
									if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
									if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;
									
									if (TESTVNY2) printf("nextnextpos %1.9E %1.9E \n", nextnextpos.x, nextnextpos.y);
									f64 r1 = nextnextpos.modulus();
									f64 r2 = endpt1.modulus();
									f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
									endpt1 = point;

								}
								else {
									// safest way: put point at the projection of our own position to insulator, maybe slightly off.
									info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);

								}
								edge_normal.x = endpt1.y - endpt0.y;
								edge_normal.y = endpt0.x - endpt1.x;

								f64 nT_edge = p_n_minor[iMinor].n_n*ourT;

								MAR_neut -= Make3(nT_edge*over_m_n*edge_normal, 0.0);
								if ((TESTVNY2)) printf("Looking into ins: contrib %1.8E nT_edge %1.8E edge_normal %1.8E endpot01 %1.8E %1.8E , %1.8E %1.8E\n", -nT_edge*over_m_n*edge_normal.y,
									nT_edge, edge_normal.y, endpt0.x, endpt0.y, endpt1.x, endpt1.y);
								// will be a 0 contribution if endpt1 = endpt0, that's ok.
							}; // CROSSING_INS neigh or not
						}; // domain triangle neigh opposite or not

					} else {


						MAR_neut -= Make3(0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal, 0.0);


						if (TESTVNY2) printf("domain! PRESSURE %d : %d %d MAR_neut %1.10E contrib %1.10E n01 %1.8E %1.8E\n"
							"T %1.10E %1.10E edge_normal %1.9E %1.9E \n",
							iMinor, i, izNeighMinor[i], MAR_neut.y, 0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal.y,
							n0, n1, T0, T1, edge_normal.x, edge_normal.y);


					};
					if ((TESTVNY2))
						printf("iMinor %d : flag %d : %d %d [flag %d] n01 %1.9E %1.9E T01 %1.9E %1.9E oppT %1.9E contrib %1.10E MAR %1.9E\n",
							iMinor, info.flag, i, izNeighMinor[i],
							p_info_minor[izNeighMinor[i]].flag,
							n0, n1, T0, T1, oppT, -0.5*(n0*T0 + n1*T1)*over_m_n*edge_normal.y,
							MAR_neut.y);
				}

				endpt0 = endpt1;
				prevT = oppT;
				prevpos = opppos;
				oppT = nextT;
				opppos = nextpos;
				iprev = i;
			};

			memcpy(&(p_MAR_neut[iMinor]), &(MAR_neut), sizeof(f64_vec3));
		} else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================

			// Crossing cath goes here: no pressure.
			// AreaMinor unknown ???

		} // non-domain tri
	}; // was it FRILL
}


__global__ void kernelNeutral_momflux(
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighTriMinor,
	char * __restrict__ p_szPBCtriminor,
	LONG3 * __restrict__ p_who_am_I_to_corners,
	LONG3 * __restrict__ p_tricornerindex,

	f64_vec3 * __restrict__ p_v_n_minor,
	ShardModel * __restrict__ p_n_shards,
	nvals * __restrict__ p_n_minor, // Just to handle insulator

	f64_vec2 * __restrict__ p_v_overall_minor,
	f64_vec3 * __restrict__ p_MAR_neut,

	NTrates * __restrict__ NT_addition_tri
)
{
	__shared__ f64_vec3 shared_v_n[threadsPerTileMinor];
	__shared__ f64_vec2 shared_v_overall[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	
	__shared__ ShardModel shared_n_shards[threadsPerTileMajor];

	__shared__ f64_vec3 shared_v_n_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_v_overall_verts[threadsPerTileMajor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
														  // shame we couldn't get down to 16 per minor thread, and if we could then that might be better even if we load on-the-fly something.

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos; // QUESTION: DOES THIS LOAD CONTIGUOUSLY?
	shared_v_n[threadIdx.x] = p_v_n_minor[iMinor];
	shared_v_overall[threadIdx.x] = p_v_overall_minor[iMinor];
	
														// Advection should be an outer cycle at 1e-10 s.

	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		if ((info.flag == DOMAIN_VERTEX) || (info.flag == OUTERMOST)) {
			memcpy(&(shared_n_shards[threadIdx.x]), &(p_n_shards[iVertex]), sizeof(ShardModel)); // + 13
			memcpy(&(shared_v_n_verts[threadIdx.x]), &(p_v_n_minor[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));
			shared_v_overall_verts[threadIdx.x] = p_v_overall_minor[iVertex + BEGINNING_OF_CENTRAL];
		} else {
			// it still manages to coalesce, let's hope, because ShardModel is 13 doubles not 12.
			memset(&(shared_n_shards[threadIdx.x]), 0, sizeof(ShardModel)); // + 13
			memset(&(shared_v_n_verts[threadIdx.x]), 0, sizeof(f64_vec3));
			memset(&(shared_v_overall_verts[threadIdx.x]), 0, sizeof(f64_vec2));
		};
	};

	__syncthreads();

	f64_vec3 our_v, opp_v, prev_v, next_v;
		f64_vec2 our_v_overall, prev_v_overall, next_v_overall, opp_v_overall;
	f64_vec2 opppos, prevpos, nextpos;
	f64 AreaMinor;

	if (threadIdx.x < threadsPerTileMajor) {
		AreaMinor = 0.0;
		f64_vec3 MAR_neut;
		memcpy(&(MAR_neut), &(p_MAR_neut[iVertex + BEGINNING_OF_CENTRAL]), sizeof(f64_vec3));

		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		our_v = shared_v_n_verts[threadIdx.x];
		our_v_overall = shared_v_overall_verts[threadIdx.x];
		
		if (info.flag == DOMAIN_VERTEX) {

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prev_v = shared_v_n[izTri[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				prev_v = p_v_n_minor[izTri[iprev]];
				prev_v_overall = p_v_overall_minor[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v = Clockwise_rotate3(prev_v);
				prev_v_overall = Clockwise_d*prev_v_overall;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v = Anticlock_rotate3(prev_v);
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opp_v = shared_v_n[izTri[i] - StartMinor];
				opp_v_overall = shared_v_overall[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				opp_v = p_v_n_minor[izTri[i]];
				opp_v_overall = p_v_overall_minor[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v = Clockwise_rotate3(opp_v);
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v = Anticlock_rotate3(opp_v);
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			// Think carefully: DOMAIN vertex cases for n,T ...
			f64 n0 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[iprev] + shared_n_shards[threadIdx.x].n_cent);
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);

			f64_vec2 endpt1, edge_normal;

			short iend = tri_len;
			// DOMAIN_VERTEX only here!
			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					next_v = shared_v_n[izTri[inext] - StartMinor];
					next_v_overall = shared_v_overall[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					next_v = p_v_n_minor[izTri[inext]];
					next_v_overall = p_v_overall_minor[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v = Clockwise_rotate3(next_v);
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v = Anticlock_rotate3(next_v);
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				f64 n1;
				n1 = THIRD*(shared_n_shards[threadIdx.x].n[i] + shared_n_shards[threadIdx.x].n[inext] + shared_n_shards[threadIdx.x].n_cent);

				f64_vec3 v0 = THIRD*(our_v + prev_v + opp_v);
				f64_vec3 v1 = THIRD*(our_v + opp_v + next_v);

				f64 relvnormal = 0.5*((v0 + v1).xypart()
					- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
					- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
					).dot(edge_normal);
				// CHANGES 20th August 2019
				// OLD, unstable:
				// MAR_neut -= 0.5*relvnormal* (n0 *(v0-our_v) + n1 * (v1 - our_v));

				int neighflag = p_info_minor[izTri[i]].flag;
				if (neighflag == DOMAIN_TRIANGLE) {

					if (relvnormal > 0.0) {
						// losing stuff 
						MAR_neut -= 0.5*relvnormal*(n0 + n1)*our_v;
					}
					else {
						MAR_neut -= 0.5*relvnormal*(n0 + n1)*opp_v;
						// Why it's minus?
						// relvnormal was less than zero but we gain a positive amt of opp_v.
					};
				};

				if (TESTVNY3) {
					printf("%d | %d %d | MAR_neuty %1.9E contrib %1.9E %1.9E n0+n1 %1.9E v0.y %1.9E \n"
						"our_v_overall %1.9E next_v_overall %1.9E prev_v_overall %1.9E \n"
						"our_v %1.9E opp_v %1.9E next_v %1.9E prev_v %1.9E\n",
						iVertex, i, izTri[i], MAR_neut.y, 
						-0.5*relvnormal* (n0 + n1) *(our_v.y), 
						-0.5*relvnormal* (n0 + n1) *(opp_v.y),
						n0+n1, v0.y,
						our_v_overall.y, next_v_overall.y, prev_v_overall.y,
						our_v.y, opp_v.y, next_v.y, prev_v.y);
				}
				if (TESTVNXVERT) {
					printf("%d | %d %d | MAR_neutx %1.9E contrib %1.9E %1.9E n0+n1 %1.9E v0.y %1.9E \n"
						"our_v_overall %1.9E next_v_overall %1.9E prev_v_overall %1.9E \n"
						"our_v %1.9E opp_v %1.9E next_v %1.9E prev_v %1.9E\n",
						iVertex, i, izTri[i], MAR_neut.x,
						-0.5*relvnormal* (n0 + n1) *(our_v.x),
						-0.5*relvnormal* (n0 + n1) *(opp_v.x),
						n0 + n1, v0.x,
						our_v_overall.x, next_v_overall.x, prev_v_overall.x,
						our_v.x, opp_v.x, next_v.x, prev_v.x);
				}

				// ______________________________________________________
				//// whether the v that is leaving is greater than our v ..
				//// Formula:
				//// dv/dt = (d(Nv)/dt - dN/dt v) / N
				//// We include the divide by N when we enter the accel routine.

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				n0 = n1;
				prevpos = opppos;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;

				opppos = nextpos;
				opp_v = next_v;
				opp_v_overall = next_v_overall;
			}; // next i


			memcpy(p_MAR_neut + iVertex + BEGINNING_OF_CENTRAL, &(MAR_neut), sizeof(f64_vec3));
		}
		else {
			// NOT domain vertex: Do nothing
		};
	}; // was it domain vertex or Az-only
	   // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	   // __syncthreads(); // end of first vertex part
	   // Do we need syncthreads? Not overwriting any shared data here...

	   // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	our_v = shared_v_n[threadIdx.x];
	our_v_overall = shared_v_overall[threadIdx.x];

	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighTriMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	f64_vec3 MAR_neut;
	memcpy(&(MAR_neut), &(p_MAR_neut[iMinor]), sizeof(f64_vec3));

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
		// Do nothing? Who cares what it is.
	}
	else {
		AreaMinor = 0.0;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			short inext, iprev = 5, i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				memcpy(&prev_v, &(shared_v_n[izNeighMinor[iprev] - StartMinor]), sizeof(f64_vec3));
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
				prev_v_overall = shared_v_overall[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&prev_v, &(shared_v_n_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					prev_v_overall = shared_v_overall_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
					memcpy(&prev_v, &(p_v_n_minor[izNeighMinor[iprev]]), sizeof(f64_vec3));
					prev_v_overall = p_v_overall_minor[izNeighMinor[iprev]];
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
				prev_v = Clockwise_rotate3(prev_v);
				prev_v_overall = Clockwise_d*prev_v_overall;
			};
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
				prev_v = Anticlock_rotate3(prev_v);
				prev_v_overall = Anticlockwise_d*prev_v_overall;
			};

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				memcpy(&opp_v, &(shared_v_n[izNeighMinor[i] - StartMinor]), sizeof(f64_vec3));
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
				opp_v_overall = shared_v_overall[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					memcpy(&opp_v, &(shared_v_n_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
					opp_v_overall = shared_v_overall_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
					memcpy(&opp_v, &(p_v_n_minor[izNeighMinor[i]]), sizeof(f64_vec3));
					opp_v_overall = p_v_overall_minor[izNeighMinor[i]];
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
				opp_v = Clockwise_rotate3(opp_v);
				opp_v_overall = Clockwise_d*opp_v_overall;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
				opp_v = Anticlock_rotate3(opp_v);
				opp_v_overall = Anticlockwise_d*opp_v_overall;
			}

			long who_am_I_to_corners[3];
			memcpy(who_am_I_to_corners, &(p_who_am_I_to_corners[iMinor]), sizeof(long) * 3);
			LONG3 cornerindex = p_tricornerindex[iMinor];
			// each corner we want to pick up 3 values off n_shards, as well as n_cent.
			// The three values will not always be contiguous!!!

			// Let's make life easier and load up an array of 6 n's beforehand.
			f64 n_array[6];
			f64 n0, n1;

			short who_am_I = who_am_I_to_corners[0];
			short tri_len = p_info_minor[cornerindex.i1 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i1 >= StartMajor) && (cornerindex.i1 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[0] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);
				n_array[1] = THIRD*(shared_n_shards[cornerindex.i1 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i1 - StartMajor].n_cent);

			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i1].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i1].n, sizeof(f64_vec2));
					n_array[0] = THIRD*(temp.x + temp.y + ncent);
					n_array[1] = THIRD*(p_n_shards[cornerindex.i1].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64_vec2));
						n_array[0] = THIRD*(p_n_shards[cornerindex.i1].n[0] + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i1].n[who_prev]), sizeof(f64) * 3);
						n_array[0] = THIRD*(temp.z + temp.y + ncent);
						n_array[1] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[1];
			tri_len = p_info_minor[cornerindex.i2 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i2 >= StartMajor) && (cornerindex.i2 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[2] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
				n_array[3] = THIRD*(shared_n_shards[cornerindex.i2 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i2 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i2].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i2].n, sizeof(f64_vec2));
					n_array[2] = THIRD*(temp.x + temp.y + ncent);
					n_array[3] = THIRD*(p_n_shards[cornerindex.i2].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64_vec2));
						n_array[2] = THIRD*(p_n_shards[cornerindex.i2].n[0] + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i2].n[who_prev]), sizeof(f64) * 3);
						n_array[2] = THIRD*(temp.z + temp.y + ncent);
						n_array[3] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

			who_am_I = who_am_I_to_corners[2];
			tri_len = p_info_minor[cornerindex.i3 + BEGINNING_OF_CENTRAL].neigh_len;

			if ((cornerindex.i3 >= StartMajor) && (cornerindex.i3 < EndMajor))
			{
				short who_prev = who_am_I - 1;
				if (who_prev < 0) who_prev = tri_len - 1;
				// Worry about pathological cases later.
				short who_next = who_am_I + 1;
				if (who_next == tri_len) who_next = 0;
				n_array[4] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_next]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
				n_array[5] = THIRD*(shared_n_shards[cornerindex.i3 - StartMajor].n[who_prev]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n[who_am_I]
					+ shared_n_shards[cornerindex.i3 - StartMajor].n_cent);
			}
			else {
				// comes from elsewhere
				f64 ncent = p_n_shards[cornerindex.i3].n_cent;
				short who_prev = who_am_I - 1;
				if (who_prev < 0) {
					who_prev = tri_len - 1;
					f64_vec2 temp;
					memcpy(&temp, p_n_shards[cornerindex.i3].n, sizeof(f64_vec2));
					n_array[4] = THIRD*(temp.x + temp.y + ncent);
					n_array[5] = THIRD*(p_n_shards[cornerindex.i3].n[who_prev] + temp.x + ncent);
				}
				else {
					short who_next = who_am_I + 1;
					if (who_next == tri_len) {
						f64_vec2 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64_vec2));
						n_array[4] = THIRD*(p_n_shards[cornerindex.i3].n[0] + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					}
					else {
						// typical case
						f64_vec3 temp;
						memcpy(&temp, &(p_n_shards[cornerindex.i3].n[who_prev]), sizeof(f64) * 3);
						n_array[4] = THIRD*(temp.z + temp.y + ncent);
						n_array[5] = THIRD*(temp.x + temp.y + ncent);
					};
				};
			}

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;
				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					memcpy(&next_v, &(shared_v_n[izNeighMinor[inext] - StartMinor]), sizeof(f64_vec3));
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
					next_v_overall = shared_v_overall[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						memcpy(&next_v, &(shared_v_n_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor]), sizeof(f64_vec3));
						next_v_overall = shared_v_overall_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					} else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
						memcpy(&next_v, &(p_v_n_minor[izNeighMinor[inext]]), sizeof(f64_vec3));
						next_v_overall = p_v_overall_minor[izNeighMinor[inext]];
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
					next_v = Clockwise_rotate3(next_v);
					next_v_overall = Clockwise_d*next_v_overall;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
					next_v = Anticlock_rotate3(next_v);
					next_v_overall = Anticlockwise_d*next_v_overall;
				}

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1, edge_normal;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				n0 = n_array[i];
				n1 = n_array[inext]; // 0,1 are either side of corner 0. What is seq of MinorNeigh ?
									 // Assume neighs 0,1 are relevant to border with tri 0 minor.

				f64_vec3 v0 = THIRD*(our_v + prev_v + opp_v);
				f64_vec3 v1 = THIRD*(our_v + next_v + opp_v);
								
				f64 relvnormal = 0.5*((v0 + v1).xypart()
						- (THIRD * (our_v_overall + next_v_overall + opp_v_overall))
						- (THIRD * (our_v_overall + prev_v_overall + opp_v_overall))
						).dot(edge_normal);				

				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
					{

						if (izNeighMinor[i] < BEGINNING_OF_CENTRAL) {
							// Note that average instead of upwind, is of course unstable.
							if (relvnormal > 0.0) {
								MAR_neut -= 0.5*relvnormal* (n0 + n1) *(our_v);
							}
							else {
								MAR_neut -= 0.5*relvnormal* (n0 + n1) *(opp_v);
							};
						}

					} else {
						
						if (flag == CROSSING_INS) {
							// make the edge go from the upper point, down to the insulator.

							// Basically radius changes almost linearly as we move from endpt1 to endpt0.
							f64 r1 = endpt0.modulus();
							f64 r2 = endpt1.modulus();

							f64_vec2 v_overall0, v_overall1;
							v_overall0 = THIRD * (our_v_overall + prev_v_overall + opp_v_overall);
							v_overall1 = THIRD * (our_v_overall + next_v_overall + opp_v_overall);

							// Note that this follows from the arithmetic definition of the thing.

							if (r1 > r2) {
								// 0 is higher								
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(endpt0 - endpt1);
								endpt1 = point; // use this value again later for AreaMinor if nothing else

								// endpt1 is defined in this way, so its motion must be defined accordingly.
								// The v_overall of the below-insulator point is actually 0.
								f64 r3 = nextpos.modulus();
								v_overall1 = ((DEVICE_RADIUS_INSULATOR_OUTER - r3) / (r1 - r3))*v_overall0;
								// but has no radial component:
								v_overall1 -= (v_overall1.dot(endpt1)) / (endpt1.dot(endpt1))*endpt1;

							}
							else {
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
								endpt0 = point;

								f64 r3 = prevpos.modulus();
								v_overall0 = ((DEVICE_RADIUS_INSULATOR_OUTER - r3) / (r2 - r3))*v_overall1;
								// but has no radial component:
								v_overall0 -= (v_overall0.dot(endpt0)) / (endpt0.dot(endpt0))*endpt1;
							};
							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;

							// have not yet handled how to do momflux between two CROSSING_INS tris.
							// the above vxy1 etc will be invalid because of taking data from insulator points.

							// Does that mean we will get weird effects? Probably. Have to think here then.

							// Reset relvnormal:

							if (prev_v.z == 0.0) v0 = 0.5*(our_v + opp_v);
							if (next_v.z == 0.0) v1 = 0.5*(our_v + opp_v);

							if (n0 == 0.0) // generated from shardmodel from inside the insulator, then it should come out 0.
								n0 = 0.5*(p_n_minor[iMinor].n_n + p_n_minor[izNeighMinor[i]].n_n);
							if (n1 == 0.0)
								n1 = 0.5*(p_n_minor[iMinor].n_n + p_n_minor[izNeighMinor[i]].n_n);

							relvnormal = 0.5*((v0 + v1).xypart()
								- v_overall0 - v_overall1).dot(edge_normal);
							

							if (relvnormal > 0.0) {
								MAR_neut -= 0.5*relvnormal* (n0 + n1) *(our_v);
							} else {
								MAR_neut -= 0.5*relvnormal* (n0 + n1) *(opp_v);
							};
						} else {
							// Looking down into insulator. 

							// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
							char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
							char nextflag = p_info_minor[izNeighMinor[inext]].flag;

							// Let's distinguish several cases:
							if (prevflag == CROSSING_INS)
							{
								int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
								f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
								if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
								if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;
								f64 r1 = prevprevpos.modulus();
								f64 r2 = endpt0.modulus();
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
								endpt0 = point;
							}
							else {
								// prevflag will say it is below ins. 
								// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
								info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);
							};

							if (nextflag == CROSSING_INS)
							{
								// We still want to move towards vertex above. But now it's 2 next
								int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
								f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
								if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
								if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;
								f64 r1 = nextnextpos.modulus();
								f64 r2 = endpt1.modulus();
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
								endpt1 = point;
							}
							else {
								// safest way: put point at the projection of our own position to insulator, maybe slightly off.
								info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);
							};
							// will be a 0 contribution if endpt1 = endpt0, that's ok.

							edge_normal.x = endpt1.y - endpt0.y;
							edge_normal.y = endpt0.x - endpt1.x;
							// should be facing towards (0,0).

							// Insulator arc isn't moving, no v_overall.
							relvnormal = our_v.dotxy(edge_normal);

							if (relvnormal > 0.0) {
								f64 n_edge = p_n_minor[iMinor].n_n;

								// Only the vr component is reversed!!!
								// f64 vr = -our_v.vxy.dot(edge_normal) / edge_normal.modulus();								
								// rhat = -edge_normal/edge_normal.modulus();
								// v-= vr rhat 
								f64_vec2 vr_rhat = edge_normal*((our_v.dotxy(edge_normal)) /
									(edge_normal.dot(edge_normal)));
								// positive amt * negative r vector = negative amt * positive r vector.

								f64 vr_squared = our_v.dotxy(edge_normal)*our_v.dotxy(edge_normal) /
									edge_normal.dot(edge_normal);

								MAR_neut -= 2.0*relvnormal*n_edge*Make3(vr_rhat, 0.0);
								
								// Now add heat:
								// change in 0.5 Nvv = 0.5v d/dt(Nv) = vr*vr*n_edge*relvnormal  since v dot vr rhat = vr^2
								// change in 1.5 NT should cancel this.
								NT_addition_tri[iMinor].NnTn += 0.6666666666667*m_n*vr_squared*n_edge*relvnormal;								
							};
							// If we are pulling away from the ins, do nothing!

						};
					};
				} else {
					// Typical edge.
					
					if (relvnormal > 0.0) {
						MAR_neut -= 0.5*relvnormal* (n0 + n1) *(our_v);
					} else {
						MAR_neut -= 0.5*relvnormal* (n0 + n1) *(opp_v);
					};

					if (((TESTVNY2)))
						printf("advectiveGPU %d i %d MAR_neut.y %1.12E contrib >0 %1.12E <0 %1.12E relvnormal %1.12E\n"
							"n0 %1.12E n1 %1.12E v01.y %1.12E %1.12E vxyours.y %1.12E opp %1.12E\n"
							"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
							CHOSEN, i,
							MAR_neut.y,
							-0.5*relvnormal* (n0 + n1) *(our_v.y), -0.5*relvnormal* (n0 + n1) *opp_v.y,
							relvnormal,
							n0, n1, v0.y, v1.y, our_v.y, opp_v.y);
				};
					
				// Notice that we also conserved momentum while we were doing ionization changes, or that was the intention.
				
				iprev = i;
				endpt0 = endpt1;
				prevpos = opppos;
				prev_v = opp_v;
				prev_v_overall = opp_v_overall;
				opppos = nextpos;
				opp_v = next_v;
				opp_v_overall = next_v_overall;
			};
			memcpy(&(p_MAR_neut[iMinor]), &(MAR_neut), sizeof(f64_vec3));
		} else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================
		} // non-domain tri
	}; // was it FRILL
}


__global__ void kernelAntiAdvect(
	f64 const h_use,
	structural * __restrict__ p_info_minor,

	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighTriMinor,
	char * __restrict__ p_szPBCtriminor,

	AAdot * __restrict__ p_AAdot,
	f64_vec2 * __restrict__ p_v_overall_minor,
	AAdot * __restrict__ p_AAdot_dest
)
{
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ AAdot shared_AAdot[threadsPerTileMinor]; // 3+2+2+1=8 per thread

	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];
	__shared__ AAdot shared_AAdot_verts[threadsPerTileMajor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor
	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	structural info = p_info_minor[iMinor];

	shared_pos[threadIdx.x] = info.pos;
	shared_AAdot[threadIdx.x] = p_AAdot[iMinor];
	
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;
		shared_AAdot_verts[threadIdx.x] = p_AAdot[iVertex + BEGINNING_OF_CENTRAL];
	};

	__syncthreads();


	AAdot oppA, prevA, nextA, ourA;
	f64_vec2 opppos, prevpos, nextpos, Integ_grad_Az, Integ_grad_Azdot;
	f64 AreaMinor;
	
	if (threadIdx.x < threadsPerTileMajor) 
	{
		if (info.flag == DOMAIN_VERTEX) {// otherwise no move
			AreaMinor = 0.0;

			Integ_grad_Az.x = 0.0;
			Integ_grad_Az.y = 0.0;
			Integ_grad_Azdot.x = 0.0;
			Integ_grad_Azdot.y = 0.0;
			long izTri[MAXNEIGH];
			char szPBC[MAXNEIGH];
			short tri_len = info.neigh_len;

			memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
			memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

			ourA = shared_AAdot_verts[threadIdx.x];

			short iprev = tri_len - 1;
			short i = 0;
			short inext;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevA = shared_AAdot[izTri[iprev] - StartMinor];
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			}
			else {
				prevA = p_AAdot[izTri[iprev]];
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
				prevpos = Clockwise_d*prevpos;
			}
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
				prevpos = Anticlockwise_d*prevpos;
			}

			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				oppA = shared_AAdot[izTri[i] - StartMinor];
				opppos = shared_pos[izTri[i] - StartMinor];
			}
			else {
				oppA = p_AAdot[izTri[i]];
				opppos = p_info_minor[izTri[i]].pos;
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
				opppos = Clockwise_d*opppos;
			}
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
				opppos = Anticlockwise_d*opppos;
			}

			// Think carefully: DOMAIN vertex cases for n,T ...
			f64_vec2 endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec2 endpt1, edge_normal;

			short iend = tri_len;
			// we said DOMAIN_VERTEX

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextA = shared_AAdot[izTri[inext] - StartMinor];
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					nextA = p_AAdot[izTri[inext]];
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
					nextpos = Clockwise_d*nextpos;
				}
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
					nextpos = Anticlockwise_d*nextpos;
				}

				f64_vec2 endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				// ______________________________________________________-

				// And we did what? We took n at centre of a triangle WITHIN this major cell 
				// But did not take upwind n ---- is that consistent for all advection?

				f64 Az_edge = SIXTH * (2.0*ourA.Az + 2.0*oppA.Az + prevA.Az + nextA.Az);
				Integ_grad_Az += Az_edge*edge_normal;
				f64 Azdot_edge = SIXTH * (2.0*ourA.Azdot + 2.0*oppA.Azdot + prevA.Azdot + nextA.Azdot);
				Integ_grad_Azdot += Azdot_edge*edge_normal;

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				prevpos = opppos;
				prevA = oppA;

				opppos = nextpos;
				oppA = nextA;
			}; // next i

			f64_vec2 Grad_Az = Integ_grad_Az / AreaMinor;
			f64_vec2 Grad_Azdot = Integ_grad_Azdot / AreaMinor;

			AAdot AAdot_dest;

			f64_vec2 v_overall = p_v_overall_minor[iVertex + BEGINNING_OF_CENTRAL];

			AAdot_dest.Az = ourA.Az + h_use*Grad_Az.dot(v_overall);
			AAdot_dest.Azdot = ourA.Azdot + h_use*Grad_Azdot.dot(v_overall);

			// Why was this minus?

			p_AAdot_dest[iVertex + BEGINNING_OF_CENTRAL] = AAdot_dest;
		} else {
			p_AAdot_dest[iVertex + BEGINNING_OF_CENTRAL] = shared_AAdot_verts[threadIdx.x];
		};
	}; 

	

	// now the minor with n_ion part:
	info = p_info_minor[iMinor];
	ourA = shared_AAdot[threadIdx.x];
	
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighTriMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);
		
	AreaMinor = 0.0;
	if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

		Integ_grad_Az.x = 0.0;
		Integ_grad_Az.y = 0.0;
		Integ_grad_Azdot.x = 0.0;
		Integ_grad_Azdot.y = 0.0;

		short inext, iprev = 5, i = 0;
		if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
		{
			prevA = shared_AAdot[izNeighMinor[iprev] - StartMinor];
			prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
		}
		else {
			if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				prevA = shared_AAdot_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				prevA = p_AAdot[izNeighMinor[iprev]];
			};
		};
		if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) {
			prevpos = Clockwise_d*prevpos;
		};
		if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) {
			prevpos = Anticlockwise_d*prevpos;
		};

		i = 0;
		if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
		{
			opppos = shared_pos[izNeighMinor[i] - StartMinor];
			oppA = shared_AAdot[izNeighMinor[i] - StartMinor];
		}
		else {
			if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
				(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
			{
				opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				oppA = shared_AAdot_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
			}
			else {
				opppos = p_info_minor[izNeighMinor[i]].pos;
				oppA = p_AAdot[izNeighMinor[i]];
			};
		};
		if (szPBC[i] == ROTATE_ME_CLOCKWISE) {
			opppos = Clockwise_d*opppos;
		}
		if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) {
			opppos = Anticlockwise_d*opppos;
		}

			
#pragma unroll 
		for (i = 0; i < 6; i++)
		{
			inext = i + 1; if (inext > 5) inext = 0;
			if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
			{
				nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				nextA = shared_AAdot[izNeighMinor[inext] - StartMinor];
			}
			else {
				if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					nextA = shared_AAdot_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					nextpos = p_info_minor[izNeighMinor[inext]].pos;
					nextA = p_AAdot[izNeighMinor[inext]];
				};
			};
			if (szPBC[inext] == ROTATE_ME_CLOCKWISE) {
				nextpos = Clockwise_d*nextpos;
			}
			if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) {
				nextpos = Anticlockwise_d*nextpos;
			}

			// New definition of endpoint of minor edge:
			f64_vec2 endpt0, endpt1, edge_normal;

			endpt0 = THIRD * (prevpos + info.pos + opppos);
			endpt1 = THIRD * (nextpos + info.pos + opppos);

			edge_normal.x = endpt1.y - endpt0.y;
			edge_normal.y = endpt0.x - endpt1.x;

			f64 Az_edge = SIXTH * (2.0*ourA.Az + 2.0*oppA.Az + prevA.Az + nextA.Az);
			Integ_grad_Az += Az_edge*edge_normal;
			f64 Azdot_edge = SIXTH * (2.0*ourA.Azdot + 2.0*oppA.Azdot + prevA.Azdot + nextA.Azdot);
			Integ_grad_Azdot += Azdot_edge*edge_normal;

			AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
			
			endpt0 = endpt1;
			prevA = oppA;
			prevpos = opppos;
			oppA = nextA;
			opppos = nextpos;				
		};

		f64_vec2 Grad_Az = Integ_grad_Az / AreaMinor;
		f64_vec2 Grad_Azdot = Integ_grad_Azdot / AreaMinor;

		AAdot AAdot_dest;

		f64_vec2 v_overall = p_v_overall_minor[iMinor];

		AAdot_dest.Az = ourA.Az + h_use*Grad_Az.dot(v_overall);
		AAdot_dest.Azdot = ourA.Azdot + h_use*Grad_Azdot.dot(v_overall);

		// Why was this minus?

		p_AAdot_dest[iMinor] = AAdot_dest;

	} else {
		p_AAdot_dest[iMinor] = shared_AAdot[threadIdx.x]; // no move
	};
	
}

__global__ void kernelGet_AreaMinorFluid(
	structural * __restrict__ p_info_minor,
	long * __restrict__ p_izTri,
	char * __restrict__ p_szPBC,
	long * __restrict__ p_izNeighMinor,
	char * __restrict__ p_szPBCtriminor,	
	bool * __restrict__ bz_pressureflag,
	f64 * __restrict__ p_AreaMinor
)
{	
	__shared__ f64_vec2 shared_pos[threadsPerTileMinor];
	__shared__ f64_vec2 shared_pos_verts[threadsPerTileMajor];

	long const iMinor = blockDim.x*blockIdx.x + threadIdx.x;
	long const iVertex = threadsPerTileMajor*blockIdx.x + threadIdx.x; // only meaningful threadIdx.x < threadsPerTileMajor

	long const StartMinor = threadsPerTileMinor*blockIdx.x;
	long const StartMajor = threadsPerTileMajor*blockIdx.x;
	long const EndMinor = StartMinor + threadsPerTileMinor;
	long const EndMajor = StartMajor + threadsPerTileMajor;

	shared_pos[threadIdx.x] = p_info_minor[iMinor].pos;
	
	structural info;
	if (threadIdx.x < threadsPerTileMajor) {
		info = p_info_minor[iVertex + BEGINNING_OF_CENTRAL];
		shared_pos_verts[threadIdx.x] = info.pos;		
	};

	__syncthreads();

	f64_vec2 opppos, prevpos, nextpos;

	if (threadIdx.x < threadsPerTileMajor) {
		
		f64 AreaMinor = 0.0;
		long izTri[MAXNEIGH];
		char szPBC[MAXNEIGH];
		short tri_len = info.neigh_len;

		memcpy(izTri, p_izTri + iVertex*MAXNEIGH, MAXNEIGH * sizeof(long));
		memcpy(szPBC, p_szPBC + iVertex*MAXNEIGH, MAXNEIGH * sizeof(char));

		{
			// Let's say we do for every vertex.

			short iprev = tri_len - 1;
			if ((izTri[iprev] >= StartMinor) && (izTri[iprev] < EndMinor))
			{
				prevpos = shared_pos[izTri[iprev] - StartMinor];
			} else {
				prevpos = p_info_minor[izTri[iprev]].pos;
			}
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			short inext, i = 0;
			if ((izTri[i] >= StartMinor) && (izTri[i] < EndMinor))
			{
				opppos = shared_pos[izTri[i] - StartMinor];
			} else {
				opppos = p_info_minor[izTri[i]].pos;
			}
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

			// Think carefully: DOMAIN vertex cases for n,T ...

			f64_vec2 endpt1, endpt0 = THIRD * (info.pos + opppos + prevpos);
			f64_vec2 store_first_point = endpt0;
			short iend = tri_len;
			f64_vec2 projendpt0, edge_normal;
			if ((info.flag == INNERMOST) || (info.flag == OUTERMOST)) iend = tri_len - 2;

			// Bear in mind for OUTERMOST, the triangles go clockwise not anticlockwise.

			if ((info.flag == INNERMOST)) {
				endpt0.project_to_radius(projendpt0, FRILL_CENTROID_INNER_RADIUS_d); // back of cell for Lap purposes
				edge_normal.x = endpt0.y - projendpt0.y;
				edge_normal.y = projendpt0.x - endpt0.x;
				AreaMinor += (0.5*projendpt0.x + 0.5*endpt0.x)*edge_normal.x;
			};

			for (i = 0; i < iend; i++)
			{
				// Tri 0 is anticlockwise of neighbour 0, we think
				inext = i + 1; if (inext >= tri_len) inext = 0;

				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextpos = shared_pos[izTri[inext] - StartMinor];
				} else {
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				endpt1 = THIRD * (nextpos + info.pos + opppos);
				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
				endpt0 = endpt1;
				prevpos = opppos;
				opppos = nextpos;

			}; // next i

			if (info.flag == INNERMOST) {
				// Now add on the final sides to give area:

				f64_vec2 projendpt1;
				endpt1.project_to_radius(projendpt1, FRILL_CENTROID_INNER_RADIUS_d);
				edge_normal.x = projendpt1.y - endpt1.y;
				edge_normal.y = endpt1.x - projendpt1.x;
				AreaMinor += (0.5*projendpt1.x + 0.5*endpt1.x)*edge_normal.x;
				edge_normal.x = projendpt0.y - projendpt1.y;
				edge_normal.y = projendpt1.x - projendpt0.x;
				AreaMinor += (0.5*projendpt1.x + 0.5*projendpt0.x)*edge_normal.x;
				// unchanged... check later
			}

			if (info.flag == OUTERMOST)
			{
				// 3 sides to add.

				//       3   4
				//     2       0
				//         1
				// endpt0=endpt1 is now the point north of edge facing 2.
				// opppos is centre of tri (3).

				info.pos.project_to_radius(nextpos, FRILL_CENTROID_OUTER_RADIUS_d);
				endpt1 = THIRD*(opppos + info.pos + nextpos);

				// map radially inwards so that radius is halfway out to the zero arc:
				f64 radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;
				// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				endpt0 = endpt1;
				prevpos = opppos;
				opppos = nextpos;
				
				inext = tri_len - 1;
				if ((izTri[inext] >= StartMinor) && (izTri[inext] < EndMinor))
				{
					nextpos = shared_pos[izTri[inext] - StartMinor];
				}
				else {
					nextpos = p_info_minor[izTri[inext]].pos;
				}
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;
				endpt1 = THIRD*(opppos + info.pos + nextpos);

				// map radially inwards so that radius is halfway out to the zero arc.
				radiusnow = endpt1.modulus();
				endpt1 *= ((0.5*(info.pos.modulus() + FRILL_CENTROID_OUTER_RADIUS_d)) / radiusnow);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;
				// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				// That was the side looking out.

				endpt0 = endpt1;
				prevpos = opppos;
				opppos = nextpos;

				endpt1 = store_first_point;
				nextpos = p_info_minor[izTri[0]].pos;

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;
				// As with our other points, edge_normal points inwards because 1 is clockwise of 0.
			
				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;
			};			
			p_AreaMinor[iVertex + BEGINNING_OF_CENTRAL] = AreaMinor;
		};

	};//  if (threadIdx.x < threadsPerTileMajor) 
	  // This branching is itself a good argument for doing Az in ITS own separate routine with no need for n_shard.

	  // __syncthreads(); // end of first vertex part
	  // Do we need syncthreads? Not overwriting any shared data here...

	  // now the minor with n_ion part:
	info = p_info_minor[iMinor];
	long izNeighMinor[6];
	char szPBC[6];
	memcpy(izNeighMinor, p_izNeighMinor + iMinor * 6, sizeof(long) * 6);
	memcpy(szPBC, p_szPBCtriminor + iMinor * 6, sizeof(char) * 6);

	if ((info.flag == OUTER_FRILL) || (info.flag == INNER_FRILL)) {
	
		p_AreaMinor[iMinor] = 1.0e-12;
	}
	else {
		f64 AreaMinor = 0.0;

		short iprev, inext, i;
		if ((info.flag == DOMAIN_TRIANGLE) || (info.flag == CROSSING_INS)) {

			iprev = 5;
			i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;

#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {

						nextpos = p_info_minor[izNeighMinor[inext]].pos;

					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:
				f64_vec2 endpt0, endpt1;
				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

			//	if (iMinor == CHOSEN) printf("%d : endpt %1.8E %1.8E | %1.8E %1.8E ;\n ",
			//		iMinor, endpt0.x, endpt0.y, endpt1.x, endpt1.y);
				// To get integral grad we add the averages along the edges times edge_normals

			//	if (iMinor == CHOSEN) printf("%d : %d opppos  %1.8E %1.8E \n",
			//		iMinor, izNeighMinor[i], opppos.x, opppos.y);
				// To get integral grad we add the averages along the edges times edge_normals

				if (info.flag == CROSSING_INS) {
					char flag = p_info_minor[izNeighMinor[i]].flag;
					if ((flag == DOMAIN_TRIANGLE) || (flag == DOMAIN_VERTEX))
					{
					
					} else {
						// Looking into the insulator we see a reflection of nT. Here we look into an out-of-domain tri or vert below ins.
						// Or allowed a below-ins value to affect something anyway.

						if (flag == CROSSING_INS) {

							// make the edge go from the upper point, down to the insulator.

							//							endpt0 = THIRD * (prevpos + info.pos + opppos);
							//							endpt1 = THIRD * (nextpos + info.pos + opppos);
							//							edge_normal.x = endpt1.y - endpt0.y;
							//							edge_normal.y = endpt0.x - endpt1.x;

							// Basically radius changes almost linearly as we move from endpt1 to endpt0.
							f64 r1 = endpt0.modulus();
							f64 r2 = endpt1.modulus();

							if (r1 > r2) {
								// 0 is higher								
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(endpt0 - endpt1);
								endpt1 = point; // use this value again later for AreaMinor if nothing else
							} else {
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r1) / (r2 - r1))*(endpt1 - endpt0);
								endpt0 = point;
							};
							
						} else {
							// looking out the bottom of the insulator triangle at a within-insulator vertex or triangle.
							// so we want to project the point up to the insulator.

							// Use prevpos, nextpos to determine what we are looking at? Can't. need flags.
							char prevflag = p_info_minor[izNeighMinor[iprev]].flag;
							char nextflag = p_info_minor[izNeighMinor[inext]].flag;

							// Let's distinguish several cases:
							if (prevflag == CROSSING_INS)
							{
								// endpt0 is THIRD * (prevpos + info.pos + opppos) 

								// move towards the position that is 2 previous --- ie the vertex above.
								// (Don't forget PBC.)
								int iprevprev = iprev - 1; if (iprevprev < 0) iprevprev = 5;
								f64_vec2 prevprevpos = p_info_minor[izNeighMinor[iprevprev]].pos;
								if (szPBC[iprevprev] == ROTATE_ME_CLOCKWISE) prevprevpos = Clockwise_d*prevprevpos;
								if (szPBC[iprevprev] == ROTATE_ME_ANTICLOCKWISE) prevprevpos = Anticlockwise_d*prevprevpos;

								f64 r1 = prevprevpos.modulus();
								f64 r2 = endpt0.modulus();
								f64_vec2 point = endpt0 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(prevprevpos - endpt0);
								endpt0 = point;

							} else {
								// prevflag will say it is below ins. 
								// Safest way: put point at the projection of our own position to insulator, maybe slightly off.								
								info.pos.project_to_radius(endpt0, DEVICE_RADIUS_INSULATOR_OUTER);
							};

							if (nextflag == CROSSING_INS)
							{
								// We still want to move towards vertex above. But now it's 2 next
								// Don't forget PBC
								int inextnext = inext + 1; if (inextnext == 6) inextnext = 0;
								f64_vec2 nextnextpos = p_info_minor[izNeighMinor[inextnext]].pos;
								if (szPBC[inextnext] == ROTATE_ME_CLOCKWISE) nextnextpos = Clockwise_d*nextnextpos;
								if (szPBC[inextnext] == ROTATE_ME_ANTICLOCKWISE) nextnextpos = Anticlockwise_d*nextnextpos;

								f64 r1 = nextnextpos.modulus();
								f64 r2 = endpt1.modulus();
								f64_vec2 point = endpt1 + ((DEVICE_RADIUS_INSULATOR_OUTER - r2) / (r1 - r2))*(nextnextpos - endpt1);
								endpt1 = point;
							}
							else {
								// safest way: put point at the projection of our own position to insulator, maybe slightly off.
								info.pos.project_to_radius(endpt1, DEVICE_RADIUS_INSULATOR_OUTER);
							}
						
							// will be a 0 contribution if endpt1 = endpt0, that's ok.
						};
					}; // domain triangle opposite or not
				} else {
					
				};

			//	if (iMinor == CHOSEN) printf(" endpt %1.8E %1.8E | %1.8E %1.8E ; \n",
			//		endpt0.x, endpt0.y, endpt1.x, endpt1.y);

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*(endpt1.y - endpt0.y);
				// See a way that FP accuracy was eroded: we take a difference of two close things already to get edge_normal.
				// can that be cleverly avoided? For all calcs?
				
				endpt0 = endpt1;

				iprev = i;
				prevpos = opppos;
				opppos = nextpos;
			};

			// No setting a_r = 0

			p_AreaMinor[iMinor] = AreaMinor;

		}
		else {
			// Not domain, not crossing_ins, not a frill
			// ==========================================

			iprev = 5; i = 0;
			if ((izNeighMinor[iprev] >= StartMinor) && (izNeighMinor[iprev] < EndMinor))
			{
				prevpos = shared_pos[izNeighMinor[iprev] - StartMinor];
			}
			else {
				if ((izNeighMinor[iprev] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[iprev] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					prevpos = shared_pos_verts[izNeighMinor[iprev] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					prevpos = p_info_minor[izNeighMinor[iprev]].pos;
				};
			};
			if (szPBC[iprev] == ROTATE_ME_CLOCKWISE) prevpos = Clockwise_d*prevpos;
			if (szPBC[iprev] == ROTATE_ME_ANTICLOCKWISE) prevpos = Anticlockwise_d*prevpos;

			i = 0;
			if ((izNeighMinor[i] >= StartMinor) && (izNeighMinor[i] < EndMinor))
			{
				opppos = shared_pos[izNeighMinor[i] - StartMinor];
			}
			else {
				if ((izNeighMinor[i] >= StartMajor + BEGINNING_OF_CENTRAL) &&
					(izNeighMinor[i] < EndMajor + BEGINNING_OF_CENTRAL))
				{
					opppos = shared_pos_verts[izNeighMinor[i] - BEGINNING_OF_CENTRAL - StartMajor];
				}
				else {
					opppos = p_info_minor[izNeighMinor[i]].pos;
				};
			};
			if (szPBC[i] == ROTATE_ME_CLOCKWISE) opppos = Clockwise_d*opppos;
			if (szPBC[i] == ROTATE_ME_ANTICLOCKWISE) opppos = Anticlockwise_d*opppos;


#pragma unroll 
			for (i = 0; i < 6; i++)
			{
				inext = i + 1; if (inext > 5) inext = 0;

				if ((izNeighMinor[inext] >= StartMinor) && (izNeighMinor[inext] < EndMinor))
				{
					nextpos = shared_pos[izNeighMinor[inext] - StartMinor];
				}
				else {
					if ((izNeighMinor[inext] >= StartMajor + BEGINNING_OF_CENTRAL) &&
						(izNeighMinor[inext] < EndMajor + BEGINNING_OF_CENTRAL))
					{
						nextpos = shared_pos_verts[izNeighMinor[inext] - BEGINNING_OF_CENTRAL - StartMajor];
					}
					else {
						nextpos = p_info_minor[izNeighMinor[inext]].pos;
					};
				};
				if (szPBC[inext] == ROTATE_ME_CLOCKWISE) nextpos = Clockwise_d*nextpos;
				if (szPBC[inext] == ROTATE_ME_ANTICLOCKWISE) nextpos = Anticlockwise_d*nextpos;

				// New definition of endpoint of minor edge:

				f64_vec2 endpt0, endpt1, edge_normal, integ_grad_Az;

				endpt0 = THIRD * (prevpos + info.pos + opppos);
				endpt1 = THIRD * (nextpos + info.pos + opppos);

				edge_normal.x = endpt1.y - endpt0.y;
				edge_normal.y = endpt0.x - endpt1.x;

				AreaMinor += (0.5*endpt0.x + 0.5*endpt1.x)*edge_normal.x;

				endpt0 = endpt1;

				prevpos = opppos;			
				opppos = nextpos;

			};

			p_AreaMinor[iMinor] = AreaMinor;
		} // non-domain tri
	}; // was it FRILL
}



__global__ void kernelAccumulateSummandsVisc(
	f64_vec2 * __restrict__ p_eps_xy, // 
	f64 * __restrict__ p_eps_iz,
	f64 * __restrict__ p_eps_ez,
	f64_vec2 * __restrict__ p_d_epsxy_by_d_beta, // f64_vec2
	f64 * __restrict__ p_d_eps_iz_by_d_beta,
	f64 * __restrict__ p_d_eps_ez_by_d_beta,	
	// outputs:
	f64 * __restrict__ p_sum_eps_deps_,  // 8 values for this block
	f64 * __restrict__ p_sum_product_matrix_
)
{
	__shared__ f64 sumdata_eps_deps[threadsPerTileMinor / 4][REGRESSORS];
	__shared__ f64 sum_product[threadsPerTileMinor / 4][REGRESSORS][REGRESSORS];
	// Call with threadsPerTileMinor/4

	// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
	long const iMinor = threadIdx.x + blockIdx.x * threadsPerTileMinor;
	// !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

	f64_vec2 depsbydbeta2[REGRESSORS];
	f64 depsbydbeta[REGRESSORS], depsbydbeta_e[REGRESSORS];

	f64_vec2 eps_xy;
	f64 eps_iz, eps_ez;
	int i, j;
	memset(&(sumdata_eps_deps[threadIdx.x]), 0, sizeof(f64)*REGRESSORS);
	memset(&(sum_product[threadIdx.x]), 0, sizeof(f64)*REGRESSORS*REGRESSORS);
	
	eps_xy = p_eps_xy[iMinor];
	eps_iz = p_eps_iz[iMinor];
	eps_ez = p_eps_ez[iMinor];
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		depsbydbeta2[i] = p_d_epsxy_by_d_beta[iMinor + i*NMINOR];
		depsbydbeta[i] = p_d_eps_iz_by_d_beta[iMinor + i*NMINOR];
		depsbydbeta_e[i] = p_d_eps_ez_by_d_beta[iMinor + i*NMINOR];
	};
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		sumdata_eps_deps[threadIdx.x][i] = depsbydbeta2[i].dot(eps_xy)
			+ depsbydbeta[i]*eps_iz + depsbydbeta_e[i]*eps_ez;
		for (j = 0; j < REGRESSORS; j++)
			sum_product[threadIdx.x][i][j] = depsbydbeta2[i].dot(depsbydbeta2[j])
			+ depsbydbeta[i] * depsbydbeta[j] + depsbydbeta_e[i] * depsbydbeta_e[j];				
	};


	eps_xy = p_eps_xy[iMinor + threadsPerTileMinor / 4];
	eps_iz = p_eps_iz[iMinor + threadsPerTileMinor / 4];
	eps_ez = p_eps_ez[iMinor + threadsPerTileMinor / 4];
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		depsbydbeta2[i] = p_d_epsxy_by_d_beta[iMinor + threadsPerTileMinor / 4 + i*NMINOR];
		depsbydbeta[i] = p_d_eps_iz_by_d_beta[iMinor + threadsPerTileMinor / 4 + i*NMINOR];
		depsbydbeta_e[i] = p_d_eps_ez_by_d_beta[iMinor + threadsPerTileMinor / 4 + i*NMINOR];
	};
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		sumdata_eps_deps[threadIdx.x][i] += depsbydbeta2[i].dot(eps_xy)
			+ depsbydbeta[i] * eps_iz + depsbydbeta_e[i] * eps_ez;
		for (j = 0; j < REGRESSORS; j++)
			sum_product[threadIdx.x][i][j] += depsbydbeta2[i].dot(depsbydbeta2[j])
			+ depsbydbeta[i] * depsbydbeta[j] + depsbydbeta_e[i] * depsbydbeta_e[j];
	};




	eps_xy = p_eps_xy[iMinor + threadsPerTileMinor / 2];
	eps_iz = p_eps_iz[iMinor + threadsPerTileMinor / 2];
	eps_ez = p_eps_ez[iMinor + threadsPerTileMinor / 2];
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		depsbydbeta2[i] = p_d_epsxy_by_d_beta[iMinor + threadsPerTileMinor / 2 + i*NMINOR];
		depsbydbeta[i] = p_d_eps_iz_by_d_beta[iMinor + threadsPerTileMinor / 2 + i*NMINOR];
		depsbydbeta_e[i] = p_d_eps_ez_by_d_beta[iMinor + threadsPerTileMinor / 2 + i*NMINOR];
	};
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		sumdata_eps_deps[threadIdx.x][i] += depsbydbeta2[i].dot(eps_xy)
			+ depsbydbeta[i] * eps_iz + depsbydbeta_e[i] * eps_ez;
		for (j = 0; j < REGRESSORS; j++)
			sum_product[threadIdx.x][i][j] += depsbydbeta2[i].dot(depsbydbeta2[j])
			+ depsbydbeta[i] * depsbydbeta[j] + depsbydbeta_e[i] * depsbydbeta_e[j];
	};

	
	eps_xy = p_eps_xy[iMinor + 3 * threadsPerTileMinor / 4];
	eps_iz = p_eps_iz[iMinor + 3 * threadsPerTileMinor / 4];
	eps_ez = p_eps_ez[iMinor + 3 * threadsPerTileMinor / 4];
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		depsbydbeta2[i] = p_d_epsxy_by_d_beta[iMinor + 3 * threadsPerTileMinor / 4 + i*NMINOR];
		depsbydbeta[i] = p_d_eps_iz_by_d_beta[iMinor + 3 * threadsPerTileMinor / 4 + i*NMINOR];
		depsbydbeta_e[i] = p_d_eps_ez_by_d_beta[iMinor + 3 * threadsPerTileMinor / 4 + i*NMINOR];
	};
#pragma unroll
	for (i = 0; i < REGRESSORS; i++)
	{
		sumdata_eps_deps[threadIdx.x][i] += depsbydbeta2[i].dot(eps_xy)
			+ depsbydbeta[i] * eps_iz + depsbydbeta_e[i] * eps_ez;
		for (j = 0; j < REGRESSORS; j++)
			sum_product[threadIdx.x][i][j] += depsbydbeta2[i].dot(depsbydbeta2[j])
			+ depsbydbeta[i] * depsbydbeta[j] + depsbydbeta_e[i] * depsbydbeta_e[j];
	};



	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			for (i = 0; i < REGRESSORS; i++)
			{
				sumdata_eps_deps[threadIdx.x][i] += sumdata_eps_deps[threadIdx.x + k][i];
				for (j = 0; j < REGRESSORS; j++)
					sum_product[threadIdx.x][i][j] += sum_product[threadIdx.x + k][i][j];
			};
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			for (i = 0; i < REGRESSORS; i++)
			{
				sumdata_eps_deps[threadIdx.x][i] += sumdata_eps_deps[threadIdx.x + s - 1][i];
				for (j = 0; j < REGRESSORS; j++)
					sum_product[threadIdx.x][i][j] += sum_product[threadIdx.x + s - 1][i][j];
			};
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		memcpy(&(p_sum_eps_deps_[blockIdx.x*REGRESSORS]), &(sumdata_eps_deps[0][0]), sizeof(f64)*REGRESSORS);
		memcpy(&(p_sum_product_matrix_[blockIdx.x*REGRESSORS*REGRESSORS]), &(sum_product[0][0][0]), sizeof(f64)*REGRESSORS*REGRESSORS);
	};
}


__global__ void SplitVector4(
	f64_vec2 * __restrict__ p_xy,
	f64 * __restrict__ p_z1,
	f64 * __restrict__ p_z2,
	v4 * __restrict__ p_v4,
	int * __restrict__ p_Select
)
{
	long const iMinor = threadIdx.x + blockIdx.x * threadsPerTileMinor;
	v4 temp;
	if (p_Select[iMinor] == 0) {
		memset(&temp, 0, sizeof(v4));
	} else {
		temp = p_v4[iMinor];
	}
	p_xy[iMinor] = temp.vxy;
	p_z1[iMinor] = temp.viz;
	p_z2[iMinor] = temp.vez;
}


__global__ void kernelAccumulateSummandsProduct(
	f64_vec2 * __restrict__ p_eps_xy, // 
	f64 * __restrict__ p_eps_iz,
	f64 * __restrict__ p_eps_ez,
	f64_vec2 * __restrict__ p_d_epsxy_by_d_beta, // f64_vec2
	f64 * __restrict__ p_d_eps_iz_by_d_beta,
	f64 * __restrict__ p_d_eps_ez_by_d_beta,
	// outputs:
	f64 * __restrict__ p_sum_eps_deps_
)
{
	__shared__ f64 sumdata_eps_deps[threadsPerTileMinor];

	long const iMinor = threadIdx.x + blockIdx.x * threadsPerTileMinor;

	f64_vec2 depsbydbeta2;
	f64 depsbydbeta, depsbydbeta_e;

	f64_vec2 eps_xy;
	f64 eps_iz, eps_ez;
	int i, j;

	sumdata_eps_deps[threadIdx.x] = 0.0;

	//memset(&(sumdata_eps_deps[threadIdx.x]), 0, sizeof(f64)*REGRESSORS);
	
	eps_xy = p_eps_xy[iMinor];
	eps_iz = p_eps_iz[iMinor];
	eps_ez = p_eps_ez[iMinor];
	
	depsbydbeta2 = p_d_epsxy_by_d_beta[iMinor];
	depsbydbeta = p_d_eps_iz_by_d_beta[iMinor];
	depsbydbeta_e = p_d_eps_ez_by_d_beta[iMinor];
	
	sumdata_eps_deps[threadIdx.x] = depsbydbeta2.dot(eps_xy)
		+ depsbydbeta * eps_iz + depsbydbeta_e * eps_ez;
		
	__syncthreads();

	int s = blockDim.x;
	int k = s / 2;

	while (s != 1) {
		if (threadIdx.x < k)
		{
			sumdata_eps_deps[threadIdx.x] += sumdata_eps_deps[threadIdx.x + k];			
		};
		__syncthreads();

		// Modify for case blockdim not 2^n:
		if ((s % 2 == 1) && (threadIdx.x == k - 1)) {
			sumdata_eps_deps[threadIdx.x] += sumdata_eps_deps[threadIdx.x + s - 1];			
		};
		// In case k == 81, add [39] += [80]
		// Otherwise we only get to 39+40=79.
		s = k;
		k = s / 2;
		__syncthreads();
	};

	if (threadIdx.x == 0)
	{
		p_sum_eps_deps_[blockIdx.x] = sumdata_eps_deps[0];
	};
}
